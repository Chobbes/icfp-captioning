So our next speaker is Klaus Ostermann of university TÃ¼bingen and he will be speaking about introduction of elimination of rules and programming languages.
>> Thanks everybody, this is work of...
All of which except Paul are also here at the conference, this is a talk about the...
about program composition, and more specifically about the structure of typing rules, and to explain the title, and make sure that we're all on the same page let me remind you of what left right, introduction elimination refers to. It refers to the structure of proof rules, or typing rules. And refers to whether the main logical connective we're talking about is below the bar or top of the bar. And difference between introduction and elimination rule, or whether the connectionive shows up on left or right, left and right elimination, left and right rules.
Okay, so... the setting that programming we're most used to.
Right introduction and right elimination rules, that's the basis of natural deduction, lamda calculus. And so forth.
That's by far the most studied kind of rule system.
The Sequent calculus is very common variant, we have introduction rules, left and right. In this talk we're interested in all these four Calculi, including the possibility to have only left introductions and left elimination rules, or only have elimination rules. So why are these interesting?
Well, we think they're interesting because each of these Calculi implies, a different setting, a different kind of programme structure.
So for instance, we have natural deduction, that we're always computing with an implicit output with nesting of expression, and inner to outer.
And... so symmetrically dealing with implicit input, that's very symmetric, if you have sequence calculus we have consist of constructers only, and have computation from the outside to the inside. We have a statement or command, and that's the way the computation goes. Maybe strangestrangest elimination, we have super formula property and computation is more inside out.
Okay. Let's make this a bit more concrete, and let's look at some examples. So for instance here, program that swaps component of binary... using natural deduction write rules, this is something we're all very familiar with. Those of you who are familiar with computational sequence calculi will also recognize how we can do this using only introduction rules in something like the lamda...
calculus, there we have these double bars, they denote demands, and on the left hand of the double bar we have... and on right hand side, consumer term, and here we see this match that's the left introduction rule of the sum, and we match itit... we put it against the input, and the bodies of the pattern matches, and there we have, again, commands that send the result right X to the output alpha. So this setting we have name output, no longer implicit output but we need to give every output a name.
Okay, let me show you another brief example to illustrate the influence of this rule choice on programme structure.
So here we are considering computation where we have product type and we have X inside and we need to navigate to the X and want to produce sum type as an output and need to inject the X into the sum type alpha.
And here... we see maybe most familiar form of how we do that in natural deduction, using the right rule, so out are the right elimination forms of the product, and injection into the sum, and gray... I put the output there, because we have this implicit output and put it there just to make it clear.
If we do the same using introduction rules, then... we can no longer use right rules...
sorry, elimination rules of course.
And so instead we have to use left introduction rules of the productproduct. The consumers of the X. And then, inside... you can't see the cursor. Inside we need to reify the currently consumed value using the mutator operator, the name of the currently consumed value, and then we can use the right introduction rules of the sum to construct the result and send it to alpha.
One interesting thing here is what I want you to watch out for is the sequence of the indices.
Here we have 2-1, and here we have 1-2. So the order in which we traverse is kind of reversed from inside-out to outside-in.
So using only left rules, this becomes a little bit simpler.
Note that I'm overloading the names in and out here for reasons that we will talk about later.
So in interaction rules, and elimination rules, we use for both products and sums. What I want you to notice here, here we have again a different order.
1-1-2. As opposed to 2-1-1 in the line above. And finally if we only consider elimination rules, we have the 4th possible way to sort these indices.
If we look at the resulting program structure we see that these four calculi correspond to four very different ways to structure the program to compute from the outside-in or from the inside-out, I think this ties nicely into recent discussions about how to design programs, and coprograms, and discussions about how to teach programming, and whether they should follow the structure of the input, or structure of the output and so forth. And I think these things are very clear here.
Okay, so let's get a bit more formal.
So we have developed calculu, in which all the rules can be expressed and put them all towing, or can only use a subset of them, and core rules that are on this slide, that are just standard rules from the bar U TATer calculus. So we have three forms of rules.
Sorry... cursor doesn't work.
One form of rules for producer terms, one form of rules for consumer terms, and then on the right we have the cut rule, which cuts producer and consumer of the same thing together.
I guess many of you have seen these kind of rules before.
So let's look at an example...
in this case, some type what the typing rules look like. So we have familiar right introduction rule for +... the right Elimination Rule is pretty much standard, except the bodies of the cases that they are commands instead of producer terms, so CI instead of EI.
And the reason for that will become clear soon.
It's trivial to encode the traditional case so the command is strictly more powerful than just producer term, left introduction rule basically like the right elimination rule, except we don't have term to discrimination, that happens on the other side of the cut. And then we have left elimination rule that then takes this form we have consumer of type T1 + T2 and then we can project out of that sum.
Okay so one of the interesting results in our work is the notion of what we call Bi-expressibility. That's a criterion to make sure all the rules are with each other. And specifically means that encode each rule diagonally opposed rule, using only diagonally opposed rules, and core rules.
So for instance, right introduction rule can be encoded withwith... and the out I left elimination rule. And one thing that is important here all these encodings no information is ever duplicated or eliminated. And same works for all the other rules. We can always eneach rule by diagonally opposed rules. And here you see the body and case match why they need to be commands and not producer terms or consumer terms because otherwise the diagonal encoding wouldn't work.
Okay, another main point of our paper is the usage of what we call the proof refutation duality. The basis of duality was paper from Tranchini, 2012.
And natural deduction style calculus, Logic calculus is not about programming. It's only in the logic domain for reputations and what's compelling about this work calculus that looks exactly like the natural deduction, calculus that we know of, except that all connectors are replaced by their respective rules. This is relation rule refuting junction. Introduction elimination. And looks exactly like standard con junction rule, and so this gave us the idea that we could have a kind of polymorphism where we interpret every proof of proposition also as refutation of dual proposition, so you have the ability to interpret terms polymorphically, with respect to proof refutational, producer consumer.
>> So let me show you an example. So from the typing rules for plus that we just saw, we can mechanically derive the typing rules for this product type.
And here we see now why we have chosen the same constructor rules, then we can interpret the same program in two ways, and we see that it works all very nicely with all the typing rules, they are exactly duel and can use exactly the same terms.
I think I will skip over the positive con junction and negative disjunction, because I want to use the remaining time to talk about something else.
Namely, we have developed practical -- somewhat practical programming language that we call Duo very much inspired by this work, and see a screenshot hereof where we translated the program from above to this language. It doesn't have... it into force to arbitrary data type and arbitrary code data typeseneral pattern matching and pattern matching. And we can use these combinations of rules together. With regard to producer, consumer polymorphism.
Having first step towards that as well, but wanted to show you a brief example, here is a function written in Duo. And what we can do in Duo, we get a quick fix and say dualize this term and it will awesomely generate copierce law. The exact SDUL of the program, that's a very ad hoc way of reusing the same programme. But at least first toward the idea.
Okay. So let me conclude.
We have identified these four equally expressive subcalculi corresponding to different programming styles, and can could exist and interact, and new idea of consumer/producer polymorphism. You can try out our language tool which is very much unfinished, but if you still want to write this, you a better ideawould be to ask us at the conference would be to ask us at the conference . Thank you.