>> Hello everyone, my name is Lisa Vasilenko I'm going to present joint work with Niki Vazou, and imagine we have two basket ballplayers throwing ball to basket with different chance of success. And we ask them to perform and throw each. And interested about two comparison questions about the results.
So if we knew inequality between chance of success, does it necessarily imply inequality between the results. Or even more, if we knew the difference between chance of success, can we know the difference between results in the end.
So this sort of question can be called a relational question, and the basketballplayers here are the illustration of probabilistic process. And so in this talk, I'm going to present a library for verification of probabilistic programs in Haskell, which I do in relation style, and also do in Haskell itself. And I continue with the balls and example of verification, and using safe coupling, and then explained we use library for more real world examples from ML. To define a probabilistic program, I will need a bunch of primitives at the word list. I want to simulate the practice of throwing about to event, which is going to be a Bernoulli distribution. It's a flip the coin distribution, it takes the first argument probability p, and returns the integer of uncertainuncertain value. And zero probability one minus. So the CRM, I use as a type for uncertainty. And since they promised to use Haskell as verification means, I need more elaborate type system. Use refinement types, which is provided by Haskell plugin liquid Haskell. So the new probability type here is a typical deployment type, it has two components, the first one is usual Haskell, type and double will give it a name. And then the second component of requirement type is a predicate that says that this double always has to be between zero and one. Then return type of Bernoulli also changes. We had done certain integer, and now we also know that it's necessarily 001. Define a monad interface for my type zero. So the pure is a single on distribution it takes one element of type A, and produces a distribution which is So I have a distribution of elements a and a function that can operate on a single sample that distribution, and result is the result of something one element from the first distribution and applying the function. .
So now that we define all the primitives, we're going to define the probabilistic formula.
Which is going to simulate one basketball player. Here I use a do notation which is a shorthand for to bind the operations and the function takes the number of throws the probability of success, and returns the score, which is probabilistic in the base case we didn't do any ...
throws so the result is constant zero.
In the inductive case we sample from recursive call to sample from a distribution flip the coin, and the results at the results and that's our result So, remember we had two basketball players, and relation property about it becomes the relation property between two programs. Now, interested to find out how the result of...
relates to bins nq. And there is he will can't way to verify programs, called relational types, so what typing judgment of we can compare left hand to it's type, and with some relational assertion. Which is very commonly, one is in sense better than the other, expression. Or oops, sorry, need to go back.
So the other common way to define assertion, American assertionassertion. And so in my system, I combine this two common approaches with a single connective, which consist of real valK, and bool predicate P.
And so numeric types for primitiverequire that there is a distance function. In this example, withstand some natural numbers absolute Val of the difference between two natural numbers. And meaning of judgment, the interpretation is the distance less or equal to numeric, and predicate and should relate to TROOU. So now that I defined the meaning for primitive TIEPTS, I go and for expressions that are distributions I stayed at judgment, there must exist, two distribution represented by left and right expression. What is coupling, imagine our left expression was flip of coin, glanced silver coin, and the right expression is the coin that commonly, more frequently ends up heads up and so the couplealing will be distribution on players. And flip both coins at the same time, and we want duration. How probable it is that one coin is 0 and other ises 1.
It's A, no, it's C.
So in addition to be correct coupling, these distribution pairs, need to be satisfied by more.
Which means if you ignore, one coin... if you ignore the golden coin, we just add the probabilities in the rows, and it should be one half and one.
Now, the can be in general case, more than one solution to the system of equations, and we put additional constraints on the requirement for coupling. First of them, every player in the coupling. Samples of the players that are now primitives, should satisfy the predicate if they have nonzero probability.
The second requirement is expectation of distance, which is again on sample from new, which are primitive, finishation of this distance should be less or equal to K. Now as promised, we're going to verify or system in high school so I need to translate it to our type system.
So first of all, I translate our typing rules which I have for each of the primitives such as for example Bernoulli distribution. This typing rule says that distribution it's disstance between Bernoulli is less than or equal to absolute value of the difference of the arguments, and the lifted relation, the predicate is less or equal, if we knew ... so it translates to two assumptions, using the eword assume, and swung on the slide, and distant bound, and type of assumption, you can see inequality that we required for the meaning of the judgment.
And so having the building blocks as typing rules, we're now ready to prove our statement. So this is how you write the theoremal statement, pretty much the same but doesn't have... which means we actually need to prove the statement, so what the statement tells is the distance between two runs of the program, P and Q, can be upper bounded, multiplied by q minus p, this is what we're going to prove, and in the assumptions, as we had before .
>> Going to the proof, I will start with sentence fashion. If the program you have two cases and start with two cases in my prof. And first one one, base case, where the program was distribution. Luckily have I typing rule for the primitive.
That says disstance, between two singleton distributions, is the same as distance between their elements. In our case, zero, and zero, .
So this case is proofeded and showcase, we showcase that we also support non syntax directed proofs. Inductive case will be a little bit different. In inductive case, I still want to prove the distance between B and being skewed and do it through the intermediate three for the function in the middle, the cost function thatconstruct, and I upper bounds the difference between initial functions by two pairs of distances, mathematically, called triangular inequality. And loss function estimate, and the components, result in boundary I expected it translates Haskell, almost literally using the operators for inequality it's equal less.
And I can also supply the exemplary statements such as the triangular inequality which we have for distance class and theorems about left comparison and right comparison with a question mark. So that concludes that proof for bins. I'm going to explain stochastic gradient decent, very widely uses in machine learning, . So its algorithm function optimization, which takes four arguments.
First of them is a good function. Called Lipschitz function. It has two parameters, the data point and the vector of weights. So our goal is to make the function output, better results on data points, and we can achieve that by adjusting rates to we want to find a good weight for the function. We are given a data set on which we can try a wrong function. And then we are given the job, which we can pray to optimise the function as performance steps of optimization. Each of them has certain influence on the outcome, which is supplied by the final argument, the list of steps. What they would like to proof about that is that the function performs good on all data points, not just the data points from the dataset. So it would be good for any other data set, the result would be roughly the same as applied. So what I say in the stability property, if I have two different data sets, that differ in exactly one element, then the distance between two cause of that SGD that have upper bounded constant, nondependent on the data set. So right hand side we see that expression that depends on Lipschitz function on F, and Val dependent on number of iterations and step sizes so does not dependent on data set, good for machine learning algorithm it is, and what we use for libraries, and examples here. And the SGD, represented now, and bin spec, is that component proving that one call to... always return something was referred to the second offense. I skip this proof, interesting part about this table which compares the size of programs two sides of proof to the combination time And for base days, I presented the non, which was interesting and good to have. But the point is particularly good for proof.
This is where relational types, train, so they are significantly improve the size of the proof.
Showing SGD. We have a mix of both, so the proof is shorter faster. And it's interesting how P and K components of connective relate to each other, and how we use this interaction between them prove our different case study, and convergence, of certain reinforcement learning call greater than our highlight is fine through since we are higher order system, which allows us to infer rules for applicative interfaces as well.
So with that present higher order relation probabilistic system. I encoded in assumptions of Liquid Haskell, which allowed me both syntax directed and onto their roofs and explore the usability of library with several case studies. Thanks, feel free to contact us, and from liquid Haskell.