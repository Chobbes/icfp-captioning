Hello, very good evening, everyone. My name is Bernard. I'm from college in Singapore. Today I am presenting on HenBlocks structuring editing for Coq. It is intended to help beginners to write. Let me give you some background. What is Coq? The Coq system is an interactive system. It's an example of a -- direct Coq proofs. On the left-hand side we have a proof with [indiscernible] tactics. On the left-hand side we have the policies and goals of the current proof state.
So Coq can be used for mathematical logic. Of course it can also be used to define functional programmes and proof of -- about them. So we've added a few pinpoints of the system. It's complex and difficult to understand. Secondly, there's difficulty in learning new specification and tactic languages. Thirdly, there is friction in user experience.
This friction is best illustrated with syntax errors. Can you guess what caused this syntax error? What about this? Or this?
Maybe this, to give you a better clue. Actually, all four of these syntax errors are caused by the same problem, which is missing the period at the end of a command or tactic.
[ Applause ].
Next, what is structure editing? Instead of user making low-level edits by directly modifying the -- the editor helps them make higher-level edits. One example is scratch.
The user can use a tool and connect them together to build up the programme. Two benefits, firstly, vocabulary discovery because all of the available constructs are available. Secondly, the code is guaranteed to be 100% correct.
So I present to you my solution, HenBlocks. HenBlocks is intended for undergrad students to experience this functional programming with little to no experience in proving, and the use case is to learn, discover and practise proving and essential transition to writing proofs with editors. It is an online web app that runs without any server.
The front end is built on -- and the back end is GS Coq. There are four panels on the left-hand side with the tool box followed by the workspace, the code and then the --
Against the advice of everyone, I will be giving a live demonstration today.
So as you can see over here, this partial proof proving that this is --
So let me choose a tactic from this tool box because I already know I want to use the left tactic. You can see that the code is immediately available here. I can step through the code and keyboard short cuts. Now I need to proof Q and you can use the exact tactic for that.
And now I need to use the hypothesis HQ, and instead of typing out HQ I can select from a dropdown list populated by the programme.
HP is not available in this branch. Why only HQ is here. Furthermore, this has been disappeared from the context so it's not available. Let's say I want to provide a more informative name. I will rename it right now. This is not the right keyboard layout. Never mind. As you can see, I've changed it to something else.
It is automatically renaming, and the suggestions will be re-named as well. Thirdly, automatic --
Let's say I have three cases. Let me add one more and you can see the programme has populated an extra start because there are now three.
So that was my third advanced feature.
Now moving back to my presentation.
Of course there are a few limitations. Potential for visual clutter. Dragging and dropping is slower than typing and there is limited vocabulary in what my programme supports. However, I contend that these limitations are mitigated because HenBlocks is intended for beginners, and these limitations are not that important when starting out.
This is also because HenBlocks can be used in the first few weeks of an undergrad class, and then after that you can transition to proper text editors.
For future work, most top priority is user testing. This could be, for example, following the undergrad students taking a Coq class or maybe AB as an AB study. Further development, we are letting users write the Coq code and generate the blocks from that.
Thirdly, user experience. For example, customising for teaching. Imagine an instructor that teaches a Coq class and they want students to only use these three tactics. Then they can customise it in editor and students when they use HenBlocks, they are constrained to only using these three tactics.
To conclude, I have applied fully fledged structured editing to proof writing. Developed advanced structured editing features. It's a promising approach to proof writing that warrants more exploration, development and testing. These are my references and acknowledgements. Thank you very much.
[ Applause ].
>> Thank you very much. That was almost five minutes on the dot, so we have to move on to the next speaker, but I encourage you to seek out Bernard and ask him questions in the break.
So the next speaker, is Eric Zhao, talking about compiling functional programmes with holes.
>> Do these mics work? Hello? Okay.
>> This is kind of exciting.
[Chuckling].
>> [Indiscernible].
>> There we go. All right. Please.
>> Hello, everyone. I'm a second-year undergraduate at the University of Michigan presenting joint work with Hilbert Chen and Yanjun Chen on compiling functional programming with holes. A little bit of context first. We're doing this in the context of developing Hazel, for those of you unfamiliar with it, it's a pure functional programming language with typed holes and also a live programming environment. This comes in two parts. There's a structure editor that enensures that all edit states are well formed, so no syntax errors, and semantics provide both static and dynamic means to all edit states.
In particular this means we can actually evaluate incomplete programmes or programmes that contain holes in them.
So as an example, we have this example, Hazel programme that shows the quantity three plus four plus this middle gap is a hole. And then plus five times six, so we will do three plus four, add the hole and this gives us an undeterminate result, and we continue around the hole, and then do five times six, which is 30, and then our final result is five plus hole plus 30. This is evaluation with holes.
And so our work is exploring how we can compile Hazel into a format that might be executed faster. This might be useful for programmes with portions that are incomplete, but also portions that have potentially intensive computation, and so we're concerned with efficient run time operations and representations for holes, as well as integrating into the live programming environment that is hazel.
And for now we're compiling -- because Hazel is web-based, and we're doing this through grain, a functional language that targets WASMbly, and this way we're kind of focusing on the problem with holes and not compiling functional programmes.
And more broadly we're exploring the design space of this area.
And so one of the key challenges is that indeterminate results are not values, and they are kind of like this tree structure. So they have very different behaviour from the values that a traditional compiler might expect, and so during run time we need to build these result trees and then because we want it to be fast we also want to minimise the overhead of dynamically checking if something is a value or an indeterminate resolve all the time.
So we have a specific memory layout for results that fits into the existing run time system to take advantage of features like garbage collection. To check if something is -- all we need to do is a simple tag check and we have a run-time library that defines how we compose these indeterminate results, like in the example that I showed.
Because we want it to be fast, we also would prefer that ideally when you're executing this programme, the complete parts of the programme can use run time -- fast run time primitives, and so we perform a static analysis to determine whether or not an expression is guaranteed to be a value or an indeterminate result of run time.
We define this notion of completeness, which has three options. First is necessarily complete. This expression must be a value at run time. There is necessarily incomplete, must be indeterminate result, and then there is indeterminate incomplete, which means we can't really know statistically, and this can -- statically, and this can rise in the failed cast and things like that.
You can see here it's represented by K. The filled dot is necessarily complete. Unfilled is necessarily incomplete. Half filled is indeterminately incomplete, and that's part of the type.
So a couple of rules we have that kind of describe these -- the type assignment for this language. As you can see, holes obviously are indeterminate results, so we say that they are necessarily incomplete. Numbers are necessarily complete. And then we have this case complete construct that takes an indeterminately incomplete scrutiny and does a dynamic check and branches based on that check.
And then because the Hazel internal language has casting to deal with holes, for casting in our run time we are first using the type index embedding and projecting approach, where casting to the hole type is embedding type information, and then projecting is to cast away from the hole type, and that can fail and produce failed cast form, which is indeterminate result.
And in particular you can see that the rule for projection produces something that is statically indeterminately incomplete.
And so this is still very early work, so we're still exploring options for pattern matching, for example, because in Hazel patterns contain pattern holes.
More efficient casting through a coercion-based approach, which can mitigate the exponential blowup that can come with the embedding projecting approach.
And then also the integration to the live programming environment that I described before, and that requires a few things. In particular we're thinking about handoff between the compiler and the evaluator, such as the user selecting which part of the code that they want to be compiled and executed quickly.
And then we're also interested in proving that this compilation pipeline is in fact preserving Hazel.
And yeah, that's it. Thanks.
[ Applause ].
>> That was another one, five minutes on the dot, so well done. We move on to the next speaker, who is Francis Rinaldi, talking about typing recurs of data structures of futures for graph types.
>> Awesome, my name is Francis Rinaldi. I work under Stephan Muller, and this is the work we've been doing.
So many of you may know programmes when they execute they can be represented as a run time DAG in the execution path where a node may only execute after its parents have executed.
So this is for some examples. Here's some sequential code, A then B then C. Here A then B then C in parallel and then it does D.
Then it can go to either B or C and then both at the same time, and then it goes to D.
So because these graphs depend on and -- sorry, depend on run-time information, it can reveal a lot about run time, and so there's many benefits to if we could statically determine these graphs. The problem is that that is impossible because of variance and input, whether it be user input or, you know, random variables.
So this led Muller to introduce kind of the core of this language, which is graph types, which is a representation of all possible DAGs that a programme can make while executing, and so the trade-off is that it is weaker because it tells you less information, but you can determine it before the programme runs.
As an example, here's a comp and add, which takes in two inputs and creates a future for both and then touches them, adds them together. For those who don't know, a future is basically an expression that spawns a new thread and computes whatever's inside of it.
And the later you can touch it to get the result.
So if you look at the type here, two ints and then returns an int, and this is the graph type, saying that the graph type G when it executes. And so the graph visually looks like this right here, where the top node, it starts there, and then it spawns going to the left and the right, new futures, and then the main thread of execution goes down at the bottom node, and the bottom node will touch both futures.
Here is another function, make future list. Mandatory makes a list of futures. So if you want to give this a type, it would be, you know, the type of a list of futures. First, in order to determine what that is, we need to look at the type of a future. So in our language, future not only has a tau, which is saying the return type of the future, but also has this U. This U is a vertex, which is a unique reference to the future in order to enable graph types to work.
Therefore, because each vertex is unique, each type for each individual future is distinct, and that brings us to the problem with this original lambda G is that it didn't allow recursive data structures of futures. The reason being that because each future has a different type, the list of futures would be heterogeneous, and there's no method for iterating over vertices in order to allow this.
So we solved this problem in the lambda Gmu rkz which is the new language, data structures of vertices.
So yeah, now this is looking a little daunting.
This is basically the type of a list of futures. So there's three things going on here. The first is the recursive binding to a variable alpha, and what is bound to alpha is the middle part, this long in-between the comma and the semi-colon, starts with a type level function that takes in a vertex function that is a stream of vertices, and it constructs a product type that is a future at the -- with the vertex that is the head of the stream, and a recursive call, which is the rest of the list, with applied to the rest of the stream.
And the third thing that's going on here is that this whole function is initially applied with the vertex structure, which is a stream, that the type is indexed with.
So finally we can give this the type, and we can give this a graph type as well, more importantly.
So this graph type here starts with the recursive binding at the very top, and then the piU is the same stream just as the type, and then it either goes down to the right it's just saying we're done making the list. To the left it's the right path on the left is saying we're going to make a future at the head of the stream, and then the left path is saying we're going to call it recursively because it's a recursive function, and passing the rest of the stream.
Here is another example. Touch future list. It's very similar. Instead of creating futures, it touches futures.
So at this point many of you might be wondering, well, this is a very specific problem. Why was it, you know, worth exploring? And to that, I do say it is a very specific problem, but kind of like as a motivating example, this function here parallelised which takes in a list of any errority and paralyses all the work upon it. This is just a -- takes a list and puts in a list of futures that executes and then you touch the list.
Thank you.
[ Applause ].
>> Thank you very much. That was the third in a row five minute on the dot. Very well done. Let's see if the graduates can do the same.
So we move on to the graduate category now, and the first one there is Nathan Corbyn to talk about generalised free extensions.
>> Hello, hi, right. I'm Nathan, and in case you haven't seen enough abstract nonsense this week, I'm going to bleat about categories for a bit.
So for extensions, when you hear this, everyone glazes over, but we're really just talking about open terms up to semantics. By open terms we mean expressions with variables and constants, but we're not caring about the explicit syntax. We care about the extension of this thing, what it actually means.
This is useful in all kinds of areas. Staged metaprogramming. You're producing code which might have unknowns in it. You might want to optimise that computation.
Partial evaluation. Normalisation by evaluation, producing expressions to the most normal form, even in the presence of free variables.
Also extremely useful for proof synthesis, proof by normalisation. Normalise the left-hand side and right-hand side side. See if they are equal. If so, you have proof that the extension is equal. These are all the applications we're looking at.
At the moment we focus on extensions of algebras, but we are really interested in free extensions of generalised algebras. These are algebras with dependently typed operations, and the point of my work is that this is actually new and we need to understand the universal property behind these generalised free extensions.
Here's a state of play at the moment. We've got a whole bunch of generalised algebraic theories that we're looking at. Various applications, and we're looking at formalising all of these. I've covered categories at the moment.
So to get concrete for a minute, we've got a commutative N plus and we want to chuck in free variables, XYZ. So we can build terms in this world, like the one on screen now. We build them using our binary -- but we're allowed to draw free variables from the set we started with. Scattered various here, and here's some concrete values 2 and 3.
Now when we actually look at this syntax, we don't care about its structure because we know we don't need brackets and we know we can commute as much as we like, so when normalising, all we actually care about is how many times X appeared, twice, Y, once, Z, once, chuck it all in a bag and pair it with the folded constant and you get normal form.
What we're really talking about, though, is how do we know that this is a correct normal form? We want a correctness condition for class of normal forms for algebraic theories.
It turns out that this structure satisfies universal property. I know eyes glazing. It's fine, honestly. We have a free algebra at the top, which is all the expressions we can build over the free variables. We've got the algebra of constants on the left, which we can include. And we get this new algebra of terms containing these expressions.
And it just so happens that if you've done your job properly, and you've constructed the algebra of normal forms, then for any arbitrary algebra W and a map from your algebra of constants into W and a map of variables into W, there should be a unique way of reducing your term to an element of W.
This is just a co-product with a free algebra. We've got a universal property. This is our correctness condition.
The problem is when moving to generalised algebra, this breaks down. We start out with this category and you extend by some new free unknown vector spaces, and then some linear maps. Here we've got F from R skwaird to X, a concrete space to a free space. G between free spaces and H from a free space to a concrete space.
We've also got some type dependency here. F type depends on X, which we've just introduced in our context. So why are co-products not good enough for this? Well, a co-product of categories, you just paste next to each other. How many maps in this picture go from R squared to X? Absolutely none. They are just side by side. We cannot put F in here.
The observation is that we can pretend for a minute that the source of F was a free object that we didn't know about, and then we can quotient here and identify V1 with R squared.
This moves our picture from looking like a co-product to looking like a restricted push out. Here we are just pushing out along the weakening and gamma here represents our collection of supporting variables that we've chucked in in order to make delta a meaningful extension of our constant algebra. Yeah, that's everything I've got to say.
[ Applause ].
>> Thank you. So this is exciting. We have time for one brief question. If anybody dares to ask. Otherwise we will move on. I can't see if there's any questions. There's too much light here. All right. I guess we will move on. Thank you.
[ Applause ].
So the next speaker is Denis Carnier who is going to talk about programme logics from mechanizing type checking.
>> Okay. Hi, everyone. Thank you, first of all, for all the wonderful questions that I got yesterday on the poster session. I enjoy talking to very many of you. We have seen already a couple of really great presentations. I did not put this much effort into my slides, but I will try to convey what I wanted to talk about today.
So maybe I should say this too. I did this work at VUB with my advisor who is a wonderful person. He's here also, Steven --
And I'm now starting a PhD at Ljubljana as well because of this.
So here's a slide from my motivation. This is from a presentation that I first gave 10 or so months ago when I wanted to work on this idea, and basically what I wanted to do is come up with this re-usable method to do executable monnatic mechanized constraint-based type inference with elaboration. Over the past 10 months we have come up with a really cool way to do this.
On the programming side we came up with this interface in which you can express constrained-based inference. We did this for a very small language, CLC without type annotation. The first one in green, this sort of interface is enough to do constrained-based type inference for a CLC with type annotations, but when you let go of the typing annotation requirements, you need existential quantification because you need to basically be able to express that you don't know what a type [indiscernible] but through unification you will figure out what that type is.
There's multiple different inferences that you can derive for entries like this, and maybe an easy way to think about the first three is as a writer monnade where you just sort of add type equalities as you come across them.
Once you add an extential it becomes more complicated, and maybe the easiest way to reason about it is through the free instance. Both of these instances actually give you an approach where you can actually sort of decouple constraint generation from the solving ports.
On the reasoning side, this was also mechanized in Coq, and there's a full proof of the functional correctness of the algorithm in the shallow embedding setting, which I will get to in a bit.
We used predicate transformers.
The problem with this is it's essentially a logic which means you have to come up with these proof trees. Much cooler way to do this, and I would really like give a big shutout to Dexter who first came up with this in the '70s is to use predicate transformers where you can sort of -- essentially the way it's been taught to me is it's a calculus so you can compute the ultimate weakest precondition for your triples, and we realised this through a system of syntax-directed reasoning rules, and basically we can do the soundness and complete these proofs in about 50 lines of LTAC where you basically say try to apply all the rules and at any point only one of them will fit, and then you just apply that one until you get basically trivial propositions to prove.
There's a problem with the shallow embedding that we used, which is basically that you're relying on the meta languages functions to represent these unification variables, and so if you really want to say that you did type an inference, then you sort of have to let go of the proof in itself, and so you need some symbolic way of representing unification variables, and the problem with that is you now have to keep track of the scoping because you no longer have the functions in the metalanguage that you were using.
This gets really hairy and complicated, and the way that we figured out that we could do this is possible role semantics. If you had the pleasure of going to Steven's talk this afternoon, first session after lunch, he also talked about this. Basically we index types by world, which represents the unification variables that are in scope to generate sort of constraints that are well formed and well scoped by construction.
And that's all I have. Thank you.
[ Applause ].
>> We have time for one question, if anybody has a question. Maybe I will take off my glasses to see. I don't see any hands anywhere. No? All right.
Well, we will move on.
[ Applause ].
So we move on to the final speaker of the session, and that is Arthur Correnson who will talk about formal verification of a lazy software model chair.
>> Hi, everybody. My name is Arthur, and I am going to present my work on the formal verification of lazy abstraction model checker.
So what is a lazy abstraction model checker, first of all? It's a methodology to ultimately check that a programme is safe to execute, and we do these kind of verification by exhaustively exploring an -- of the programme. Such model is much smaller and much easier to verify.
What makes lazy abstraction model checking so special in comparison to other techniques to do that as well is that we can refine the model during verification itself to make it more precise and conclude.
How do we do that? So to build on the refiner model of a programme, we encode programme states as first order formulas and then we can build and explore and even refine this model by -- solvers. One big limitation of this approach is that many issues are -- we are facing many issues when we do that. So the first issue is can we trust the result of the SMP solvers that we are using, and what if we made an error in the SMT queries themselves that we are sending queries?
For all those reasons, writing a model checker is prone to a lot of errors.
To address those issues, what we would like to do is develop a formally -- that is entirely developed in the Coq proof assistant. This allows us to write the code, the specifications, as well as the formal proof of correctness in one unified language. We can then abstract the co-code to correct an executable -- code.
While doing so, we are mainly facing two big challenges. The first one is fighting with the proof assistant. So yeah, Coq is a purely functional programming language, which is sometimes a problem, and it's also like a very strict setting where every function must terminate.
Another big problem we are facing is that we need to verify the integrated SMT solver. Why so? It's simply because the correctness of the entire model checker strongly depends on the correctness of the SMT solver that we are using.
The problem with that is that verifying an integrated SMT solver is in itself a difficult research topic.
So yeah, all together this is really not an ideal context if we want performance. So how do we solve those issues? So to solve the first points, it turns out that we can formulate the model checking algorithm as a transition system. We have the transition function is pure. It makes the code easier to reason about. It also opens a lot of possibilities for efficient compilation, and on top of that, it makes it easy to force the termination by using the so-called fuel technique, so iterate the function of iterations.
Now for the integrated SMT solver. So mainly there are two approaches that we can use. The first one is to just ignore the problem by saying we're just going to use the solvers as unverified oracles. This approach is not completely satisfactory.
Another approach would be to develop a defensive style of programming where we don't trust the solvers and we systematically verify the solvers at run time. This can be done by using proof validators that double-check the results.
And to do so we could, for example, leverage the frameworks that already provides a set of very efficient validators for SMT solvers.
Now the current state of the project. We do have a first prototype that uses SMT solvers as unverified oracles. However, the remainder part of the project is entirely proven in Coq and it's able to verify programme safety for programmes with integer arithmetic, and we also obstructed these to executable -- and we can run it on small programmes.
Now for future work, we really would like to extend the prototype to more realistic programmes, also make it scale a little bit, and ideally we would integrate SMT Coq as part of the development.
I thank you very much for your attention.