We want to be able to say everything, but it should be constrained enough so we can argue -- or proper semantics for it.
And I tell you that statement has such a profound impact on me that within a few months, three, four months after this Tony, the size of IDD language, has reduced by 50%, because 50% was just decoration. Nothing you do, anything real in it. I'll stop there, Simon.
>> You landed at MIT, and so this was the time at which you were first connecting data flow and lambda calculus.
>> That's right.
>> Initially it was sort of a graphical thing, right? Nothing to do with lambda calculus.
>> I think that was very clear is that data flow, just like functional languages, had no side effects. Right?
So that was the intellectual closeness of the two things, but when you start with data flow graphs and you start with pure lambda calculus, you know, the distance is very large, and it was very, very hard for me to connect the two, and that took a long time, and several people who played key role in this was Zaina was one. She was equally interested in that topic, and several others, and there was another there with us at MIT. We used to struggle with these issues, and it was great fun to try to understand all this.
And if I may just say one more thing, it was in view of bringing these communities together, because data flow was already a great community at that time. We used to have annual workshops and so on. You know, we organised a workshop at Irvine where both Backes and Dennis were speakers, and then we organised this functional programming languages in computer architecture, and I was able to find the -- can you see this?
>> Yes, yes. FPCA, a conference you started, right?
>> Right, right, right.
And within about -- after about five, six meetings of that conference, it was alternating with another conference at that time. It was clear that people from the architecture community were not going to submit papers to this conference, so it did not make sense.
>> We're going to come back to that. Let's talk to Guy a bit. So Guy, around the same time, you were -- I think you described yourself as sort of hanging around at MIT in the late '70s, early '80s. Is that right?
>> Yes, so my journey to 1978, that comes in two parts. But it started when I was 14. I was a student at the Boston Latin School, and unusually that school had an IBM-1130 for its students available to use. I got involved with the MIT high school studies programme shortly after that.
MIT had many courses that MIT students would teach to high school students. This gave me access to computers at MIT as well, and I learned that if you were well behaved and didn't cause any trouble, you could just walk into the document room at Technology Square and get artificial intelligence memos.
And so I helped myself to various memos there and just -- I was an avid reader, and some of these were about how this worked. Through those connections I also learned about the MIT press book sale, and for cheap on my limited budget I was able to buy some artificial intelligence books by Marvin Minski, and also copy the list 1.5 programmers manual.
Also there were no locks on the doors. I could just walk in and use the main computers at the AI lab. Again, as long as I kind of stayed out of the way. So I was able to use Maclisp and compare it with what I was reading in the manuals, and by about 1971 I thought this is interesting. I'm going to try and implement it myself for the 1130. I did that as a project.
I was probably 16 then, and so by the time I graduated and was ready to go to college, I had a pretty good working knowledge of lisp, and I had implemented one, and my parents said, okay, now you need a summer job. You know, between high school and college.
So I applied to a couple of local firms to work as a keypunch operator. I figured that was a skill I had. I knew how to punch cards, but they wouldn't hire me because I wasn't 18 yet.
But then I heard the Professor William Martin at MIT was interested in hiring lisp programmers, so I walked into his office and said I heard you're looking for lisp programmers, and he kind of looks me up and down and said you have to take my quiz. I took the quiz and passed, so he hired me on the spot.
And so I became the maintainer of mac lisp for the next eight years while John White took over the -- doing both.
So that's how I got a job at MIT, and I held that job for eight years, all while I was in school. It was part-time during the school year.
So that's how I got into lisp, in short, and about the time I became a graduate student I was looking for an advisor, found Jerry Sussman, whom I already knew because I'd been with the AI lab for a few years serving as a maintenance programmer for mac lisp. And we were interested in Carl Hewett's theory of actors, which is what we call very objectory [indiscernible] and we wanted to understand how this very complicated language worked.
So we decided to build a simulator, a small simulator, and we implemented that simulator in lisp because lisp was a good rapid prototyping language.
And so we implemented some of the actor features, and we built it on the substrate of something like [indiscernible] teaching those [indiscernible] and thought that the lexical scoping features would help us to better reflect actors, need to keep track of acquaintances.
So we did that, and we ended up building like a small lisp that was lexically stoked with actor features added in, and we found that we could apply lambdas and it behaved like lambda calculus, and we could send messages to actors and they would react. We were very happy with this.
We also experimented with other things, such as [indiscernible] and experimented with various efficiency [indiscernible] track of environments.
But one day while inspecting the code of our interpreter we suddenly had the realisation that the code was identical. We didn't understand why because we thought it was a very different behaviour, but we decided, well, if they have identical implementations, then they must have identical behaviour.
After struggling through it, we realised that the differences in their behaviour had to do with the primitives we were using, not with the essential nature of an actor or a lambda. We were using actors and lambdas, and that's why we were seeing a difference in behaviour, but the difference was an illusion.
And so that was the invention of scheme. Decided to build a compiler. I did most of that work in 1976 to 1977, and that brings me up to 1978. So that's what I was doing before then.
>> And your collaboration with Joey susman went on to produce an amazing sequence of papers. If everybody here hasn't read them, you should go and read them. They were a source of inspiration to me. How did they come about, that particular sequence. They were kind of MIT AI memos, weren't they?
>> Yes, we were just busy with the excitement of discovery, and we wrote things down and tossed them into the memo bin as quickly as we could. We were less interested in conference publication than just finding out what's going going on here.
So now that we've managed to explain actors, we realised that, well, lambda calculus seems to be -- lambda expressions seemed to be pretty powerful. We were aware there was a literature, so we started reading the literature.
And realised there was already this work available that explained how a lot of programming languages worked in terms of lambda calculus. We had, in effect, built a practical interpreter for lambda calculus, so we merely transcribed all those lambda expressions into parenthesis syntax, and we were able to run it all properly.
So we got into in effect bringing denotational -- into recoding them into a programming language and then wrote this down in memos.
We kind of feel like we hadn't invented anything new. We were kind of transcribing it into a working engineers notation. We were trying to explain all the imperative constructs and languages, and with lambda -- we were trying to understand binding and how that worked. And we wrote a number of other papers from there.
We were just trying to explain to ourselves all the things about the programming language that worked, and the discovery that lambda calculus was so powerful by accident because we started with actors was tremendously liberating intellectually. Kind of opened up whole fields of literature for us to exploit, so to speak.
>> I really like the way that you sort of somehow shared that joy in a very immediate way, by writing an AI memo quickly, not waiting for conference publication. Just slammed it out, and just did that successively for a sequence of papers. I found it personally very helpful to me, and I'm sure other people have -- wave if you've read some of these lambda the ultimate papers. There you go. There's a lot of waving going on.
Should we shift gears a bit. Arvind, you functioned FCPA, and part of John Backes Turing award carried the subtext of maybe we should in liberating programmes, maybe we should liberate programme execution from the architecture and we could build radically different machines. Maybe data flow machines, or SK combinator machines, who knows, to execute these things, and that would transform computer architecture as well.
Quite a bit of that was going on. You started the conference. I think Joey Sussman implemented this chip which you maybe had something to do with, Guy. It was lisp on a chip.
Some friends of mine at Cambridge implemented in microcode a machine for executing SK combinators directly in microcode. Its machine code was SK combinators.
So there was kind of a lot going on, and we thought we're going to change the world. But in the end the the world kind of -- oh, and of course very significantly, as you put in our correspondence before this, Arvind, you say you were consumed during the '80s with the task of building monsoon, which was the world's first physical embodiment of a large scale dynamic data flow machine. You may want to tell us something about that.
I'd be interested both in your feelings about what you were doing at the time and what you were trying to do and hoping to achieve, but also why in the end we didn't end up -- you know, FCPA isn't a conference anymore, and why I think our aspirations to influence the world of hardware have changed quite a bit.
So, Arvind, why don't you start on that. Anywhere in that general territory.
>> So you know, it's hard to build any big piece of hardware, and monsoon was a gigantic piece of hardware. It was a 16-node machine, but it was built from scratch. I wasn't using any microprocessors to emulate. Its architecture reflected what a data flow machine would actually be, and I think the reason why the project was exhausting and very big was because we had to do everything, all the hardware, all the software, and one lesson I learned from that was there is some -- I call it visible software, right?
So if you have to build a compiler for it, everybody understands. If you have to build some sort of operating system or source manager for the language, people understand that. People even understand that you have to build some sort of a simulator, you know, for the machine, because you have to test out ideas.
But in spite of all these high-level things, an amount of invisible software involved in building the machine would do by -- outdo it by a factor of two, right? People are just slogging, always coding, coding, coding, right? And then you will -- because, for example, debugging monsoon.
I mean, we had no idea at that time. We were so naive that you need a proper test plan for it. Then to prepare a test plan and then to actually implement it and then somebody comes and tells me: You realise that on monsoon 200 by 200 matrix multiply gives the right answer, but 400 by 400 matrix multiply doesn't.
[LAUGHTER].
>> Oops!
>> You know, I didn't even know where to begin debugging this thing because we simulated everything, and it turned out that it was causing some -- in the queue, and according to us we were within the specs of the 54 queue chips that we had bought, but apparently we were outside it or something.
It took two months of debugging to figure that out. There is nothing much to tell about that, except like this. And during cocktail hour, you can say, oh, these are my stories.
I think the main reason why these ideas didn't go very far then, because I think they may still see light of the day, is because the progress of microprocessor in terms of performance was so dramatic, so dramatic that really none of us expected it. So parallel machines kind of lost their raison d'Ãªtre because you produce a processor machine and the next microprocessor is almost there already. People were not interested in these parallel machines in terms of actual performance.
And that's the reason why almost all the start-ups in this space died, right? And the situation remained like that. It was a little bit disappointing, but we understood what was going on. It was only in the early 2000s where finally big companies changed their -- to number of cores in their machine, but just to tell you, I mean, the difference was often an order of magnitude.
My clock speed was 10 megahertz while intel was already talking of building a microprocessor that would run at 80 megahertz or something like that.
So how can you compete? So I think from users perspective, parallel machines lost their thing.
But one of the fascinating things for me is many things we did in '80s people come and ask me, you know, even today that, oh, do you know about this thing? And because people keep rediscovering, you know, as things are getting more and more parallel.
>> Every super scale microprocessor is a data flow machine with register renaming and annexe scheduling, isn't there?
>> Sorry, I can't hear you, Simon? Say it again?
>> Just saying that inside every super scaler microprocessor is a data flow machine.
>> Oh, yes.
>> With, you know, a token store and dynamic matching and scheduling. All kinds of things that happened in monsoon.
>> Right, and I think it's also happening now at the software level. I mean, when people are exploiting more and more fine grained parallelism in their software, right? Many ideas are being borrowed from there, and then people start talking about is there any hardware support we can provide for these more fine-grained parallel activities.
>> Guy? In this general territory of functional programming, computer architecture and particularly in the sort of '80s and '90s?
>> Yeah, I'd like to jump in and say that I'm fascinated by the fact that we did have so much innovative hardware being designed in the 1970s and '80s. Monsoon is an excellent example of that. There were things like list machines going on. Eventually there were the large parallel machine companies in the '80s and '90s. I belonged to one of those, thinking machines corporation. And some of the ideas developed in the monsoon and the id language influenced me in the days, as did the lecture.
All these threads were coming together and weaving together in different ways.
But I think Arvind is right. It was very hard for the data flow machines and the list machines and the scheme chips and the combinator machines to compete with Moore's law. And if all that stuff were happening today, it might be a different story now that we've mostly run out of Moore's law. We aren't seeing the clock speeds accelerate, the number of transitions on chip are somewhat -- still growing slightly. We're mostly using it to make parallel copies of stuff.
So it was an exciting time, but as I say, we couldn't keep up.
Oh, we mentioned the Conway book happening too in 1978. I was a student in the course that Lincolnway taught at MIT in spring of '78, and my student project was the first scheme ship, and by scheme chip we meant a chip that would implement the list structure in memory directly, and for technical reasons my student project failed. There was a design error. But the later scheme '79 chip and '81 chips that Sussman and Holloway and Bell and others helped design, did work and worked at a very reasonable speed, but still koond keep up with Moore's law.
In fact, implementing this structure directly is really not a great way to implement scheme. It was more of a demonstration that you could do that, and you're much better off letting a compiler jump in, and compile it into something better.
>> It's not just Moore's law. It was what we learnt in the '80s was that it's silly to build hardware to interpret stuff which you can instead do static analysis of and compile into instruction streams.
So rather than interpret something at run time in hardware, do the work statically instead. That's why an SK combinator machine I regard as being somewhat misconceived.
And so then we can ride Moore's law instead of trying to compete with it.
>> Well, the other thing I learned from Moore's law is that if my machine isn't quite powerful enough to solve a problem, instead of beating my head against a wall, I should just wait three years, buy a faster computer and let it solve the problem.
>> So true.
>> I did that a couple of times during my career. I told my son, who is now a computer programmer, I was able to benefit from that trick. You won't be able to. It's too bad.
>> So true.
>> Do you think there are any -- I don't think we're going to radically change the face of computer architecture. I still wonder if there are things that we could somehow tweak existing architectures to be better for functional programming in ways that might be better for other languages as well.
Are there any things that you wish, when you're building a compiler, that you wish were in the market processor you were compiling for? Things like, I don't know, faster read barriers for garbage collectors, or tag bits, or something. Or do you think that, you know, that day is over? I guess either of you.
>> This question is for me or for Guy? Guy, you want to go first?
>> Either of you. I'm sure you both have opinions.
>> Yeah, I -- some help for storage allocators is always welcome and helpful, but typically doesn't buy you more than factors of 1.5 or 2, and the garbage collector algorithms are continuing to evolve anyway, so it's really not clear what to standardise on after five decades.
The other thing is that some of the lisp languages at least tend to be polymorphic in nature, and support for polymorphic arithmetic could buy you a fair amount of performance.
For example, the spark processor architecture had some support for looking at the low two bits of a pointer to see whether it's really an encoded integer, the idea simply being to relieve the storage allocator of the load and instead of encoding them in pointers.
So little tricks like that might buy you factors of 20% or 50%, or maybe even a factor of 2 in a really arithmetic-intensive computation, but it's not clear that it's worth competing contorting the architecture to make all that happen.
>> Simon, I don't believe that in general purpose architectures any very specific support for a programming language is going to buy you much, again for the reasons you are giving, that compiling technology advances very rapidly too, and algorithms change constantly.
So you know, but I think many of these tricks do find some place in hardware accelerators. So if you're designing a piece of hardware for accelerating something, I mean, then you do all kinds of strange arithmetic, right? I mean, you may want to do -- arithmetic and so on because it's directly being used, you know, in that.
But the moment you start asking this question in a general purpose setting, it's very, very hard because number of jobs that actually use that stuff is so small that it's very hard to justify, and nobody can keep up with the speed of, you know, designs of the commercial microprocessors. Just because of [indiscernible].
>> Okay. Let's move on. I wanted to spend just a few minutes on sort of what happened next for you, Arvind, and then come back to you, Guy, about other programming languages.
So Arvind, you sort of moved on from data flow, founded sun burst and sort of pivoted in tally to work on blue spec and hardware design and verification. Do you want to say a little bit about how -- what caused you to do that pivot and how rewarding you found it and why you ended up doing that?
[LAUGHTER].
>> I think that's very interesting question. The way I like to describe it is that until 2000 I was primarily interested in what is the best parallel programming language, because we were designing these hardware machines, and of course want your programming language to be portable and so on.
But after 2000, my objective became exactly the opposite. I was trying to see what is the best way of designing hardware, and hardware is all about parallelism, right? I mean, in hardware, you never think in terms of one thing happening at a time, so -- and I also knew in some sense that the data flow model is inadequate because there are lots and lots of -- at least at the specification level, and even at the design level, there are lots of things which you don't want to specify.
I will give you a very simple example. In speculative processors, you don't care after how many speculations the processor starts doing the right thing, right? I mean, that's a non-deterministic thing. Depends on many, many factors inside the machine.
So I knew that I needed a parallel language which dealt with non-determinism in a very friendly and good way, right? Desirable non-determinism of the things, and I started applying the things I had learned in describing programming languages, right? Like, operational semantics. You give re-write rules for various constructs. So what happened was that I was -- I taught -- architecture subject at MIT for about 25 years, and you know, by mid '90s it was very clear that these speculative processors are very, very hard to describe precisely. Same thing was true for cache -- protocols.
So when your audience is smart, somebody says, well, what if I do it like this, right? And you say, hmm, sometimes it was clear to me that that won't work, and sometimes I said the idea is very clever, but I don't know what it does to the answers, right? Is it still correct?
So that's how I got into it. So describe hardware very precisely using term re-writing systems, and that went on for three, four years. And we were convinced that we can describe anything regardless of how complex the digital hardware you gave me, but then I went and gave talks at IBM and Intel and so on. My friends took me aside and said: Why are you doing this? Right? Who cares about this? That you have described it precisely?
I said well then you can decide whether it's right or wrong. He said, Arvind, you know we like to design fast things, so leave us alone. So this -- I was really going through a crisis trying to decide what is the next thing I want to do, and until we were having lunch, my students and I, which we often do, and one of them said: Why can't we go the other way? I mean, if you can describe these hardwares so precisely, suppose I gave you this description and didn't show you the hardware? Why can't you generate it?
And at the lunch table everybody looked at each other and said: Why not? And that's what consumed us then, and then even to synthesis. How to constrain the high-level description so we can actually generate hardware, and that's how blue spec was born, and some people played very, very key role in that, I still remember sitting under the Eiffel Tower after one of the ICFP meetings there and explaining him how he would compile hardware from DRSs, and he said, hmm, makes sense.
So that's how the campaign was born, and later on blue spec. And that's what I still do.
>> Yeah, so it's been a big shift, the last two decades, this is what you've been doing, designing and -- hardware.
>> Yes. Yep.
>> And Arvind has put a -- Arvind was talking about a very, very important idea in the last several decades, which is that you're often better off -- rather than designing something complicated directly, writing a programme to design it, or writing a programme that describes it, and of course his work with blue spec, things like verrie log, essentially no complicated hardware anymore is designed without writing a programme in effect. Even though the hardware designers will deny being programmers, that's what they are doing.
I've had them deny it to my face, and I said, no, verrie log is a programming language. This is what you're doing.
>> That's right. That's what programming people do, don't they we say to people what you're doing is really programming. I say that to Excel people. You're really just programming. They laugh.
>> At san burst, what I discovered was the difference between outstanding hardware designers and ordinary hardware designers was their understanding of programming languages. I mean, outstanding hardware designers have a very good understanding of programming languages. I mean, they just have to use them, right?
And more languages they learn, you know, better off they are.
So people who say that I don't do programming languages, I do hardware, you know, they are kidding themselves. In future it's all programming.
>> Let me shift back to you, for a sec. So there's a lot of -- quite a lot of people have played a leading role in one programming language. You know, like C + + or -- with many languages, there's a one or two individuals you can specifically associate, but I don't know anybody who has played a truly leading role in as many languages as Guy. Scheme, common list, high performance, FORTRAN, fortress and java, and these are not just dabbled in them. You played a significant role in their design and implementation.
So I'm interested in how that -- I don't know, a bit about how that journey came about. People here probably don't know as much as fortress as they know about some of these other languages, so just interested in you -- have you got any reflections on that multi-language journey that you've done and how you ended up being so influential in so many languages. I'm quite curious.
>> I think it's because I just cheerfully volunteered to do the skut work that nobody else wanted to do. In particular I enjoy writing, and I enjoy writing language specifications, and so when some committee said let's design a language, okay, who's going to write it down. I said I'll write it down. The project editor gets some influence.
>> Right.
>> So I've actually haven't initiated that many languages. I found existing projects where I found I could make a contribution. I didn't design lisp, but I helped implement mac lisp and extended a lot of ways.
I wasn't the principal on any of the implementations that went into common lisp, but I volunteered to write the book and run the committee and stuff like that. If you volunteer, you get a chance to do some work. That's really what it is.
And I was fortunate to have a lot of opportunities, you know, more than I expected to have, to do that.
Fortress is an interesting case where I was asked to lead a language design as part of a much larger project for sun microsystems. It's a part of a darba high computing systems project. It was a hardware and programming language and operating system design, and we came up with a I think really a grand plan, but it was one of those competitive procurement things, and sun ended up being number three on a list that includes cray and IBM, so we got two rounds of funding, did not get the third round. Fortress was developed to a certain extent.
I think it's an interesting design, perhaps too ambitious. In a way, it was destined to fail. We learned a lot from it. It had a 20% chance of succeeding. We had some ambitious aspects.
There are three or four things that I could point to in fortress that I think have influenced other language designs. For example, the way it does a conditional test and a conditional binding and an if statement has been adopted by swift, I think. And I still think the fortress's approach to operator precedence is superior, which is not to assign every operator a number and then compare the numbers, but rather to have a precedence graph, and the rule is if you did learn the precedence rule in high school you shouldn't use it in a programming language.
Classic example of that is plus has higher precedence than greater than. Greater than has higher precedence than and/or, but you don't want plus to have higher precedence than and/or. That's just confusing. No point in allowing that.
>> Tell us a bit about the background to your talk "growing a language," which was a -- I don't know if any of you -- wave if you've heard this talk or read it? Sort of probably a minority of waving, but it's quite a tour de force, and I'm sure it didn't just think one night, oh, that would be a cool idea, and produce it in the morning. How did it come about? Maybe tell the story, what was special about it? Why am I making a big deal about this particular talk, Guy?
>> I was asked to give a keynote took in 1998, so I was asked probably close to a year ahead, so java had been out for a couple of years, and I was reflecting on how java had progressed so far and the arguments we had had about the directions java should take in the future, and so I got this idea that I wanted to talk about the evolution of programming languages, and I was thinking about the fact that other languages I had worked on, such as common lisp, had grown over time.
And I was trying to think about guiding principles for that design and how do you keep it from just being this large hodgepodge of unrelated features. And I had some ideas for topics, but really wasn't quite sure how to structure the talk, and I had written a draft, and it was really kind of boring reading. And my colleague, Dick Gabriel who is, by the way, a professional poet, as well as a programmer, challenged me to come up with some artificial constraint.
And this interestingly reminds me of Arvind's anecdote about Tony, saying you need this artificial constraint to guide your thinking.
So he said come up with some kind of artificial constraint and live with it. That's what poets do, you know? Write it as sonnets, or something like that.
And I thought, well, suppose I can find myself -- the language or the talk itself to -- confined the language to some standard set of English. That didn't work. It still sounded too Latin. There were too many words from Latin in it.
I said I'm going to confine myself to words of one syllable. It was an arbitrary constraint. And see what happens.
And I discovered that the experience of trying to write the talk using only words of one syllable actually guided my thinking about the content of the talk in unexpected ways. And so the content of the talk grew together, and it was a surprise one-off experience, and I don't think I'll ever replicate that experience again.
And I came that close to writing a compiler for the dialect of English that I was using to make sure that things were defined before they were used. But I've decided I didn't have time to write that compiler before I had to do the talk, so I just ended up eyeball checking it, and I think I caught all the mistakes barely in time.
So the rule for the talk was -- I couldn't use any words that wasn't one syllable unless I defined them first in the talk, and so writing the talk felt very much like writing a programme.
>> Yeah, you allowed yourself to use other words, but only when you defined them using one-syllable words.
>> And I realised that what a programmer is doing, as he writes new methods, he proceeds -- in effect he is creating a new language in which he can then write further code and eventually the end application.
>> Yeah.
>> So that is a thought that I think is -- I have seen quoted a lot on Twitter. You know, a programmer just doesn't write procedures. He defines a new language in which to go further.
>> So yeah. We're at 10 minutes. I'm on it.
So in fact, I think we're going to sort of move forwards wrapping up. So for Guy and Arvind, I was going to ask you something about what the future holds, right?
So when I talk about functional programming, I often say when the limestone of imperative programming is worn away, the granite of functional programming will be revealed underneath.
But I think that --
>> If I may, I would like to jump in and ask for 90 seconds. There's one topic I would like to address that is in our notes.
One time, eight or ten years ago, I asked you, knowing now what you knew then, if you'd known then what you know now, could haskal have been called by name, and you gave me a very interesting answer. Do you recall that answer?
>> I probably said -- I'd have probably made it call by value, but then five years after that I'd say I wish I'd made it call by need.
>> I remember you told me is that, yes, all the stuff about monadss and all the other stuff could have been done in a call-by language, but you would have been tempted to put in side effects. Call by name kept you honest about being pure and functional. I thought that was a very interesting insight.
Sussman and I looked at making scheme call by name or call by need rather than call by value, but our project [indiscernible] including the side effects. We discovered that if we tried to use call by name, we couldn't understand what was going on with the side effects.
And so I think I see a linear arc where with scheme we provided a piece of engineering that was call by value, allowed you to use the side effects and explain a lot about programming languages, and for a time, scheme was also used as a basis for functional programming.
But once Haskell appeared, that took over, and scheme kind of decreased in the functional programming language and Haskell became the language of choice for those further explanations, and I feel very satisfied with that arc of history.
>> There's much more impurities, much more important than laziness.
>> Yes, the purity is the important thing.
>> [Indiscernible] purity.
>> So scheme played a role in -- results of very good that Haskell took over when it did.
>> Is there anybody who wants to ask Guy or Arvind anything? Just sort of wave your arms, and somebody with a microphone go near.
Meanwhile, I'm just going to ask Arvind if there's anything you want to say about what you think the future of functional programming might be, or anything that's come up here that you haven't had a chance to say.
But meanwhile, in parallel --
>> One thing I just wanted to say is I have been wrong about lots of things in my life, but one thing I deeply believe was in '80s that parallel programming would be the normal programming, and sequential programming would be taught as a special case of parallel programming.
I really thought this would happen at MIT to freshmen, we would teach parallel programming, and 40 years later, you know, this is just wrong. I mean, sequential programming remains very important, and I think people will have to master sequential languages. They just have to understand something about independence and lack of dependencies between things, that things can be done in any order, right?
But that's a separate concern, and by and large people do not worry about performance. Just as -- I mean, most people. Most programmers don't. Just as, you know, we expect Intel will give me a faster machine next year. We expect improvements, and they have nothing to do with it.
But parallel programming in that sense has fallen behind advances in concurrency, which is also very, very tough.
>> Okay, I'm going to -- there's a question here. So I hope you can hear this. Yell if you can't. Go.
>> So 13 years ago Guy Steele gave a key note, gave a key note at the --
>> Hang on just a sec. Can you both hear this?
>> Yes.
>> Yes.
>> Good. Carry on.
>> 2009, ICFP at Edinburgh. I believe Guy Steele gave a key note on fold considered harmful. So my question is just whether Guy Steele still considers full is harmful.
>> I do, and for example, the reasons Arvind just enunciated, which is that the ability to express parallelism and independence in computation is important. The particular definition of fold imposes a sequential composition on the functional programme you write. And a very slightly looser thing, namely reduce, enables the parallelism in the computation that allows the efficiency that Arvind talked about.
>> You mean you want an associative operator so that you can do it in parallel? Is that what you mean?
>> Yes, precisely.
>> Uh-huh.
Anyone else? I'm looking, can you -- can microphone people, can you run to the spot? Thank you. Swiftly. As our precious seconds are running out. Run!
[LAUGHTER].
This is Ron Garcia, by the way.
>> Hello.
>> I think.
>> Thank you very much for this conversation. I guess my -- I have two questions, but I'll do one.
To Guy Steele, as Simon was mentioning, you're one of the rare people who have been on many standards committees for programming languages, and I was wondering if you had any insights on what makes a standards committee or a standards process successful or not, what kinds of properties of the people and the process.
>> While Guy is responding, somebody else can wave an arm and the microphone can go, please. Thanks. Guy?
>> I think a lot depends on whether the committee is working towards some kind of common understanding and common good. Sometimes individuals who want to make their mark on a language, and that can kind of bog down the process. On the other hand sometimes they have good individual contributions to make.
I don't have any kind of wonderful insight on what kinds of committees work better than others.
Every successful committee I think has had one strong person who guides it or volunteers to do a lot of the work.
In other cases you have people who lead the way, and also does a lot of the work.
>> And as Simon has played in hasktel. -- Haskell. I think more than the committee it's the leader who can guide everybody, who can reach consensus as well as lead. I think that's what is essential for success of good definition.
>> Simon is an excellent example. I also mention Rick hickey's leadership of closure.
>> In the early days of the Haskell committee, we had a chair of the committee, but we also had a syntax czar whose job it was to -- we all agreed syntax wasn't important, except it was the thing I felt passionately about. So the syntax czar was responsible for causing a decision to happen.
Do we have time for one more question? I'm looking at Andre now. Yes, go, thank you.
>> So I've been doing this for a long time as well. I remember going to a summer school in 1981, and I was very inspired by David Turner and other people who claimed that -- crisis was the big problem we're facing and we needed an order of magnitude improvement in programme and productivity. Remember that? And functional programming was going to make us 10 times as productive.
I think if you compare with languages like Pascal and C and FORTRAN, that has certainly been delivered.
But where will the next order of magnitude come from?
>> Oh, where does the next order of magnitude. This is John Hughes, by the way, in case you didn't see on the video. Where does the next order of magnitude in programming productivity come from? A good open-ended question. Either of you. I'm off the hook here.
[LAUGHTER].
>> Maybe in 10 years artificial intelligence will do for programming what it's now doing for photographic processing.
>> I think programme synthesis.
>> Yes, it will synthesise programmes that will be a little bit bizarre in some of their corners but mostly get the job done.
>> Also I believe in AI as a programme synthesis tool, except for big programmes, by the way.
Use the microphone so they can hear you.
>> I would put my hope in programme synthesis also, but it's quite a tall order. I mean, it means that you have to specify what you want an order of magnitude more compactly than we do today, and that sounds quite difficult.
If anything can do it, maybe programme synthesis can.
>> Good. Okay. I can see Andre looking at me in a significant kind of way, and it's 6.00. We promised that we would finish at 6, so Arvind and Guy, it's been a privilege for us to have enjoyed this conversation with you. Thank you so much for what you've done, for all of us, your scholarship, your leadership, your engineering skill, and actually your personal friendship and support for many of us.
Thank you so much. We salute you.
[ Applause ]
>> Thank you for the opportunity.
[ Applause ].
>> Okay, very good. Thank you so much. I think we're -- it's heading [indiscernible] now. Thanks to the technical folk who caused these connections to take place and actually work, mostly. Thank you very much.
[ Applause ].
I know it wasn't without trials. And maybe we could do something like this again. It was quite good fun, but perhaps in person next time. So get well, Arvind. Thank you, Guy.
>> Thank you. Bye-bye.
