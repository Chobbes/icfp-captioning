>> All right. Good morning, everyone, welcome to the first regular ICFP session of the day, we have 5 exciting talks on logics, and first speaker is Simon Spies, who will tell us how to make proofs more fun.
>> Thanks, can you all hear me in the back. Hi, SIEM Simon, , a new technique that you can use to solve your step indexing troubles, this is joint work with Leonard who's sitting overoverin the audience, and Joseph, Ralf, and Lars, and Derek also find them too. So how do you build semantic models with languages with cyclic features.
Could be mixed variance recursive types, and higher order state, and Java mutable objects. What all these language features have in common. Naive semantic models for them, cyclic. And it's not obvious what you do. Right now the dominant choice to build models for the languages is use step and next logic. So the idea is step index, that a natural number either decreases in each iteration, and eventually bottoms out,.
There's just one problem.
Anybody who has done step-indexstep-indexing can attest to.
You do the proof, You have to do a tedious amount of bookkeeping.
I've taken a page from a proof here on the right and I've marked every occurrence of something related to step indexing, it's important you do these things. Because if you get them wrong the proof could be wrong. However, not very interesting things, things like G double, prime is strictly smaller than JJ is less or equal to k minus two and you need to show J w prime is strictly smaller than k minus two. And so ever since the introduction of step-indexing in the early 2000s, there was on going push to simplify step indexing. And this little triangle here, that hides all the natural numbers, and, arithmetic behind the modality inlogic, and how you can use... make sure you didn't mess up the step index. And this brings me to the problem that motivated this work. The later modality is a double-edge sword. On the one hand it's this very powerful tool that logic developers can use to reasonable, recursive constructions soundly and at a high level of abstraction. In various places they can put later R, and, just leaving off the guard would be inconsistent.
However that's not how the users typically think ant the late modality. I don't know about you I'm not typically filled with joys, being saved from inconsistency, all I worry about how to get rid of it. It's a guard and I want to access the contents it's guarding. Let me make this more precise with example that will be the running example in the talk. And predicative invariants in iris.
Used to share state between multiple threads. They could I agree on the invariant R and share it between each other, and each thread can in a single atomic step open the invariant, and get access to the contents of invariance, and get access to R. And then show again, after step R holds. At least that's how this rule is presented on the first page of the first.
Iris paper. With the disclaimer it's simplified for presentation purposes, and what's simplified about it are two things, one is masks, and the other is later modalities. For presentation purposes I'm not going to talk about MAFSHGS in the rest of the talk, but do want to talk about later modality. Impredicative invariants, s the name indicates can store an arbitrary Iris proposition, including other invariants. And as a result their model is cyclic, and the developers of iris, have decided to cut these cycles using step indexing user of the logic doesn't see much of that except for the later modality popping up later in the rule.
And now we come to the tension I mentioned earlier. For user of the logic it's kind of an nuisance, here is a simple example of what you can do in iris, shared invariant, L will store natural number N and want to load from that location. The first step that we do is use invariant opening rule and get the contents of that invariant, guarded by -- what the rule says, the MEEiate next step in the proof is remove the later.
We want to get rid of the the later, because we want to access the contents it's guarding to justify the next step, want to do a load, and only way to do that in separation logic, you have ownership of the location that you want to load from. And how do you do this?
You can't get rid of later modality in general, but there are specific instances you can.
Call the later elimination problem. We have later in our context, but we need R to actually proceed and approve.
And before this work, there were three existing options. That you could use.
The first one is timeless propositions, which are propositions that do not care about step indexing at all, which means you can just remove the later modality in front of them. And maybe your invariant is not entirely timeless but there are tricks that you can use you can use commuting rules to maybe push the later in across, separating conjunctions existential quantifiers and so on, until reach time with parts, you can access those. And finally, there's something called programs steps, which you can use to get ridlater modality, and to do that you need to step the program, and that's how internally step indexing is made sound, and these existing solutions are very useful, they work in most cases. For example for the simple invariant I showed you earlier, they work. The contents of invariant are timeless, and doesn't care about step indexing at all. You can just use them. However the solutions are no silver bullet, there are cases and admittedly they are rare, but they do occur every once in a while, the solutions don't help, a very simple example nested invariant, and we take the original invariant we have, and put inside another invariant, and the contents of that invariant are no longer timeless, what does this mean for your proof, if you now open the outer invariant, we're stuck: it's not timeless, and can't get rid of later modality there. And nothing to commute. And we need to access the points before the next program step, so can't use the stepping rule.
So... what do we do?
Well... you could say that's actually not a useful example Simon, kind of ridiculous, why would you take that invariant and put into another invariant.
To that I would say two things, yes, that example is ridiculous, but characteristic of a problem that does occur in more advance use cases, but too complicated for slide 11 of this talk. And other thing I would say, I was in a situation very similar to this about 3 weeks into the learning Iris and happily nested my invariance, and probably didn't carefully read the documentation I should have read, and pretty much in that situation. So what did I do?
I cried for help.
I went to the Iris help desk, a chat on Iris with loads of very helpful people, and said, I have my invariance like this and now I'm stuck, what do I do. You can do that, unfortunate thing is result you get in these cases is typically frustrating. So someone very clever, nonlocal refactoring of the proof, that sucks because you have to touch a lot of code, that means you flatten entire invariant hierarchy, and maintenance stuff that keeps track of how everything relates and redo everything you already did.
Because the main invariant changed, or no one comes up with something, and you can always give up. A lot of interesting problems that do not involve step indexing, so maybe try one of those.
[Laughter] we looked at this, and well, we didn't like it very much.
So we thought, "can we not develop another option we can use ".
And that's what we did with later credits. Later credits turn the right to eliminate to later. So R into the R, and ownable resource, which denote with little pound sign here.
And that resource, is subject to all the traditional separation logic reasoning, from separation logic.
You can pass them around in preand post conditions, and frame them around function calls and share them be invariance, and rest of the talk I'm going to show you what that looks like. In a nutshell, we take the stepping rule and split into the two parts. One, rule that gives us a later credit after taking a physical step, basically being rewarded for taking that step, and then a second rule that allows us to spend that later credit for later elimination, if you take the rules and put them together you get a rule from above, nothing changed.
But you don't have to use them right after each other. You can use earlier steps of computation, to justify later elimitations down the road, for example, in this case, take a later credit we get in adecision to 41 + 1. And justify, and get the load of location L, specifically we take one step, of computation and get later credit. And now, use the power of separation logic to say, look a function call, and resources don't need them for function call, I want to frame them around the function call. That means after the function call we still have the later credit, and get to the nested invariant, and open it up, and have inner invariant, and can spend that credit to get rid of later modality, and suddenly we just have our invariant we had earlier in the example, and from here on the proof is pretty straightforward.
So this is what we call prepaid reasoning and that's what later credits enable, and looked at this, and thought can we scale this to even more, and one example I want to discuss in the talk is prepaid invariants.
As you might have guessed by now, our life would be a lot simpler if there were no later modalities in that rule. So before this work I went to the experts and asked can't we just remove later modalities and they were like ahhh, I don't think that's sound. Right?
So let's see what happens when we put later credits into invariant, the idea is relatively simple, build a new kind of invariant that stores the contents that we want and the later credit.
And then when we open that invariant, we have the later credit in hand, so we can use it for later elimination. So we get direct access to the contents of that invariant.
And then we need to put one later credit back, that's generated by the next step of computation.
In fact... we can actually generalize the later credit mechanism a little bit to generate more than one later credit first step, and we do get the full rule, where there is no later here, and also don't have to put a credit back after the next step.
And with that, I want to start to rev up. There is more in the paper I didn't cover in the talk, for example, we applied later credits to two large examples, and did logical relation to reordering, including reordering some stateful operations for promise data type, and did logic, atomicity proofs. As a quick side note, logically atomicity is how you prove linearize ability in iris,nd when you do that in iris, it's easy to get in tricky situations, we could simplify those we tackled a challenging counter, and we were able to simplify the notiom of logical atomicity and Iris improve the soundness of the entire mechanism and as I mentioned earlier we develop extensions and also discuss what mean, because you can generate more than one later credit per step. Bbnd with that, I want to conclude as I promised in the beginning, Later credits and the right to limit a later into an honorable resource which is subject to all the traditional separation logic, reasoning, as we've seen in this talk, we can pass them around via pre and post conditions we can across function calls, and can even put them into invariance. If you want to use them, they're now part of Iris by default, and they have already been applied by multiple ongoing projects.
Thank you for your attention, and I'm happy to take any questions you may have.
[APPLAUSE]
>> ILYA SERGEY: Thank you, Simon, we have time for a couple of questions.
>> Thanks for this interesting talk, there is this paper called the clocks are ticking no more delay. If you enter a later modality, what you get in the context is a tick, which sounds a lot like the later credits you talk about, and so it's not usable as a resource. So wonder if you provide intuition of the semantics of the credit and why they behave like objects.
>> Yes, this basically goes to how the entire mechanism is proven sound, if you really want to know, find me later and can discuss in more detail. The simple answer we observe in Iris and most step logic relations.
In the end matters you only look at final execution of length N, and then you only care about the total number of later elimitations, not where exactly in the program they are used.
And realized this and said, maybe we can use a resource to basically control when... how many, but not exactly where.
And we developed new modality, and new version of update modalities that allows whenever you have such a credit to spend later credit there and do careful tracking that you didn't exceed the maximum amount of later credits you were allowed to spend.
>> More questions?
There is one from Klaus.
>> So... you talked here about having generation of later credits before you spend them, have you looked at all to the idea of later credits negative, where you spend them, and then generate them afterwards since as you say you only care about the total number not about where they exist. .
>> That's a very interesting idea. We have not thought that much about it. One thing I would be worried of, I promise I will later provide you with later credit here, and then could easily get into the cycles you say I promise I will later on give you later credit, and promise I'll give you later credit and build another cycle in the proof, but maybe you could do something, interesting direction to explore in future work.
>> All right, slightly behind the schedule, let's take the rest of the questions offline, and thank Simon again.
[APPLAUSE]
So our next speaker is Klaus Ostermann of university TÃ¼bingen and he will be speaking about introduction of elimination of rules and programming languages.
>> Thanks everybody, this is work of...
All of which except Paul are also here at the conference, this is a talk about the...
about program composition, and more specifically about the structure of typing rules, and to explain the title, and make sure that we're all on the same page let me remind you of what left right, introduction elimination refers to. It refers to the structure of proof rules, or typing rules. And refers to whether the main logical connective we're talking about is below the bar or top of the bar. And difference between introduction and elimination rule, or whether the connectionive shows up on left or right, left and right elimination, left and right rules.
Okay, so... the setting that programming we're most used to.
Right introduction and right elimination rules, that's the basis of natural deduction, lamda calculus. And so forth.
That's by far the most studied kind of rule system.
The Sequent calculus is very common variant, we have introduction rules, left and right. In this talk we're interested in all these four Calculi, including the possibility to have only left introductions and left elimination rules, or only have elimination rules. So why are these interesting?
Well, we think they're interesting because each of these Calculi implies, a different setting, a different kind of programme structure.
So for instance, we have natural deduction, that we're always computing with an implicit output with nesting of expression, and inner to outer.
And... so symmetrically dealing with implicit input, that's very symmetric, if you have sequence calculus we have consist of constructers only, and have computation from the outside to the inside. We have a statement or command, and that's the way the computation goes. Maybe strangestrangest elimination, we have super formula property and computation is more inside out.
Okay. Let's make this a bit more concrete, and let's look at some examples. So for instance here, program that swaps component of binary... using natural deduction write rules, this is something we're all very familiar with. Those of you who are familiar with computational sequence calculi will also recognize how we can do this using only introduction rules in something like the lamda...
calculus, there we have these double bars, they denote demands, and on the left hand of the double bar we have... and on right hand side, consumer term, and here we see this match that's the left introduction rule of the sum, and we match itit... we put it against the input, and the bodies of the pattern matches, and there we have, again, commands that send the result right X to the output alpha. So this setting we have name output, no longer implicit output but we need to give every output a name.
Okay, let me show you another brief example to illustrate the influence of this rule choice on programme structure.
So here we are considering computation where we have product type and we have X inside and we need to navigate to the X and want to produce sum type as an output and need to inject the X into the sum type alpha.
And here... we see maybe most familiar form of how we do that in natural deduction, using the right rule, so out are the right elimination forms of the product, and injection into the sum, and gray... I put the output there, because we have this implicit output and put it there just to make it clear.
If we do the same using introduction rules, then... we can no longer use right rules...
sorry, elimination rules of course.
And so instead we have to use left introduction rules of the productproduct. The consumers of the X. And then, inside... you can't see the cursor. Inside we need to reify the currently consumed value using the mutator operator, the name of the currently consumed value, and then we can use the right introduction rules of the sum to construct the result and send it to alpha.
One interesting thing here is what I want you to watch out for is the sequence of the indices.
Here we have 2-1, and here we have 1-2. So the order in which we traverse is kind of reversed from inside-out to outside-in.
So using only left rules, this becomes a little bit simpler.
Note that I'm overloading the names in and out here for reasons that we will talk about later.
So in interaction rules, and elimination rules, we use for both products and sums. What I want you to notice here, here we have again a different order.
1-1-2. As opposed to 2-1-1 in the line above. And finally if we only consider elimination rules, we have the 4th possible way to sort these indices.
If we look at the resulting program structure we see that these four calculi correspond to four very different ways to structure the program to compute from the outside-in or from the inside-out, I think this ties nicely into recent discussions about how to design programs, and coprograms, and discussions about how to teach programming, and whether they should follow the structure of the input, or structure of the output and so forth. And I think these things are very clear here.
Okay, so let's get a bit more formal.
So we have developed calculu, in which all the rules can be expressed and put them all towing, or can only use a subset of them, and core rules that are on this slide, that are just standard rules from the bar U TATer calculus. So we have three forms of rules.
Sorry... cursor doesn't work.
One form of rules for producer terms, one form of rules for consumer terms, and then on the right we have the cut rule, which cuts producer and consumer of the same thing together.
I guess many of you have seen these kind of rules before.
So let's look at an example...
in this case, some type what the typing rules look like. So we have familiar right introduction rule for +... the right Elimination Rule is pretty much standard, except the bodies of the cases that they are commands instead of producer terms, so CI instead of EI.
And the reason for that will become clear soon.
It's trivial to encode the traditional case so the command is strictly more powerful than just producer term, left introduction rule basically like the right elimination rule, except we don't have term to discrimination, that happens on the other side of the cut. And then we have left elimination rule that then takes this form we have consumer of type T1 + T2 and then we can project out of that sum.
Okay so one of the interesting results in our work is the notion of what we call Bi-expressibility. That's a criterion to make sure all the rules are with each other. And specifically means that encode each rule diagonally opposed rule, using only diagonally opposed rules, and core rules.
So for instance, right introduction rule can be encoded withwith... and the out I left elimination rule. And one thing that is important here all these encodings no information is ever duplicated or eliminated. And same works for all the other rules. We can always eneach rule by diagonally opposed rules. And here you see the body and case match why they need to be commands and not producer terms or consumer terms because otherwise the diagonal encoding wouldn't work.
Okay, another main point of our paper is the usage of what we call the proof refutation duality. The basis of duality was paper from Tranchini, 2012.
And natural deduction style calculus, Logic calculus is not about programming. It's only in the logic domain for reputations and what's compelling about this work calculus that looks exactly like the natural deduction, calculus that we know of, except that all connectors are replaced by their respective rules. This is relation rule refuting junction. Introduction elimination. And looks exactly like standard con junction rule, and so this gave us the idea that we could have a kind of polymorphism where we interpret every proof of proposition also as refutation of dual proposition, so you have the ability to interpret terms polymorphically, with respect to proof refutational, producer consumer.
>> So let me show you an example. So from the typing rules for plus that we just saw, we can mechanically derive the typing rules for this product type.
And here we see now why we have chosen the same constructor rules, then we can interpret the same program in two ways, and we see that it works all very nicely with all the typing rules, they are exactly duel and can use exactly the same terms.
I think I will skip over the positive con junction and negative disjunction, because I want to use the remaining time to talk about something else.
Namely, we have developed practical -- somewhat practical programming language that we call Duo very much inspired by this work, and see a screenshot hereof where we translated the program from above to this language. It doesn't have... it into force to arbitrary data type and arbitrary code data typeseneral pattern matching and pattern matching. And we can use these combinations of rules together. With regard to producer, consumer polymorphism.
Having first step towards that as well, but wanted to show you a brief example, here is a function written in Duo. And what we can do in Duo, we get a quick fix and say dualize this term and it will awesomely generate copierce law. The exact SDUL of the program, that's a very ad hoc way of reusing the same programme. But at least first toward the idea.
Okay. So let me conclude.
We have identified these four equally expressive subcalculi corresponding to different programming styles, and can could exist and interact, and new idea of consumer/producer polymorphism. You can try out our language tool which is very much unfinished, but if you still want to write this, you a better ideawould be to ask us at the conference would be to ask us at the conference . Thank you.
[APPLAUSE]
>> Thank you, Klaus. Time for one quick question. Let's go there.
>> I guess so in your language you have some form of type checking algorithm, and I'm thinking in bidirectal type checking community there is kind of this thing about the fact that you want elimination forms to infer times, and constructor forms to check against the type.
And I think it's not clearly understood why this is the case and might be other possibilities, and wondering if you have seen these kind of things implementing your type of systems and if you had any intuition on this kind of question.
>> I think I'm not sufficiently familiar with by-directional type checking. But I would glad to talk to you in detail about this.
>> Let's talk at break thank you.
>> So rest ofs questions, and personal demos, let's take them offline and thank Klaus again.
[APPLAUSE]
>> ILYA SERGEY: So the next talk is on the topic of normalization for feature style model calculi.
I have to note this talk presents a paper distinguished paper at SACP, and pay attention. And speaker...
>> Hi, I'm from Chalmers, and I have done this work with Carlos and Fabian, and distinguish paper... warm welcome to the community, and let me get to the point. And normalization for Fitch-style normalization.
>> So what I do know here at the least modality is type some properties and operations. And we're going to commit to special class of modalities, specific the necessity modality, and the operations that this modality is equipped with is the so called necessitating rule which says if you ave a closed value versus long term, basically a way of saying you don't have any free variables in your term of type A, and you can box it up as a template boxes. And you also have the so called axiom k which is constantly box are going to squash them together and you get a box result. This is basic foundation we commit to. But not enough. It also depends on the application of interest.
This box, depends how we interpret this box. Many ways to interpret it. One way think of pure value in impure language or secret value in environment of flow control. And dynamic value context... and each of these these applications demand different operations right you don't always want the same kind of operation. For example, just a so called axiom T. Which is basically in our sort of head positions as types interpretation. It's an operation which takes the alue of a box A to A, you don't want it if you want box A to mean secret right this is basically saying leak the secret you don't want that. So it really is the question here is what are the operations that you're interested in, and sometimes you're interested in different things, and mix and match them you end up in different systems and logics and have to study different systems in isolation.
Not necessarily separately, somewhat together but while respecting differences, So today I'm going to talk about these four systems but they may be some beyond the four I'm going to talk about. We're gonna stick to these four, three, and four are basically found by different combinations of these axioms or operations. And going to talk about normalization, today. And why should anybody care about normalization.
And so a fundamental interest in normalization, even if that doesn't sort of appeal to you.
The other way to look at it perhaps, you have big equivalence semantically equivalent complicated terms, and want to crunch it down to one ing canonical representative, so that you can talk about this entire equivalent class using this one canonical representative matters, because instead of dealing with the noisy terms in that you can now look at these little tiny normal forms which are very well behaved, and you can prove a number of useful things using. So in the paper this example hey look you can have a lot of examples, you can look at them.
And so it's about constructs that help you construct and eliminate the model. And we're going to look at specific way of formulating modal calculi, and extend the notionof lock operator, and thought of delimiting operator, so you have typically have a list of variables in your context and also have this lock kind of getting in the way that separating your list into a sub lists. And the fourth system that I mentioned earlier has has a uniform introduction if you have a context with a lock in it. And this is a term of type A, you ... so what is different is the elimination rule. So four systems have four different elimination rules, and each of them have... this is why the lock really comes into the play you use the lock to carefully formulate how the elimination rule should work, before we get to specifics a bit of experiment, look at rules in fronts of us, and think about implementing substitution, when I say substitution, basically what I mean is I'm going to give you a list of terms for the variable in the terms. And I wants you to implement case for unbox, if you look at this what is going on there is a lot of stuff going on, probably looking at this and going "how am I going to implement substitution, have to slice and dice things, and if your brain is going...
hold on to that fear and hopefully will convince you it's a good idea what we're doing.
So... tedious and syntactic reasoning, even if you may do it once. What will end up happening, is you say, okay, I'm happy to do for one system, but then repeat for each and every system of interest, that's the problem, and difficult to mechanize normalization, and to implement normalization algorithms and very difficult to say straightforward induction, you can not do that any more.
Have to sit and do the work yourself, and appears to be ad hoc technical device. It has elegant semantic interpretation and thus intuition, and we're going to do normalization in semantic way, and specific we're going to do this by technical normalization, by evaluation, and this technique has two critical components, right?
And so the first component is evaluator, or interpreter which is function, and takes the terms in language, and to some semantic domain, so open terms, and think of semantic values being function, some kind of function, that takes interpretation of context, and takes what was produced and invert syntax to normal form.
And The trick is of course and how do you how do you actually implement them, but before we get there, just observe here to here to the normalization function. Once you've implemented these two functions, doing all the hard work, normalization function simply falls that just like composing them right so that's the main appeal. But of course the critical question when it comes to implementing what is the semantic domain, what is the interpretation of context, what is the interpretation of the type. And what is this fat arrow that I haven't told you what it is about So this basically is asking the question of lambda calculi more.
So I'm gonna tell you a story in two parts right, the first we're going to sort of demystify what is always, you can identify the so called possible world semantics of this fits all model . And then the moment we've identified that all we gotta do model specification and you end up getting models, and the models, almost free. Right, so that's how the story goes. So the possible semantics is defined, or thought of the following way, so going to interpret our terms in so called frames, so what is a frame, the frame has 3 components, set of nodes, in this case... this W here is basically a set of words, these are the nodes in the graph, and inbetween the worlds you have two kind of relation, RI for intuitionistic accessibility and RM for modal accessibility right. So you have these two kinds of edges between these graphs, right, and how we're going to use these graphs or frames we interpret type box A at the word W, and interpretation -- model success of original work. So this is interpretation. Get around to what exactly the intuition behind this is. The interesting bit of the interpretation of base types and function types and all the other types you typically have remain the same.
Which can be... you can have a look at the paper if you are interested in that. The interesting question here perhaps, what is the interpretation of the curious lock, we interpret the context of other stuff in usual way, products, what do we do about lock, here we revisit the intuition of box. So you think about box of statement about the future, and ignored the, intuitionistic blue relation here because it is a reflexive relation to for the time being we can sort of ignore that. So if you can think of box is a statement of the future, .
If that's the intuition you are going for. It's a statement about the past. It's as if delta lock, holds it is world W and exists, you where delta holds and this is basic intuition that lock is giving us, and this is what we're going to use for interpretation. And delta lock, holds W, it means it holds... going to write this down.
And once we given the interpretation of types, must also give you what this means in terms we set up. And we have to interpret terms in models as well. And it gives us very intuitive way of thinking of the terms, and what happens over there, if you have formula type box A RNGS you can open the portal into the future and can import value of type A in your future world.
And lamda calculus, corresponds to elimination rule. Which says if you have a term of type box A, you open this portal by marking that in the context adding a lock to the context, and then you get the term unbox the of SA this is how it's defined. But of course, you might have seen some extra restrictions. There too just because you're in a future world doesn't mean you shouldn't make any further assumptions that tried this recipe contest yeah you can add, you can, you know you can make further assumptions in this future world if you want to prime sticking around after the lock so that's the justification for one of the unbox elimination rules, the corresponding export tool the national production says, ...
these boxes instruct subordinate proof. You can close it up, and export any formula type A outside as box A and lamda calculus corresponds to introduction rule, term type A, lock in the context, and get a term of box A, and that's the introduction rule for the calculus. And of course the whole point of these portals or strict suboarednate proofs so you don't copy formulas back and forth, defeat the whole purpose of the box, and can not copy things, and restriction, captured by the variable. The variable rule says you may refer to any variables to the right of the latest lock, anything to of the left of the lock, means it's in the past world, so you cannot refer to it That's the idea.
That's the restriction that the variable was baking so enlightened so that's the end of the possible but the part one of this story in line to the semantics that we've set up, what we're looking at now unroll the definition of the of the fat arrow, so basically interpret the term in the function in all worlds you have interpretation of gamma and A, and possible world SPE Manitobaics now, and normalization of course remainses the same, curious question, we defined the possible semantics which can be thought of interface, now have you interface, and want to, instantiate the parameters of this interface so that you can get a claim a normalization algorithm And we know just from existing literature for the simply typed lambda calculus, you can achieve this by instantiating the set of words to context and the intuitionistic accessibility relation to order preserving embeddings, which can also be defined in an equally, it's almost the same way. And now the interesting question here is what is this model accessibility relation. That's the only remaining bit for us to instantiate interface, and evaluation function, given an intuition for how it's implemented but the moment you pick the right model parameter model accessibility relation.
I'm not going to show the implementation here. So what is this relation is the question we're after, to understand this I want to recap rules from before, one introduction rule, and elimb makings rule depends on terms of interest. But look at rules, you see all of them, something interesting going on, all of them are extension, if you look at the context below.
Extension of context above the line with some side constraints all of them have this form, simply going to say context in conclusion is in some relationship with the context in the premises, and going to write this down, the moment you write this down, you have the possible intuition observe that this is basically saying if you have term type box A, and holds gamma and delta... successor of gamma, and arrows orange arrow to Delta, and now you know that a must hold there, just because that's the definition of box here right in model. So how we're going to read the unbox rule. So this basically what this is basically saying is that now you can look at the syntax, that we're dealing with, looking at the relationship between the context and the conclusion contact impairments, and use this to instantiate the model parameter relation, model accessibility. Relation. So, and that's exactly what we're gonna ddo related to delta for this, this is defined as the following. It says, delta can be factored as an extension of gamma with and decide condition which just before we do the exact same thing for the other.
As for system, extension of gamma and reading off the centre out wards the relationship is.
And corresponding accessibility relation. And so what really going on, it's reflexive and trancetive, expected frame conditions in model logic, that's the reason this is all coming together, expect the frame conditions for the axiom info and can do the exact same thing for the other two system, it still works out. And you can implement by instantiating in that way. So far have been talking about the four systems, right. So curious T and 4axiom that gives interface, or some restrictions of it. And interesting to ask questions, what is... there are questions you can ask beyond the specific axioms and operations we studied today, and road ahead exciting, talking about return and join, and what about interactioning modalities. The richer the modalities the more interesting the applications get. If you think, I want these applications if you can let us know, that would be really nice, and work toward the application, so this is what kind of happening next up. To summarize in nutshell the message is the following, normalization, by Fitch-style modal calculi can be achieved constructed NbE models f instances of the possible world semantics avoiding tedious syntactic arguments based on reduction. Nd the possible semantics, isolate all the modal differences to one specific parameter,
>> Thank you, we have time for questions.
>> ILYA SERGEY: Okay, Ron has a question.
>> Thanks for the talk that's really interesting. I guess related to the next step talking about interactions of modalities, do you have any comments about diamond. Any insights about diamond as this was mostly about box.
>> O we have insights into diamond from the world of intuitionistic modal logic right so what we're really we're taking these are all these are amazing work done on intuitionistic modal logics, and now we have a computational interpretation over this logic so witnesses are lucky, fortunate moment of I think the coming together. So we do not think about the logical interpretation and their semantics in the possible world semantics, there is a computational interpretation is not something I know anything but. >> Thanks.
>> All right, no more questions, let's thank.
[APPLAUSE]
Nachiappan [APPLAUSE]
>> Okay, so the next talk is on functional multiparty session types, certified dead lock freedom in framework called multiparty GV. And going to talk from Radboud university.
>> Welcome to my talk, this is joint work with Stephanie and Robbert, he's here but can not see him. So this talk is about message passing. Usually in language like go and rust. Have a channel type in roster, there's a split into receiver and sender. And on those channels you can send messages of have a fixed type. But session types are a little bit more advanced, they are more flexible. So the type of the message that you send out a particular moment can depend on the state of the protocol. And that state can change as you send more messages or receive messages. And this is handled by linear types. So let's look at a simple form of session types, binary session types. Here we have binary channel variable C, which has session type, and says we're receive that message, and receive natural number, and type will change, and after that you are supposed to receive Boolean and then send natural, so binary session these come in pairs, and other side of channel has dual type all the sends and receives are interchanged.
So we have send and receive, and also have form of choice, one party decides, it sends a Boolean orbit saying I want to go to left side of port call, or right side, and the other party has to deal with that choice, we also have recursion to deal with with infinite protocols. And interesting point of binary session, Charatoniks ordinary types, so you can send as messages over other channels, on the other hand, multiparty session times, which there are no two participants, but more than two. And in that case, we annotate these types with emphasis and the indicating, with whom you're communicating so for example in channel c zero, that is communicating with participant one. As far as sending a natural number, and then it's receiving national number from participant 2.
There isn't some kind of rule about these things consistent with each other, and not just send receive match, but that they are done in such an order there is no dialogue. So a lot of research done about session types, but here I want to focus on two families of language, GV family on one hand and multiparty session types family.
Main difference GV has binary session types, and multiparty language has multiparty session types, and you may think one is more general THANT other, but that's not really the case. GV, you get that freedom, that main... people are interested in. Duality and linear typing, and then configuration of processes look like a tree, and this tree shape is I couldn't get predom, So here each vertex is a process, and align indicates a communication channel. On the other end with multiparty language you have fully interconnected topology, and then ensure global consistency check.
This means GV family of language you can have dynamic spawning, and multiparty language you usually have one static session, at least if you want to stay in the dialogue free fragment.
GV family of language you also have channels of first class values, and multiparty languages you usually do not.
And GV, you have -- it's all embedded in functional language, where as multiparty language usually in pi calculus, this work we want to combine the two to get all the strengths of the left and more expressive types of the right, and call that combination MPGV. What does that look like?
And how are the processes kekked. Still have a tree. But now in the tree are these small sessions with multiple participants, and in each session, freedom guaranteed by global and local session check, and globely over the whole session, developed freedom insured by the tree structure.
So the main contributions design of the language, language which you lamda calculus, and multiparty message passing channels and dynamically spawn, and first class values, and we have recursive types and choice, and other features.
And... using this we can see that in our language, we really generalize the old GV to do that we have to add one additional construct that we see as useful for modular programming.
So the key property here that all session time peep are all interested in. Type check you guarantee dialogue freedom.
And that occur here, receive a message, you have to wait until the other side actually sends a message and that creates danger for dialogue, and here, if that program type checks, that can't happen. And method here is all mechanizedmechanized in fork. So let's look at what the main constructs are. And here we have a fork.
And this is an a-fork. And inspired by Carbone and what this does is combine channel creation with transporting, and essential for keeping invariant configuration looking somewhat like a tree. To this fork pass number of closures and into the closures a channel will be passed and fork also returns a channel, so what we require here is factor of session types of each channel are consistent in multiparty way.
And each has to return unit in the end, and standard linear unit that guarantees this actually consumed session type fully.
So here, you see picture of configuration at runtime, so the left you see this process is trying to spawn some children, and then, it just grows an additional session, but environment configuration looks like that is then preserved, here are simplified types of send and receive, so in the real system, you have this choice, a little more complicated but here do without choice, so sent has input as a channel, which we're now allowed to receive. And also a message we're going to send and then returns to continuation channel with the new type after send has been performed and receive the other way around returns the message that was sent. If you're allowed to send channels as messages that also has effect on configuration. So here we see two processes trying to communicate, and process going to send connection to other session, and that changed the shape a bit. In general this can be a little more complicated. The messages can be pairs of channels or list of channels, so the transformations is a bit more complicated.
Another thing in real system, we have a synchronous semantics that means there is also some interneediate state, so pictures not exactly like this. If you want to see details of that you have to look at the paper.
And so close is the last operation, very simple, you can only do it if your session has ended. And what that does, it just takes away all the connections of the session, and see, can get disconnected and on the other hand, you see the invariant is still preserved.
>> So we wanted to show this language is really more powerful than GV and you may think that's already the case, if you want the binary session you just make one with two participants but that doesn't quite work, because in this multiparty type system the types have these endices indicated we are communicating, and if you translate GV program to this language, you may end up in cases these indices don't match, with that we introduce the redirect operation, which redirect messages for one destination to another, that then allows us to change this into types, and important for modular programming.
And then, in the translation from GV to MPGV we need to insert this into particular places.
So... you may not know this, but the multiparty session type literature has problems, so here is a quote from David CastroPerezPLDI21. I think they were the first to mechanize anything about multiparty session types.
So they say: Unfortunately the more complicated the behavior is the more error prone the theory becomes the literature reveals broken proofs of subject reduction for several MPST systems. And flaw of decidability of subtyping for A synchronous MPST and all of which are caused by incorrect understanding of the A synchronous behavior of types.
So that's why we mechanize into Coq. So it's the language definition is about 500 lines, the proofs are about 10,000 lines. And this works within a small step asynchronous semantics, with treads and buffers.
So the simple theorem is safety nets as a truss don't get stuck except don't receive that love freedom says that globally, even received don't get stuck on each other. And we have leaked freedom, which says messages don't get left behind. Another problem which dot actually occur in practice is that you may not be mechanizing.
And so its... and the statement relies on some accelerate definitions, which are about 100 lines of code. O how do we know that this theorem actually said something non trivial.
So we think that you should always proven easy to understand corollary. Which would be, you know for sure saying something nontrivial, but can immediately recognize, if you prove this you have really proved something, in our case prove global progress, if you start with program, and steps to configuration, either configuration is empty or can step again, this is not a very powerful theorem, for example, if you have any threat just in the loop, this theoriom holds, but would be impossible to prove this theorem by accident, and can be stated in one line, and requires no auxiliary definitions, and sanity check on data freedom statement, and yeah, I think this is very important.
And so what's in the paper, so we have the full language definition, of semantics, asynchronous semantics, we have choice we have recursive types because of session types which can also be mutually recursive, linear types and unrestricted types. Which you can freely copycopyand discard. We have most party consistency and global types, if you know what that is.
We have dead lock freedom proof and pictures, formal details with separation logic, and encoding Giulio and MPGV, and all the theorems mechanized in Cog.
So that's my talk.
Thank you for listening.
[APPLAUSE]
[APPLAUSE]
>> Thank you, we have time for questions.
>> Thank you. That was a beautiful talk.
I'm wondering if you have thought about how difficult it would be to embed this semantics in existing programming language, or whether it is something that really requires a standalone language to work?
Like, could it work as an EDSL with sufficiently expressive type system, and if so what features would you need?
>> So I think... I think for normal GV that can work.
But the problem here is that this consistency check that you have here on the fork, I think that's really hard to express in an another type check. I think people have done something like this. But I think to do the most powerful versions I think you would need probably some dependent types here. I mean, I'm sure in Coq, or... you can definitely do this. In Java, very much doubt it.
>> Questions?
>> Let me ask one.
You mentioned that you proved the log freedom and separation logic. CSL doesn't give you...
separation lodgening, what did you use to prove that result.
>> Separation logic little bit nonstandard. Separation you do logic argument to prove something about the type system.
We don't do this, half way between preservation subproof, and traditional proof you do in separation logic.
So we rely on Iris. But I created a linear version of Iris. Normal Iris is F-separation logic, and here we needed linear version, actually you have to use the whole channel, if you just throw the channel away, the other party will be waiting and that could be dialogue, and had to create our own separation logic for that.
>> ILYA SERGEY: Thank you. Okay.
If there are no more questions, let's thank Joe.
[APPLAUSE]
So the final talk on the session of logic is based on the GFP paper with a title that google keeps trying to correct wherever I enter it. And the speaker Klaas Pruiksma,
>> I'm going to be talking about futures today, and hopefully that some of you there is alattorney nat way of handling programming than what's commonly done. And this is joint work with my former adviser, Frank Pfenning at KARN gie Mellon University.
So first, I'm going to briefly define some terms, I know that some of this can be controversial at times, especially what people call concurrency. For the purposes of this talk, I'm going to call a computation concurrent when multiple parts of it and I'll usually call these threads or tasks, independently. This is independent of whether it's actively run at the same time or time switching, or something like this. And of course concurrent programs potentially can be run in parallel, running multiple of the threads, at the same time, and this can improve performance in various ways, either just constant factors are also the asymptotic performance.
Now there are always some difficulties with concurrent programming and current programs are notoriously difficult to write and get correct and also notoriously difficult to reason about, and introduces all sorts of new opportunities for bugs, there's this nice write up about real world concurrency bugs and highlights, really a huge number of different problems that come up because of concurrency. .
So in languages for con surgeonsy, one main common approach I've seen, and probably something most people agree on.
There is sequential language that gets started with and con currency primitive added to ENL able working with, concurrent programs, despite the underlying language being sequential and there are lots of different types of primitives that can be used. And these put some sort of restriction on what programs can be written with the goal of making it easier to work with easier to reason about that sort of thing. So for instance all the threading libraries, you might use in C or libraries for fork join computations functional languages and futures which are focus of this talk.
So in case are you not familiar with them. I'm going to briefly talk about what futures are.
For this talk, I'm going to be using this X left arrow key, P, semicolon Q, sort of imperative style notation, because I think that it's fairly illustrative of what's going on. So this construct is going to allocate a new memory cell X going to create a new thread that will run P and run queue on the current the one that existed beforehand and pee on the newly spawned concurrently. The idea here is that this is going to runrunP and make that available Q, and language we developed for this. Will eventually write to X, of course, you could imagine if we had recursion, P might be entitled to write to X but not actually get there unnecessarily and Q isn't tied to read from x if it wants to. That's might also get That's might also get called in some contexts. P is providing a future in queue is touching the future to read it there are all sorts of terms for this. One thing that is critical about futures compared to some other methods, allowed to depend on the result of p and this enables something called pipelining which we'll discuss a little bit in the future. So first a simple example to just highlight what exactly the difference is between futures and some of these other systems.
Everyone here has probably seen the map function many times, the standard example to get used.
Here is usual functional implementation. And here using the same notation writing before, more imperative style we talk about destination of results get stored into. And I'm going to focus on recursive portion, and say we write result of calling F on X to some location, Y and write the result of mapping F over the tail of X to new location, YS. And finally write con's of Y and YS location to return the map list, to do SKWEKSly we do this step one at a time, and first evaluate FX, and map XF, and then finally write console into the D, if instead the arrows treated as future, we start evaluating FX, and immediately write the concept Y. Y's, into the D. And return immediately, and this can be used even if the map hasn't completely finished yet.
And so now this gives a good example of pipelining, and process data series of stages one after another, potentially in parallel, and sometimes it's nice to think about pipelines, and more of a sequential sense for reasoning.
So for instance if we wanted to map multiple functions in sequence over list we could compose the map functions itself first map F over the list L, and G over the resulting list FL. If we implementment this using futures it's suitables for the second map of g over the list to begin doing work for the first map finishes, so we can gain some speed up there potentially, and also in more complex examples which, honestly I don't fully understand but the algorithms people say that it works. It's possible for improve constant factors but also asymptotic company. So there's these algorithms for parallel mergesof trees, or various other data structures, and speed up the product of sizes of, product of the log of the sizes of the data structures to the sum of the sizes which is a pretty good gain. Oh, there's a final piece of background. The futures are far from the new idea I'm not planning and claiming to have invented futures they originated in 1985 Well, before it was born. But there are a common thing that appears in commonly used in the real world so C++ has this standard future construct C sharp realities task II JavaScript sometimes calls these promises sometimes async await depending on who you talk to.
So, these are used all over the place we had no practical use cases. But the key problem that I see at least, and hoping to address with this talk, semantic needs are very ad hoc, especially in these cases, JavaScript semantics is rather nonexistent, and even languages trying to be more formal, the semantics of futures or other concurrency primitives tends to be sort of tacked on to the base language of sequential programs.
In this talk, I'm going to focus on the first of which is giving a logical foundation for future So tying these two two proofs is programs interpretation of a logic, and particularly, this will give us incidental the first of linear futures, and I also hope hat they'll give some justification for what I call a concurrency first perspective for Concurrent programs.
And there's also a variety of other contribution here only in the paper. In general this system that we developed, while it mostly works nicely with futures. We can also present lots of other mixed concurrent sequential programming approaches and these are all interoperable which is a nice feature. Okay, so to actually give the formalization of futures, we type in judgment that looks like this. Not just variables on the left-hand side.
But also a variable on the right hand side here that is the result or destination. So this Z is the location that we're storing the result to, and C here is going to be the type of the result data. And then subject P here is of course the program, and all of these variables on the left are thought of as addresses the process P is allowed to read data from, and type describe what that data looks like, and I'll also briefly talk about state of running program looks like in the system and this is going to be set of objects we have threads and thread AP, represented thread running program P, and A is the destination where P is eventually going to store the result it computes. And then also have persistent memory cells associated with the BANG free fix. And this is going to be memory cell that stores some data D at address: A.
And for evaluation, we'll formalize this by some rules that place subset of objects in state of program with different set of objects, so multiset recredit writhing rules if you are familiar with the concept, and type of role for futures looks like this. Semicolon Q construct, and we want to say that this eventually produces some ZF type C. Well, you should produce that ZF type developmentally, and we want to allow it to use XP is going to prodouchings here we have P produced this. X of type A.
Now, this is exactly the logical rule of cut from the sequent calculus, will look a little bit later at some of their connections, but this is the main focus here. Evaluation rule is quite simple, if we have thread evaluating future this can simply take step from two separate threads, one left hand side P, and evaluating right hand side Q. Spawned this new thread to run P and continue running for you on the original thread.
And, working with a Sequent calculus here, so we have left and right, introduction rules and broadly the rules are going to write to a future the left are going to read from a future.
And the identity or axiom rule is going to copy data from one location in memory to another or from one future to another. I don't really want to get into the details for time reasons though. But taken as a whole when we have all these together the language with futures, corresponds to a presentation of logic called the semi axiomatic Sequent calculus which is from recent work by some of my collaborators, and myself. And this is another facet of the correspondence between logic and programming. So, just like we can get from natural deduction to the simply typed lambda calculus via interpreting programs. Similarly, we get the samesame type of relationship here, which is why I justify as logically founded. One note, this definition ises with a variety of logics, so it's most natural to start with structural intuitionistic logic, but we can replace that with something different to get printed behaved type a future. So, notably, if weeUSZ lain iarly logic, we get a system of linear futures which have to be read from exactly once. And, again, I'm not an algorithms expert but I am told that when your futures can be used in some type planning algorithms to get asymptotic performance improvement over regular futures which is surprising but means that there is a very real use case for this. Now this language I presented so far. The only way to compose two programs is via futures, but creating new threads can have a lot of overhead and many of the threads in the language will do very little work before terminating, maybe write single value, or read something and immediately write something else, so we don't necessarily want to spawn a new thread every time, as simple solution to this we can introduce essential composition, or sequential cut operation, which write same way as futures, but little seq over the arrow.
Should be Court of Appeal the same as futures but behave UT slightly differently when it runs. So still allocate X, but instead of spawning new thread to run P, it will run P first on existing thread, and store result in X, and only run Q once P has funry run COMPLEESHGS, completion and store the result in x. And other than that evaluation or differences to the concurrent cut using futures but by writing some programs with this, we're able to manually choose where to create new threads or not and potentially improve efficiency.
It's also potentially possible to automate this in some way, but not really something we looked into.
Okay, so briefly some results:
We have some system of typing for sing processes and possible to extend to typing program states, and running program state, some collection of threads, containing processes, as well as some collection of memory cells containing data.
And we define a final state to be one that only contain sells everything has already finished writing there are no more running threads. Once we do this we can prove some standard results for programming language, and show progress that type state is either final, so everything written already, or there's a further step we can take. We can show a preservation result that the type of the state is preserved by compensating steps And we also have a result that the order of steps and concurrently running is unimportant to the end result. In conclusion this gives formalization of futures based logic, that comes very closely from the semi axiomatic presentation of logic, and we actually get futures that as a result of this. So this suggests to me that a concurrency first approach where we startwith the concurrent portion of language, and then implements a potentiality or synchronization afterwards is a viable and potentially even better way to reason about concurrent programming, rather than starting with a sequential language and some ad hoc concurrent features, and also by adjusting the underlying futures linear futures at the only ones that so far I know other use case for but there may well be find that I just haven't encountered yet.
When finally we can implement sequentiality for this hanging the evaluation order imposed by a scheduler, rather than having to have fully accept LAT sequence, of sequential semantics. So all of this can be done in the same semantics and then we just add a scheduling layer on top to decide what gets run sequentially and once it's run concurrently and it gets us back to roughly with setting up the usual mix sequential concurrent language. So variety of possible future work. Also ongoing Like for instance, adding and recursion to the language and trying to prove termination with some sorts of restrictions maybe only having inductive types, trying to give a sense of program equivalents so that we can get to eventually having dependent, language, with the eventual goal being to have a concurrent type theory for reasoning about concurrent programs more broadly. So this is 22/25. Can't get the references to be counted. But that's all I have for you right now. Thank you.
[APPLAUSE]
>> Thank you, Klaas. We have time for questions.
>> So I can see how this can be used to prove property of any particular program that uses futures. But if I understand your introduction right you were teasing like meta properties, or properties about future systems, or like, bugs in the system itself that would not be there if it was formalized. Can you give example of that kind of problems this could help with.
>> I'm not sure I fully understand the question, you see specific programs...
The results I had before about progress and preservation, of course holds for any program written within the system, right now not anything more general, but hope with the addition of dependent types we'll be able to... I guess be able to have more expressive program specificccations and be able to say the broader classes of programs still have the correct properties.
>> In the intro, you teased.
There are actually within bugs in the future system that wasn't there if only had the framework.
>> I don't know to what extent this work will fully love that.
But certainly can say, for instance, if you write programs in the system you won't come across deadlocks result of progress and preservation theorems, I don't know if that's a specific enough case for you.
The goal for correctness reasoning is so much to be able to automatically get correctness because I think that's impossible but it easier to reason about the pecific case, you can get correctness. .
>> Maybe you glossed over this in the slide, it's not clear toe many the future are first class values, seems when you create a future as soon as you reference to value you are actually waiting for it. You can't separate.
>> What do you mean by waiting for it immediately.
>> So... this computation rule quite helpful. From a thread that's trying to spawn a new future to two threads, one of which is running to evaluate into the future and one of which is potentially able to use it.
Is your question whether this is q is immediately blocked bythere being future A or something else.
>> My question is can you actually pass around X without blocking on it.
>> Yes, Q is able to... this isn't part of the language, you have to see the paper to see the details, yeah, Q is able to do whatever it wants to with A, that might mean reading it immediately, or doing some work and reading it or send to some other process, and that process can read from it, yes. Thank you.
>> ILYA SERGEY: Any other questions. Okay, if there are none, less thank Klaas again.
[APPLAUSE]
It's time for lunch.
