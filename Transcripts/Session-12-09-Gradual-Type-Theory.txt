And our next speaker is Kenji Maillard, and he's going to talk about reasonably dependence -- reasonably gradual dependent type theory.
>> Exactly. So this is a joint talk with Meven Lennon-Bertrand that you can find in this room, and Nicolas Tabareau and Ã‰ric Tanter, once again.
So I already had a really nice introduction to the topic, but let me try to pitch it my own way another time.
So my goal in this talk is to try to build dependent type proof assistant in -- which is easier to use. So I will use the same example of fixed size list, this time called vectors, which take an argument of base type A and specification of a size of a list that is given by an actual number.
We have two kind of vectors. The empty vectors that have size zero and the -- of a head on top of a tail of size N which is a size six or so of N.
On search vectors, we might want to define values kind of functions, such as map or filter function. A filter function is a function that will take -- predicate on to element contained in the vector. A vector of some length N, and that will try to filter the list, the vector by this predicate. Allow do we do that?
In the case of the empty vector, we just return ventie vector directly, and if you have a head on top of a tail, then we check whether the predicate holds of a head and -- recurs on the tail or keep the head on top of zsh on the tail.
This function filter takes an argument of size, the size we know statically, even by length, but what's the size of a result of filtering this list by an arbitrary predicate?
Well, at first we cannot say, we cannot give a close formula immediately. We have to think a bit about, wait, how does it compute, and actually the programme for computing the size of a result is that this size of a result depends on the dynamic value of a list that we get as input.
So in order to prototype efficiently independently typed language, we might want to say, okay, for now I will just put something, some unknown value that I will write question mark, and I will try to evaluate my programme and see how it works. Let's see how things unfold.
So one thing I will not -- we have to consider as implementers of independently typed language and a proof assistant is that adding this kind of question mark to -- in the type checking procedure. For example, if I have something which should be a size vector of size 1 plus some unknown term, and I want it to be a size vector of a known, so I need to be able to optimistically assume that these two things are consistent.
So that's the kind of thing that dependently a gradually dependently typed proof assistant should be able to provide you.
Now on such an example, how do we run actually the thing? Let's try to see the example like filtering two different lists. In the first case I'm filtering the lists 1 to 4 by the predicate even, and in the second case I'm filtering venti list.
In both cases we can evaluate the results of filtering this list, and here I can both cases type check because we have result of type vector of unknown length, which is consistent with the successive length, so that we can apply the add function.
So reducing the filter function gives us in the first case head applied to a list which contain only two, and in the second case, we obtain only the nil list.
Now if the first case we can further reduce the list that only contains two and get a result that will be two.
In the second case, however, we have a nil constructor, which is supposed to be in the type successor of some unknown type, and this should be incompatible with the fact that things in a successor vector, successor size should be of a head on top of a tail.
And so we need to enrich our language with arrows to handle this kind of impossible case that happens dynamically.
And so here I will get --
For prototyping purpose, I could say this second case where I'm passing nil is not useful, so I might want to give a simple specification that refine this unknown bound. For example, saying if my input list is nil, then the resulting size will be zero, and otherwise I will over-approximate seeing that the resulting size is unknown.
In such a case, the second example where we try to take the head of filtering an empty list, will not type check anymore, so we will have gained a bit of static information on which programmes type check, and so that's the motivation for using -- dependent types for prototyping purpose.
So how do we achieve such a dependently gradual dependently typed language? Well, we propose in previous work to separate this language in two sub-language. A first sub-language which is actually gradual and a second language which is -- and the gradual language elaborates the -- while it provides a computational meaning for the gradual language.
The cast contains unknown types and terms. Every type is also inhabited by arrow terms, and we have arbitrary test between pairs of types.
In order to control this test, a notion of precision between types gives us the information about how precise the information of the type that we have is. So the most precise types are the static types while where we lose precision we get to an unknown type, which is actually maximal for the precision order.
Precision between types, controlled cast by enforcing the cast are valid in the following sense, meaning that if two types, if A is more precise than B, then casting from A to B is an injection and it has a retract, which is a cast from B to A.
And the fact that we can interpret the notion of precision as such -- projection pair will be the fundamental semantic criterion to build all our language.
So now in previous work we also observe that actually trying to get all properties that we want from such gradual dependently typed language is not possible. We cannot have at the same time the gradual guarantees that we want, and normalising language, and conservativity of the base language that we tried to gradualise.
And in order to have a proof assistant, normalisation cannot be trained. We want to have normalisation to be sure that we can actually have a decidable type checking algorithm and that we can actually implement our proof assistant.
Moreover, we cannot trade the -- of the base calculus because we want to be able to re-use all programmes from the existing literature, from existing programmes.
So in this work, I will -- so we had three different variants in previous work that were trading the different possibilities, and in this work I will concentrate on this variant, which has both an ambiting of CIC and that is normalising.
So now I will look at the cast calculus, which has this property. So I have this cast CICN which is both normalising and conservative over CIC, however it is not globally gradual, and the main question that this work tries to answer is how far it is from being gradual.
We are not able to show graduality globally for every types, but maybe it might be possible for sometimes to show some gradual properties. And how do we do that? Well, we will actually internalise the notion of precision inside the type theory so that we can actually reason inside the type theory on the precision between types.
So we could GRIP the resulting theory, which expands this calculus with a notion of precision, actually two families of precision. One for precision between types and a family for precision between terms.
And these types of precision are actually proposition that we -- that inhabit a pure universe of proposition, PROP, and this universe of proposition contains no errors, no casts and no unknown terms, which make it a good place to have sound reasoning for the type theory.
So what does this internal notion of precision actually means? Well, we can explain what it means in terms of dynamic gradual guarantee. So dynamic gradual guarantee is saying that if I have a term X, which is more precise than term Y, and a context for values in type A, applying the context to X should be more precise than applying the context to Y at Boolean types.
And we have the dynamic gradual guarantee will hold for a context C if and only if this context is related by precision inside the language at type of Boolean predicate on A, and if and only if this context is monitored.
In order to understand concretely what this formula actually means, we should look at precision on Boolean types, which contain true and false, as usual, but also the exceptional values provided by the gradual language, arrows, and unknown. And we have a lattice where arrow is the smallest element and unknown is the biggest one, meaning that the dynamic gradual guarantee actually means some kind of error approximation.
Meaning that if my value X plugged in the context C does not error, then I'm sure that value Y plugged in context C will not error either.
So now that we have some understanding of this notion of precision, how do we prove actually in our language the -- how do we derive a proof of precision? Well, let's have a look at two terms.
The first term is a function from natural to natural but add one to a natural number. And the second one takes an unknown -- a term of an unknown type and cast this element to natural numbers and then add one.
If we are able to prove that the first static term is more precise than the other known function, then this will mean that other known will not have a role on good input, and how do we do such a derivation? Well, we start from the judgment that we want to prove.
We look at function types, and at function types precision is actually extensional. It maps elements related by precision to elements related by precision.
Now we will look, we will intro the argument in the context and do some computation to unfold the add one and add unknown, and we need to compare two application of plus one of successor, and successor is actually monotone, so we can actually just forget about it and show it's enough to show that X is more precise than the cast.
Finally, to complete this proof, it's enough to know that whenever we have two types that are related by precision, the ambiting projection properties will allow us to get that this cast is actually a right so we can use it directly to prove that X is more precise than C at the type not more precise than unknown.
So now this proof that I just gave is made out of some principles that we have on precision, such that trancivity, quasi-reflexivity, the add junction properties that I just used, and also an important property that allows us to characterise precision between types but are not directly related by precision.
On the other hand, we cannot have everything in our language. In particular, the precision on types is not the same thing as the precision on the universe of types, and we have -- this allows us to keep a normalising language, but it makes reasoning a bit surprising, let's say, and we have to use some explicit cumulativity in order to regain enough expressivity in our language.
Really quickly I would like to outline that we have some interesting metatheory properties on this language. It is normalising. It is consistent, normalising, and moreover we can carve out some fragment of this language where we can automatically derive the terms are gradual.
So to conclude, we had a proof of concept of this type theory in Agda, and we have a model that validates the theory that I just alluded to.
We have non-gradual operations in this language, so for example we have a catch operation on inductive type, but I did not have time to talk more, much about. And this paper can also be seen as a way to address the question of catch in that gradual world.
And we add precision internally in order to reason posthoc on the gradual terms of this language.
So for future work, we would like to have Alex from a gradual source. Right now we are working only with a cast calculus, and we would like to have more general inductive type, maybe an actual implementation from the source language, but this will have to wait for another time.
Thank you.
[ Applause ].
>> Okay, we have time for a couple of questions.
>> Hi. So the internal precision, it looks a lot like directed equality from, like, a directed type theory. So what -- is there a relationship with that?
>> So I think -- first I think that there are many directed type theories, so I would need to have a more precise reference, I would say.
And I don't think I saw any instance of directed type theory that was exactly coinciding with this, in the sense that so I didn't have much time to explain the notion of the model that we are using, but we can actually interpret this whole theory in a model with types that are interpreted as partial pre-orders. So it's not a full pre-order, like some elements are not created to -- are not reflexive of the relation.
But in that sense, it has some kind of label of directed type theory.
>> Yeah, and then I guess related to that, you said that the ordering for the function type was extensional.
>> Yes.
>> Do you have to add that as an axiom or is that derivable?
>> I mean validated -- so the source language has it -- part of it -- the reduction role on the precision at function types, and then it's validated through the model, like in the model, the precision at function types is actually extensional.
>> Thank you.
>> More questions?
>> Okay, and then also the -- if you go back, this last one, the composition through upper bounds, is that equivalent to all of the cast being retractions?
>> All of the cast being retractions. So the thing is that here the important thing is that it allows to talk about cast between types that are not related by precision.
>> Yes.
>> Like, it allows us to -- this plus some composition operation allows us to describe the behaviour of cast outside the fragment which is directly related by precision.
>> Right, this allows us to reduce all of the reasoning about casts to up casts and down casts.
>> Yeah, mostly.
>> But what I'm saying is in my experience this kind of property is exactly what we need the retraction property for, from there being embedding projection pairs.
>> I don't know if it is equivalent, but I'm sure that I am using it in -- like, the fact that I can quantify over all upper bound X is indeed the fact that you have a retraction.
>> Okay.
>> One interesting observation around that is that we couldn't do that through an intersection. Like, it wouldn't work with a lower bound.
>> Yeah.
>> Okay. I have a quick question myself. So how does this compare with previous talk, and in particular do you support identity types?
>> So we don't support identity types.
>> Okay.
>> We are able to support inductive types with indexes only when we have enough decidability on this, for example, forceable indices or things like that.
And otherwise, no, I'm -- I don't know how a general identity type would fare in this.
>> Okay.
>> And I propose we thank the speaker again.
[ Applause ].
Our third speaker for this session is going to talk about a totally unique account of enumeration.
>> All right, so I will. So let's start with the very simple question. What is enumeration? And such a simple question deserves a very simple answer, and what we can say is it's a list for Booleans. This is true and false. And the enumeration for natural numbers contains zero, suc zero, and so on.
And at some point the conclusion we have to draw is that this sucks.
[LAUGHTER].
And is there some automation we can apply here, and that's what this paper is all about.
So to take a step back and set the stage a little bit, formal verification is great, but it's very expensive, this is one of the lightweight methods that we can use to catch errors early on in the process to reduce this cost.
But the key challenge of this approach is to find the test data to instantiate these properties with. And what we do in this paper is we use a generic approach to exhaustive enumeration of these test values, and what this gets us is a uniform enumeration of the inhabitants of algebraic data types and indexed families, and we can write some generic proofs that establish some notion of correctness for these generic enumerators.
Right, so let's dive in to the meat and potatoes of this talk, what is an enumerator.
So as we saw in the introduction, the basic idea is that an enumerator is a list, but that doesn't fare so well for infinite data types. Instead we have this enumerator data type that essentially defines one layer of recursion of these enumerators. It has an assumption list A that contains the occurrences and it produces an enumeration of a different type.
And an example of such an enumerator is the enumerator for natural numbers that we can define as follows, which just asks to enumerate the actual numbers we have zero at the head of the list or we map the successor over the re-cursively enumerated values.
And then to extract something usable from such an enumerator we can write the function that iterates these enumerators up to a given depth bound.
There are alternative interpretations of these enumerators, like co-inductors, but I won't go into that during this talk.
So why not define these enumerators directly, you might ask? This is something I would just like to quickly point out for binary trees, for example, you would write the following bounded enumeration to enumerate the infinite data types, but the obvious problem here is, of course, that the number of recursive calls grows exponentially with the depth of the type.
And the current setup avoids that.
So if we say that enumerator is correct, what do we mean by that? There are two things, actually, that we are interested in here. The first one is completeness. We say that an enumerator is complete if all the values of the type that it enumerates will eventually occur at some depth if we iterate over that enumerator, which is captured by this complete paragraph of enumerators. And we have a notion of uniqueness that says that an enumerator for type A is unique if all of the values of type A will occur at most once, and the way we formalize this is if there are two proofs -- that point to a value X in the enumeration, then actually these proofs, they point to the same location.
And then as a final thing, as a marker, I want to say for certain enumerators combinators we establish a notion of fairness that ensures that you don't enumerate one constructor first and only then get to the other constructor, but if you want to know more about that, you should read the paper.
To consider a few examples of these combinators, the simplest ones are empty and pure, which construct enumeration with no values or just list a single value into enumeration.
And then there's these operations which you might recognise as haskal's alternative type class, and we offer a choice between two enumerators. They allow you to map the function or apply the results of one enumerator to the values from another enumerator, and then the final combinator that we use, it's actually my favourite one, is for recursive, and it's just the identity function.
So let's see some examples of this. So yeah, for Booleans, a choice between either true or false, or for natural numbers, a choice between zero or successor applied to a recursive argument, and hopefully that you see theory immediately is there is a very clear correspondence between the structure of these data types and the way that we assemble these enumerators, and this correspondence goes further, because if we sit down and write the uniqueness and completeness proofs for these enumerators, we find that the proof structure also follows the data structure.
And this makes enumerators an obvious candidate for generic programming, meaning that we will define these enumerators by induction of the structure of these types. So a quick note about the setup that we have to use here. Obviously an -- value cannot pattern match on a type directly, so usually how these things are done is by creating some data type whose inhabitants correspond to Agda types, which is usually called the universe.
And this is something we can inspect, and then you write a reflection function that reflects this syntactic elements back into Agda sets, and then the enumerator writes such a description, and it enumerates into the semantic description of these.
I won't show the definition of the descriptions here, but it contains sort of the standard combinators for empty unit types and is -- on the products and co-products, and you can have -- recurrences.
And then once we have this setup fleshed out, then actually the generic enumerator is a very direct correspondence between the different constructors of this universe of types and the different combinators that I just showed you.
This is just a direct mapping and for sums and products it's also very straightforward. Please don't try to parse this, but try to appreciate the visual correspondence that hopefully is clear.
So now that we have this generic enumerator, how do we go about proving its correctness? Recall that completeness, for completeness we need to show that iterating produces every value eventually, and uniqueness then we need to show that iterating produces every value at most once.
And as it turns out, to prove these things, we have to show lemmas that mimic the introduction and elimination rule from propositional logic, and there's a brief example of that here.
So this is all nice and good, but then the question is how do we move to indexed families? So the idea here is that this recursive argument that we had in the definition of enumerators, that we can lift that to a function from index to list, which gives us this thing, the I enumerator which takes as its first search argument, which stands for the recursive occurrences, takes a function from index to set, and the assumption that it makes is going to be a function from that index to a list of values at that index.
And the way to think about this is that an I enumerator evaluates enumerators at -- but they vary with some index I.
And we can define the exact same combinators for this data type, and the idea is then that the enumerators themselves are defined as functions that computes on the index. So in this way you get a sort of two-tiered lifting of the structure that you define for regular types to the structure of index types.
And here's just a brief example for finite sets, which if the index is zero is empty and otherwise returns a choice, and notice that here we give the smaller index to this recursive combinator.
And to adapt the generic setup to index types, we use a very similar lifting from sets to I to set, and actually the completeness and uniqueness proofs in the index setting are -- they go through using very similar lemmas about the combinators about the use to implement the generic enumerator.
And then finally, so I remarked at the beginning that one of the reasons enumerators are set up this way is that you get a sharing between recursive calls for free, essentially, but that doesn't work so well for index types because the recursive occurrences are now functions, and every time we apply these functions with a new index, they recompute the result.
And in some cases this leads to excessive recomputation of these recursive enumerators. For example, if you have perfect binary trees, where for every node you have two recursive occurrences at the same index, or for bit factors where there is also a lot of sharing of substructures at the same index.
For this we can use generic memorisation technique, by re-using the generic structure that we defined for regular types, and the way this works is that we define a generic try, a co-indetective data structure, whose shape is dpept on the shape of the index of this function that compute the enumerator, and essentially these data structures represent a tabulation of this function, and if you call this function with the same index more than one time, then there will -- the results will be cached.
We have showed that this is equilt to the non-memoising enumerators, and under certain specific circumstances you get a performance gain if you enumerate index data types, which is critical, I think.
And then in conclusion, just to wrap up, if you need to take away a few things from this talk, it's going to be that you can define generic enumerators in terms of these very natural enumerator combinators. That the generic proofs follow from similar natural introduction and elimination lemmas for these combinators. This approach works for both algebraic data types and index data types, and we can even define generic memoisation on the index if this index is given by a regular description.
Thank you.
[ Applause ].
>> Yeah, thanks. We have some time for questions.
>> Hi. Over here. I wonder, is there any way to extend this abstraction to -- or is it already a monad? A lot of times when you're doing generation for testing, you have some kind of more interesting things that you -- interesting conditions that you need to make sure your values uphold.
>> Yes, that's an excellent question. In enumerators and index enumerators they are both monads as well. For algebraic data types you don't need this structure, but for the index families you use you do need this structure. That's in the paper if you are interested in that.
>> Can you show us the formula for unique again? I was a bit confused. It looked like there was a saying that there's a unique proof that the element at a certain position, and I don't quite understand how that means that there can't be the same element at different positions. Maybe just reading it wrong.
>> Yeah, so what we do here is we quantify over all possible proofs that X is included in this enumeration at depth, and we say that the enumerator is unique for all these possible proofs, that these proofs are equivalent, or equal.
>> And that works even if you have -- over a single N?
>> Sorry?
>> And that works even though you quantify over only one N? I would expect . . . I can take it offline. Maybe I'm not seeing it.
>> Yeah, sounds good.
>> I'm trying to understand exactly how you deal with size pounds on enumerated values. If you think of small check, then small check generates values up to a certain depth, whereas lean check, which I think you refer to, generates terms up to a certain size.
And that gives you a much finer division of terms. My experience, certainly, is that if you use the generator to find bugs, then the size-based approach that lean check uses is way better at finding bugs than the small check approach, which tends to run into the -- you know, the exponential explosion problem too early.
>> Yeah, so just to clarify, we used the small check interpretation of size, so this natural number denotes a certain recursive depth, and you are absolutely depth that this leads to an enormous explosion in the number of values, and is certainly something to look at in future work, to use the lean check notion of size.
>> So just to -- a follow-up question. The other thing that I found is that the lazy small check idea, if you adapt it to make a kind of lazy lean check, that works tremendously much better. Is there any prospect for including that current lazy small check idea in this work?
>> So you have to correct me on this if I'm wrong, but if I recall correctly, lazy small check uses -- pokes some holes in Haskell's pure interface, perform MYO, for example, and I think it would be challenging to include that in a formisation like this. That's one obstacle that I say that -- yeah, maybe there is a way forward.
>> For index only, is there any restriction on the type of indices?
>> Sorry?
>> Is there any restriction on the type of indices?
>> No. Unless you want to use this generic memoisation fracture that we have defined, and in that case the index has to be given by (CORRECTION) structure.
It has to be given by this -- data types because the -- yeah. You need something to match on, essentially.
>> Okay, thank you.
>> Umm, so yeah, so you can enumerate the non-index types and the index types. Can you enumerate sigma type?
>> In short, yes. But sigma is a little bit problematic because -- essentially it works the same as enumerating products.
>> Right.
>> That you have to alter things a little bit because it's now dependent, and this alteration includes that you have to rely on the numeraletic structure of enumerators.
The way we've set it up is that this first element of sigma is essentially a constant, and then you have to rely on some outside knowledge that tells you how to enumerate this constant.
An alternative way to set it up would be to say the first element of the sigma is a non-index description, and then you can sort of piggyback on to the existing you numeration infrastructure that was already there.
>> Maybe one more question, yeah.
>> Thanks for the talk. Can this -- I'm not sure, maybe you already covered it, but can this technique be used to generate sort of enumerate terms of mutually recursive data types? Thanks.
>> Yes, if you're willing to -- into sort of an indexed data type, because one of the tricks that you can use is just have a sigma where the first element sets in which data type you are, and then the second element is the -- yeah, defines the different families.
It doesn't work with these index enumerators. For mutual Li recursive index data types. I don't know. Maybe the same encoding would work. Maybe you run into problems. That's something to be tried in the future, I guess.
>> Okay. Let's thank Cas again.
[ Applause ].
And this is the end of this session. Please don't leave because in a couple of minutes there will be a fireside chat, and I think Andre will tell a bit more about it? Or, yeah. Or you will.
>> If you have to pee, you can go. Otherwise you stay.
We are going to have a panel discussion, which I think is going to be very interesting. I'm not the session chair. I'm here just to entertain you so that you don't walk away. So I'll entertain you by saying that there is a -- don't forget that there are two events. At 7.00 there is a [no audio].
.
.
.
.
If you don't know whether you registered for the guided tour, chances are you didn't. Every single person who has so far asked me this question, I checked, the answer was you didn't register. I can keep checking, but experience shows that you didn't register if you have to ask.
But I'll check Jesper now.
>> Arvind, Guy, I can see you both.
>> Good, we can see you.
>> Can you say something?
No, can't hear anything from Guy. Are you muted? No.
No, Guy, no, nothing, Guy.
>> I disconnected my mixer from the USB port and plugged it back in again.
>> Oh, hardware error. All right. Good. So we are here!
Welcome, everybody. I'm Simon Peyton Jones, and I'm thrilled to have you at this rather unusual conversation, so Arvind and Guy Steele are two giants of the functional programming community. Their influence has been profound, broad and sustained over at least five decades.
So I'm really pleased that they've agreed to have this informal conversation, which was going to be around a crackling fire in person with drinks, but alas it's somewhat remote, object their journeys through the world of functional programming and their reflections of what has happened and what has yet to happen.
Guy was always planned to be virtual from the beginning. Arvind made a long and somewhat tortuous journey and then on arriving has gone down with COVID. I'm so sorry, Arvind. If you fall over sideways on this conversation, we will know what happened. Thank you for joining us virtually.
So I think what I'm going to do is I'm going to engage in a kind of conversation with Guy and Arvind to get them to reflect a bit on their journeys, and then save up questions. I think we'll mostly have them at the end, unless there's anything super urgent. You can do that either in person, I'll try and leave some time, or by Airmeet questions.
I'm going to start off by taking us back to 1978. That was an important year, for me, anyway, that was the year on which John Hughes and I were in our final year at Cambridge. We had been studying -- John, we both started studying maths. After two years I decided it was just too difficult and went off to do electrical engineering. John, of course, cruised smoothly on to his maths degree. We both ended a postgraduate year in Cambridge. More importantly for this conversation, I think 1978 was the year you joined the faculty at MIT, Arvind. Jack Dennis hired you.
I think that was the same year at which you, Guy, published rabbit, a compiler for scheme, whose cleverness I was still decoding at least a decade later. You would enjoy reading that thesis if you haven't.
It was also the year or close to the year in which John backs won the Turing award, and in 1978 he gave the lecture which was entitled can programming be liberated from the van annoyman style, a functional style and its algebra programme.
It was saying to all of us have no truck with this dross of imperative -- functionality programming is the future, and we shall define new systems to execute it.
Pretty exciting. That for me as an undergraduate, and Guy and Arvind somewhat more established at that point, 1979 I think was the year that -- Conway published their book called introduction to the LSI systems, which was a kind of democratisation of VSLI design and moved it was out of the ambit of just at intel into at least universities.
So I was wondering if we could start maybe both of you by telling us a bit about the journey that led you to 1978, what you were trying to accomplish at this time, and what programming language research felt like at that time.
Maybe we'll go back a bit in time and lead up to 1978. Arvind, do you want to start with that?
>> Sure. I graduated -- I finished my PhD in '74 in California Irvine, and my PhD was -- there is echo here. Can you guys hear it?
>> Yeah, but we can hear you enough. Enough? We can pass, yeah.
>> All right.
>> Keep going.
>> Good. So I think my thesis was on modelling programme behaviour for operating systems evaluation and so on, and by the end of it I was convinced that this was a focus idea because the models were just so, so wrong, that anyway, the main decision I took at that point was I only want to be involved in the design of real things and evaluation of real things as opposed to models of things that may be done.
And at that time I came across some papers by Jack Dennis on data flow for signal processing, and they profoundly influenced me, so I thought that was cool, and I said I'm going to do parallel programming and languages, you know, for the next 10 years of my life, which I did, actually.
And then I also came across John backess's thing, and I don't know why I read thiz paper, but there was a paper in Bubble 2 or something, you know, on his reduction languages.
I pursued both of them, but I couldn't relate the two ideas together at all. What John was saying and what Jack was saying.
But the most interesting thing for me personally was that data flow graphs meant kind of what Jack was doing was you're to draw these graphs, and I think I was kind of a vulture on this. I was drawing very, very quick graphs, and very big is all relative.
I tried to do -- in data flow as defined by Jack Dennis. It was lots of space, and I said this is no way to write programmes. This is too verbose, and even if you were clever with visual tricks, it's not going to get there. I wanted to do a visual language, so that's how Irvine did a full language was born, and it directly translated into data flow graphs, so compiler was not an issue for me.
And this was also the time when structured programming was very big, so ID had very few of the constructs.
So on the way to MIT, I happened to meet, visit Tony at Oxford, and he said explain me, what is this ID language you're talking about, and I understood very quickly that he was least interested in data flow graphs. He just wanted to understand the properties of this particular language. And I tried and I tried, and it was very clear to me that nothing was getting across.
I mean, he was just kind of shaking his head and, you know, disgusted, or something. I don't know. When he said, stop, stop, what is the purpose of this language, and I constantly replied, he said don't you think a -- language is expressive enough. And I said, whoops, where is this going
