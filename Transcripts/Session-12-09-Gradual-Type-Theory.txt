Our third speaker for this session is going to talk about a totally unique account of enumeration.
>> All right, so I will. So let's start with the very simple question. What is enumeration? And such a simple question deserves a very simple answer, and what we can say is it's a list for Booleans. This is true and false. And the enumeration for natural numbers contains zero, suc zero, and so on.
And at some point the conclusion we have to draw is that this sucks.
[LAUGHTER].
And is there some automation we can apply here, and that's what this paper is all about.
So to take a step back and set the stage a little bit, formal verification is great, but it's very expensive, this is one of the lightweight methods that we can use to catch errors early on in the process to reduce this cost.
But the key challenge of this approach is to find the test data to instantiate these properties with. And what we do in this paper is we use a generic approach to exhaustive enumeration of these test values, and what this gets us is a uniform enumeration of the inhabitants of algebraic data types and indexed families, and we can write some generic proofs that establish some notion of correctness for these generic enumerators.
Right, so let's dive in to the meat and potatoes of this talk, what is an enumerator.
So as we saw in the introduction, the basic idea is that an enumerator is a list, but that doesn't fare so well for infinite data types. Instead we have this enumerator data type that essentially defines one layer of recursion of these enumerators. It has an assumption list A that contains the occurrences and it produces an enumeration of a different type.
And an example of such an enumerator is the enumerator for natural numbers that we can define as follows, which just asks to enumerate the actual numbers we have zero at the head of the list or we map the successor over the re-cursively enumerated values.
And then to extract something usable from such an enumerator we can write the function that iterates these enumerators up to a given depth bound.
There are alternative interpretations of these enumerators, like co-inductors, but I won't go into that during this talk.
So why not define these enumerators directly, you might ask? This is something I would just like to quickly point out for binary trees, for example, you would write the following bounded enumeration to enumerate the infinite data types, but the obvious problem here is, of course, that the number of recursive calls grows exponentially with the depth of the type.
And the current setup avoids that.
So if we say that enumerator is correct, what do we mean by that? There are two things, actually, that we are interested in here. The first one is completeness. We say that an enumerator is complete if all the values of the type that it enumerates will eventually occur at some depth if we iterate over that enumerator, which is captured by this complete paragraph of enumerators. And we have a notion of uniqueness that says that an enumerator for type A is unique if all of the values of type A will occur at most once, and the way we formalize this is if there are two proofs -- that point to a value X in the enumeration, then actually these proofs, they point to the same location.
And then as a final thing, as a marker, I want to say for certain enumerators combinators we establish a notion of fairness that ensures that you don't enumerate one constructor first and only then get to the other constructor, but if you want to know more about that, you should read the paper.
To consider a few examples of these combinators, the simplest ones are empty and pure, which construct enumeration with no values or just list a single value into enumeration.
And then there's these operations which you might recognise as haskal's alternative type class, and we offer a choice between two enumerators. They allow you to map the function or apply the results of one enumerator to the values from another enumerator, and then the final combinator that we use, it's actually my favourite one, is for recursive, and it's just the identity function.
So let's see some examples of this. So yeah, for Booleans, a choice between either true or false, or for natural numbers, a choice between zero or successor applied to a recursive argument, and hopefully that you see theory immediately is there is a very clear correspondence between the structure of these data types and the way that we assemble these enumerators, and this correspondence goes further, because if we sit down and write the uniqueness and completeness proofs for these enumerators, we find that the proof structure also follows the data structure.
And this makes enumerators an obvious candidate for generic programming, meaning that we will define these enumerators by induction of the structure of these types. So a quick note about the setup that we have to use here. Obviously an -- value cannot pattern match on a type directly, so usually how these things are done is by creating some data type whose inhabitants correspond to Agda types, which is usually called the universe.
And this is something we can inspect, and then you write a reflection function that reflects this syntactic elements back into Agda sets, and then the enumerator writes such a description, and it enumerates into the semantic description of these.
I won't show the definition of the descriptions here, but it contains sort of the standard combinators for empty unit types and is -- on the products and co-products, and you can have -- recurrences.
And then once we have this setup fleshed out, then actually the generic enumerator is a very direct correspondence between the different constructors of this universe of types and the different combinators that I just showed you.
This is just a direct mapping and for sums and products it's also very straightforward. Please don't try to parse this, but try to appreciate the visual correspondence that hopefully is clear.
So now that we have this generic enumerator, how do we go about proving its correctness? Recall that completeness, for completeness we need to show that iterating produces every value eventually, and uniqueness then we need to show that iterating produces every value at most once.
And as it turns out, to prove these things, we have to show lemmas that mimic the introduction and elimination rule from propositional logic, and there's a brief example of that here.
So this is all nice and good, but then the question is how do we move to indexed families? So the idea here is that this recursive argument that we had in the definition of enumerators, that we can lift that to a function from index to list, which gives us this thing, the I enumerator which takes as its first search argument, which stands for the recursive occurrences, takes a function from index to set, and the assumption that it makes is going to be a function from that index to a list of values at that index.
And the way to think about this is that an I enumerator evaluates enumerators at -- but they vary with some index I.
And we can define the exact same combinators for this data type, and the idea is then that the enumerators themselves are defined as functions that computes on the index. So in this way you get a sort of two-tiered lifting of the structure that you define for regular types to the structure of index types.
And here's just a brief example for finite sets, which if the index is zero is empty and otherwise returns a choice, and notice that here we give the smaller index to this recursive combinator.
And to adapt the generic setup to index types, we use a very similar lifting from sets to I to set, and actually the completeness and uniqueness proofs in the index setting are -- they go through using very similar lemmas about the combinators about the use to implement the generic enumerator.
And then finally, so I remarked at the beginning that one of the reasons enumerators are set up this way is that you get a sharing between recursive calls for free, essentially, but that doesn't work so well for index types because the recursive occurrences are now functions, and every time we apply these functions with a new index, they recompute the result.
And in some cases this leads to excessive recomputation of these recursive enumerators. For example, if you have perfect binary trees, where for every node you have two recursive occurrences at the same index, or for bit factors where there is also a lot of sharing of substructures at the same index.
For this we can use generic memorisation technique, by re-using the generic structure that we defined for regular types, and the way this works is that we define a generic try, a co-indetective data structure, whose shape is dpept on the shape of the index of this function that compute the enumerator, and essentially these data structures represent a tabulation of this function, and if you call this function with the same index more than one time, then there will -- the results will be cached.
We have showed that this is equilt to the non-memoising enumerators, and under certain specific circumstances you get a performance gain if you enumerate index data types, which is critical, I think.
And then in conclusion, just to wrap up, if you need to take away a few things from this talk, it's going to be that you can define generic enumerators in terms of these very natural enumerator combinators. That the generic proofs follow from similar natural introduction and elimination lemmas for these combinators. This approach works for both algebraic data types and index data types, and we can even define generic memoisation on the index if this index is given by a regular description.
Thank you.
[ Applause ].
>> Yeah, thanks. We have some time for questions.
>> Hi. Over here. I wonder, is there any way to extend this abstraction to -- or is it already a monad? A lot of times when you're doing generation for testing, you have some kind of more interesting things that you -- interesting conditions that you need to make sure your values uphold.
>> Yes, that's an excellent question. In enumerators and index enumerators they are both monads as well. For algebraic data types you don't need this structure, but for the index families you use you do need this structure. That's in the paper if you are interested in that.
>> Can you show us the formula for unique again? I was a bit confused. It looked like there was a saying that there's a unique proof that the element at a certain position, and I don't quite understand how that means that there can't be the same element at different positions. Maybe just reading it wrong.
>> Yeah, so what we do here is we quantify over all possible proofs that X is included in this enumeration at depth, and we say that the enumerator is unique for all these possible proofs, that these proofs are equivalent, or equal.
>> And that works even if you have -- over a single N?
>> Sorry?
>> And that works even though you quantify over only one N? I would expect . . . I can take it offline. Maybe I'm not seeing it.
>> Yeah, sounds good.
>> I'm trying to understand exactly how you deal with size pounds on enumerated values. If you think of small check, then small check generates values up to a certain depth, whereas lean check, which I think you refer to, generates terms up to a certain size.
And that gives you a much finer division of terms. My experience, certainly, is that if you use the generator to find bugs, then the size-based approach that lean check uses is way better at finding bugs than the small check approach, which tends to run into the -- you know, the exponential explosion problem too early.
>> Yeah, so just to clarify, we used the small check interpretation of size, so this natural number denotes a certain recursive depth, and you are absolutely depth that this leads to an enormous explosion in the number of values, and is certainly something to look at in future work, to use the lean check notion of size.
>> So just to -- a follow-up question. The other thing that I found is that the lazy small check idea, if you adapt it to make a kind of lazy lean check, that works tremendously much better. Is there any prospect for including that current lazy small check idea in this work?
>> So you have to correct me on this if I'm wrong, but if I recall correctly, lazy small check uses -- pokes some holes in Haskell's pure interface, perform MYO, for example, and I think it would be challenging to include that in a formisation like this. That's one obstacle that I say that -- yeah, maybe there is a way forward.
>> For index only, is there any restriction on the type of indices?
>> Sorry?
>> Is there any restriction on the type of indices?
>> No. Unless you want to use this generic memoisation fracture that we have defined, and in that case the index has to be given by (CORRECTION) structure.
It has to be given by this -- data types because the -- yeah. You need something to match on, essentially.
>> Okay, thank you.
>> Umm, so yeah, so you can enumerate the non-index types and the index types. Can you enumerate sigma type?
>> In short, yes. But sigma is a little bit problematic because -- essentially it works the same as enumerating products.
>> Right.
>> That you have to alter things a little bit because it's now dependent, and this alteration includes that you have to rely on the numeraletic structure of enumerators.
The way we've set it up is that this first element of sigma is essentially a constant, and then you have to rely on some outside knowledge that tells you how to enumerate this constant.
An alternative way to set it up would be to say the first element of the sigma is a non-index description, and then you can sort of piggyback on to the existing you numeration infrastructure that was already there.
>> Maybe one more question, yeah.
>> Thanks for the talk. Can this -- I'm not sure, maybe you already covered it, but can this technique be used to generate sort of enumerate terms of mutually recursive data types? Thanks.
>> Yes, if you're willing to -- into sort of an indexed data type, because one of the tricks that you can use is just have a sigma where the first element sets in which data type you are, and then the second element is the -- yeah, defines the different families.
It doesn't work with these index enumerators. For mutual Li recursive index data types. I don't know. Maybe the same encoding would work. Maybe you run into problems. That's something to be tried in the future, I guess.
>> Okay. Let's thank Cas again.
[ Applause ].
And this is the end of this session. Please don't leave because in a couple of minutes there will be a fireside chat, and I think Andre will tell a bit more about it? Or, yeah. Or you will.
>> If you have to pee, you can go. Otherwise you stay.
We are going to have a panel discussion, which I think is going to be very interesting. I'm not the session chair. I'm here just to entertain you so that you don't walk away. So I'll entertain you by saying that there is a -- don't forget that there are two events. At 7.00 there is a [no audio].
.
.
.
.
If you don't know whether you registered for the guided tour, chances are you didn't. Every single person who has so far asked me this question, I checked, the answer was you didn't register. I can keep checking, but experience shows that you didn't register if you have to ask.
But I'll check Jesper now.
>> Arvind, Guy, I can see you both.
>> Good, we can see you.
>> Can you say something?
No, can't hear anything from Guy. Are you muted? No.
No, Guy, no, nothing, Guy.
>> I disconnected my mixer from the USB port and plugged it back in again.
>> Oh, hardware error. All right. Good. So we are here!
Welcome, everybody. I'm Simon Peyton Jones, and I'm thrilled to have you at this rather unusual conversation, so Arvind and Guy Steele are two giants of the functional programming community. Their influence has been profound, broad and sustained over at least five decades.
So I'm really pleased that they've agreed to have this informal conversation, which was going to be around a crackling fire in person with drinks, but alas it's somewhat remote, object their journeys through the world of functional programming and their reflections of what has happened and what has yet to happen.
Guy was always planned to be virtual from the beginning. Arvind made a long and somewhat tortuous journey and then on arriving has gone down with COVID. I'm so sorry, Arvind. If you fall over sideways on this conversation, we will know what happened. Thank you for joining us virtually.
So I think what I'm going to do is I'm going to engage in a kind of conversation with Guy and Arvind to get them to reflect a bit on their journeys, and then save up questions. I think we'll mostly have them at the end, unless there's anything super urgent. You can do that either in person, I'll try and leave some time, or by Airmeet questions.
I'm going to start off by taking us back to 1978. That was an important year, for me, anyway, that was the year on which John Hughes and I were in our final year at Cambridge. We had been studying -- John, we both started studying maths. After two years I decided it was just too difficult and went off to do electrical engineering. John, of course, cruised smoothly on to his maths degree. We both ended a postgraduate year in Cambridge. More importantly for this conversation, I think 1978 was the year you joined the faculty at MIT, Arvind. Jack Dennis hired you.
I think that was the same year at which you, Guy, published rabbit, a compiler for scheme, whose cleverness I was still decoding at least a decade later. You would enjoy reading that thesis if you haven't.
It was also the year or close to the year in which John backs won the Turing award, and in 1978 he gave the lecture which was entitled can programming be liberated from the van annoyman style, a functional style and its algebra programme.
It was saying to all of us have no truck with this dross of imperative -- functionality programming is the future, and we shall define new systems to execute it.
Pretty exciting. That for me as an undergraduate, and Guy and Arvind somewhat more established at that point, 1979 I think was the year that -- Conway published their book called introduction to the LSI systems, which was a kind of democratisation of VSLI design and moved it was out of the ambit of just at intel into at least universities.
So I was wondering if we could start maybe both of you by telling us a bit about the journey that led you to 1978, what you were trying to accomplish at this time, and what programming language research felt like at that time.
Maybe we'll go back a bit in time and lead up to 1978. Arvind, do you want to start with that?
>> Sure. I graduated -- I finished my PhD in '74 in California Irvine, and my PhD was -- there is echo here. Can you guys hear it?
>> Yeah, but we can hear you enough. Enough? We can pass, yeah.
>> All right.
>> Keep going.
>> Good. So I think my thesis was on modelling programme behaviour for operating systems evaluation and so on, and by the end of it I was convinced that this was a focus idea because the models were just so, so wrong, that anyway, the main decision I took at that point was I only want to be involved in the design of real things and evaluation of real things as opposed to models of things that may be done.
And at that time I came across some papers by Jack Dennis on data flow for signal processing, and they profoundly influenced me, so I thought that was cool, and I said I'm going to do parallel programming and languages, you know, for the next 10 years of my life, which I did, actually.
And then I also came across John backess's thing, and I don't know why I read thiz paper, but there was a paper in Bubble 2 or something, you know, on his reduction languages.
I pursued both of them, but I couldn't relate the two ideas together at all. What John was saying and what Jack was saying.
But the most interesting thing for me personally was that data flow graphs meant kind of what Jack was doing was you're to draw these graphs, and I think I was kind of a vulture on this. I was drawing very, very quick graphs, and very big is all relative.
I tried to do -- in data flow as defined by Jack Dennis. It was lots of space, and I said this is no way to write programmes. This is too verbose, and even if you were clever with visual tricks, it's not going to get there. I wanted to do a visual language, so that's how Irvine did a full language was born, and it directly translated into data flow graphs, so compiler was not an issue for me.
And this was also the time when structured programming was very big, so ID had very few of the constructs.
So on the way to MIT, I happened to meet, visit Tony at Oxford, and he said explain me, what is this ID language you're talking about, and I understood very quickly that he was least interested in data flow graphs. He just wanted to understand the properties of this particular language. And I tried and I tried, and it was very clear to me that nothing was getting across.
I mean, he was just kind of shaking his head and, you know, disgusted, or something. I don't know. When he said, stop, stop, what is the purpose of this language, and I constantly replied, he said don't you think a -- language is expressive enough. And I said, whoops, where is this going
