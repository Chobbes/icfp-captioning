>> It's the session on dependently typed programming, but don't run away, please.
This is -- because this is really about making dependently typed programming more accessible and more useful friendly, I would say. So we have two talks on the gradually typed dependently typed programming and then one on the generating random data or generating data for index data types.
So yeah, our first speaker is Joseph Eremondi, and he's going to talk about compositional equality for gradual dependently typed programming. So please.
[ Applause ].
>> All right. Thank you. So this is joint work with Ron Garcia and Éric Tanter. So what are the research questions that we answer in our paper? So the first thing we're looking at is can we use gradual typing to take equational specifications and move these between run time, like assertion checking, and compile time, as in proofs of equality.
And we are able to do that, and the way that we're able to do that is with a gradual notion of propositional equality. Now the second question we answer is: Given that we have this gradual notion of propositional equality, can we include it on our language without compromising the various properties of that language that we'd like it to have?
And it turns out that we can, and the way that we do this is by having an equality that's based around an entire family of RAFL constructors, instead of just a single one, and that that uses -- it dynamically tracks information about the consistency of the equated values.
So to get us into this, I'm going to start with an example of where we might want to check specifications dynamically and why the current approaches don't quite give us what we want.
We'll start with the quick sort that everybody knows and loves, except it will have a bug. So quick sort, as you probably -- as you're familiar with, sorting the empty list gives us the empty list, and sorting cons of a head and a tail, we take the tail and we filter it into everything that's less than the head, everything that's greater than the head, and then we concatenate, sort those and concatenate them together in the right order, except there's a problem because we have greater than instead of greater than and equal to, any duplicates will get erroneously removed.
For example, if we sort 989, we get 89, which is clearly not an arrangement of the original list, but the problem with this is that this is just silently incorrect. Unless the programmer has a test to catch this or is manually looking at the results of this code, there's not going to be any sort of error thrown by this.
So how can we prevent this? Well, one way of preventing this is to use dependent types, which allow us to take the specification and write it out quite explicitly using the type system.
So the way we're going to do this is with fixed length lists. So here, a fixed length list is indexed by a number, so that's the dependent part of dependent types, the type depends on a number. And it's going to be a dependent pair between a list, so some just a regular underlying list, accompanied by a proof that the length of that list actually matches the index that we have in our type.
And so is this going to catch the bug? So let's look at how we would change our code. So we change our list in type signature to F list indexed by N, and so we're saying that whatever the input length is, the output had better have the same length. Now our nil case, nil is paired with the proof that the length was actually equal to zero, and we can use that same proof in our results.
But when we get to our cons case, we get a proof that the length of cons is one plus the length of the tail, but when we're producing our result, we don't know -- well, we need a proof to put here, and the thing that we actually have to prove looks like this.
It's that the length of doing this filter and then concatenating back together is the same as the length of the original list.
The problem is that there's a conceptual gap between the programmer seeing this and trying to produce a proof of this type, and what the actual problem is, which is that sort is accidentally removing duplicates. These two things don't look the same, and the programmer might try to fill this goal. They're never going to be able to because of the bug. But the compiler isn't giving them any feedback saying this is impossible. The only feedback they get is the difficulty they encounter trying to build an impossible proof.
One of our guiding philosophies is that difficulty proving a goal shouldn't be the only feedback the programmer gets. We obviously have to work within the realm of decidability, but when we can get more information dynamically, we want to give that information to the programmer to help them find what the problem is.
So instead of having them try to write this proof and fail, what are some alternatives? So if you've used Agda or Idris you'd say, well, we'll just put a hole in and keep working. The problem with that is now we can't run this code. The compiler has used the code as incomplete, and you can't compile it. Instead we can declare it as an axiom. Say suppose we have a proof of that type and we'll get going.
This causes problems with type checking because it blocks reduction, but the worst problem is that it doesn't actually check the error. It assumes that the thing you have is correct, and if you ever use it in a bad way, because reduction gets blocked, it will just happily proceed.
So the alternative that we're proposing is to use the imprecise term from gradual dependent types. Sorry.
So that imprecise term, we write it as question mark, and so this has been established from a couple of existing works on gradual dependent types, but what makes it different than holes or axioms is that the imprecise term has dynamics semantics associated with it.
So if we use that question mark in the result of sort and then we run it on that same concrete list, then when it runs, it performs all sorts of gradual run time checks to make sure that even in the presence of imprecision that everything is safe.
And then at the end we get 8,9 paired with this imprecise proof that it has the right length. The programmers are happy because they can keep going despite not having to write the proof.
We haven't gained anything over the version we started with. We have the same silent failure. We just got this question mark out at the end as our proof, and what's strange is that question mark ends up in this case having the type two equals three, and the run time said, well, I'm not going to throw an error. I was able to run safely. You never took the head of an empty list. You never accessed an unallocated memory, but that type is absurd, and wouldn't it be great if the run time could see that this equality couldn't ever be inhabited and notify the user of that? And that's exactly what we've designed with our gradual propositional equality.
So what does this look like? To start for our design, I'm going to do a little review of threesomes in the middle types. These come from the non-dependent gradual dependent -- or the non-dependent gradual type literature.
So if we have a cast from a type A to a type B, the threesome approach says we view that cast as going through A compose B, and so that's called the middle type, and it's precision bound for both A and B. It's a type that's as precise as both of those types.
But what's really useful about this middle type is it allows us to use a single type to collect a lot of information. So if we have a bunch of casts all happening in succession, we can now represent that as a single cast through a middle type where that middle type is the composition of all of the different types we're casting through, including the final destination.
And so the middle type gives us a way of remembering the constraints from multiple casts. So how can we adapt this to dependent types and propositional equality?
So in a static dependently typed language, you prove an equality with REFL, and REFL is the connonical proof that for every X you give it, that X is equal to itself.
Our gradual version looks like this, we have REFL that can prove that any X and Y are equal to each other, provided that you give a witness W, and that witness is a witness of consistency. So there's an entire space of possible witnesses, and that space is all of the terms that are as precise as both X and Y, and if there's something that is as precise as both of them, then the information that they have is consistent with each other and we say that we can equate them modulo that imprecision.
So how do we actually create an equality proof? So to create an equality proof, we can do our witness proofs can replicate the static approach, so X is a witness that X is gradually equal to X.
But we also can have an imprecise proof. So if we have question mark, which is the imprecise proof of X equal to Y, what this ends up reducing to is a proof that X is equal to Y, witnessed by the composition of X and Y, which is by -- witnessed by something that is as precise as both of them.
And this is what's going to give us an advantage over the previous iteration of question mark that we saw in those previous slides.
So we've got in our space of witnesses at least one thing, which is X compose Y.
Now once we have an equality proof, what can we do with it? How can we transform it? So if we're casting between equality types, and these casts are usually generated during elaboration of gradual languages. They are opaque to the user but how the semantics are defined.
When we're casting between these equality types, we're doing that by composing the two things that we're equating in the destination type, but we also compose with the witness that was associated with that proof before. And so the key here is that this produces something that is as precise as the new things we're equating, X prime and -- X and Y, but it's also as precise as X prime and Y prime, and everything else that we saw before by composing an composing and composing every time we use one of these witnesses we're able to track all of the different equality constraints on that we see as the programme runs.
So our space of witnesses, we have this X compose Y, but as we compose with more things, we get more and more precise witnesses, and this is one of the key things about our work, is that every time an equality is cast, the new witness retains all of the precision information that we had from before, and that's what lets us use this to catch errors and to inform the programmer.
So what happens when two values are inconsistent? So if you've got X and Y that are not consistent with each other, then in this case the witness space collapses to a single value, which is error. So we've got this whole space of witnesses, but that collapses down, and X compose Y, if X and Y are inconsistent with each other, is an error.
So to see how this works and how this helps us, let's go back to our sorting example. If we sort the list from before, 989, in our language, we get the same thing, 89 with this imprecise proof, but with our gradual handling of propositional equality that's question mark of type 2 equals 3, and that reduces to the proof witnessed by the composition of 2 and 3, which reduces to the proof of error.
So the programmer has now been informed by the raising of this error that their goal was impossible to prove. There must be some -- the bug must be there because two cannot possibly equal three, so the specific version is false, so the more general version must be false as well.
And now we're able to show the programmer that they need to fix something and roughly where they need to fix it.
So once we have -- we've showed you how to build equality proofs, what can we do with those equality proofs?
So in static dependent types there's a transport operation. If you have some type P that's indexed by X and you have a proof that X is equal to Y, then you can use that proof to transport your P of X into a P of Y, and from the static version, we say, well, the proof must be refl, so then we have a P of X and our destination type is also X, so we can just use the thing we originally got in.
Now of course when our X and Y might not be exactly the same, that doesn't work. So what we do is we cast through the type P indexed by the witness, and once again, now whenever we're eliminating our equality, we've retained all of the different constraints that we encountered in building that proof.
So we retain that information.
Now I talked a bit about the -- in the beginning about the challenges of the metatheory, so I'm going to give just a very quick overview of what those challenges are and how we overcome them.
So if we look at this function, it's just a function, it takes in two arguments, ignores the second one, and doubles the first argument by adding it to itself.
Now we've got another function that does the exact same thing, except it ignores its first argument and doubles the second one. These should be statically distinguishable in a static language the compiler will look at these and say, no, these are not definitionally equal. We can't use these interchangeably.
But the flip side is we have another function that does the exact same thing as our F, but it does it by doing two times instead of X plus X. Now these are observationally equivalent. We shouldn't be able to distinguish these at run time. There should be no way to crack the function open and break the static equivalences that we had in our non-gradual language.
So how can we remedy these two things, especially given that F composed with itself should give F? What can we compose this F -- with that would respect their dynamic equivalence?
So the way we do this is to compose two functions. We compose them by producing a new function that takes in its argument and then composes the bodies of the functions after applying its argument, and the key here is composition is an operator in our language, unlike in the non-dependent versions of the middle type.
And what this lets us do -- I apologise for the misplaced square -- is this whole thing is blocked by a neutral term. Because it's an operator of the language, we can say once we hit something that's neutral, if there's a variable that's blocking us, we just won't evaluate any further. We won't check if they are consistent until you supply a concrete -- to the function.
But then the question is we've got these two functions that should be different, and by composing them don't we have a witness of consistency that these two different functions are different?
And so one of the key insights of our paper is that the static notion of consistency is not the same as the dynamic notion of consistency, which is when two terms compose to a non-error. We want to use this one in the conversion check to preserve our static distinguishability, and we want to use this one for our witnesses of equality.
So there's some more goodies in the paper. We talk more about the design of our precision. We talk about the metatheory, and we talk about some extensions to the language.
So I'll leave you with that. Thank you.
[ Applause ].
>> Great. Are there any questions? I see here, this one.
>> Have you implemented it in a proof assistant, and furthermore, have you formal looized the metatheory in a proof assistant?
>> So both of those are continuing work. For formalising the metatheory, it's basically we're working on it but the easiest way to do that is with a syntactic model, and so that's the authors of the paper following mine have done a lot of really great work on that, and I'm looking at how to incorporate those approaches with the things that I'm adding on top of that.
And yeah, with implementing it, I'd like to say check back in a year and I'll have it done, but we all know how these things go.
>> Thanks.
>> Hi. So it looks very similar to non-determinism to me, like the dynamic is like a maximally non-deterministic term, and then the -- the composition is some kind of meet between the two. Is that accurate?
>> Yeah, the composition is exactly a meet. I accept the fact that it's a greatest lower bound is a conjecture, not a proof yet.
But yeah, in -- ideally it would be a meet and this is -- we've got a lattice where things are getting continually refined. So I -- I hadn't actually thought of it in terms of non-determinism, but especially if we were to incorporate, like, the AGT approach to dependent types where we view gradual -- gradual types review things as sets of static types, it's kind of similar to that idea, where we've got a whole bunch of possibilities that gets continually whittled down as the programme runs.
>> Thank you.
>> Yeah, I had a question. So in the end, what we saw with this witness is that kind of at run time the programmer sees that, like, the equality doesn't hold? Is that correct?
>> Yeah.
>> Why do we need all of this built on top of -- what additional value does all of this theory give us as opposed to, like, for example, the example that was given, you could also just have some assertions, right?
Like, and also there's -- at any point do we get, like, an actual proof? Because now it's again like testing. You just have to . . .
>> Yeah, no, for sure. That's a great question. The answer is what we get is we no longer need to duplicate the specification. We can use the same specification for static and dynamic checking.
So if you want -- so if you never intend to use dependent types, then no, don't do this. But if your goal is to eventually migrate things towards having everything proven correct, then this gives you a kind of intermediate to that during the process of proving things, but you don't have to write the specification twice. Whereas if you have already written out your dependent types and then you have to go in afterwards and add assert statements, that's a duplication of the specification.
So what it basically is is it's like assert statements, but we have types as a unified interface for writing these static assertions and these dynamic assertions.
>> Okay, thanks.
>> I think we have to move on to the next talk, or is there some question? Oh, okay, go ahead.
>> So in the --
>> Last question.
>> -- example where you end up with two compose with three and then it becomes an error, so that seems like it requires decidable equality. Like, how do you know that two and three can't compose?
>> Right, so that goes exactly back to the -- like, the way we handle functions, which is that -- so composition is defined differently depending on the types, and so for data types and other kind of first order types, you just look at the constructors and compose based on that.
And when you have higher order types, like functions, those get -- the composition gets deferred by pushing it under the lambda.
So what's nice about this is even if you don't have decidable equality for your type, can you still use this composition, but you won't get immediate feedback on things like functions until you give an argument to the function and then you get something first order that you can actually check.
But you don't need to provide your own decidability predicate for all these different types.
>> Okay. So let's thank Joey again.
[ Applause ].
And our next speaker is Kenji Maillard, and he's going to talk about reasonably dependence -- reasonably gradual dependent type theory.
>> Exactly. So this is a joint talk with Meven Lennon-Bertrand that you can find in this room, and Nicolas Tabareau and Éric Tanter, once again.
So I already had a really nice introduction to the topic, but let me try to pitch it my own way another time.
So my goal in this talk is to try to build dependent type proof assistant in -- which is easier to use. So I will use the same example of fixed size list, this time called vectors, which take an argument of base type A and specification of a size of a list that is given by an actual number.
We have two kind of vectors. The empty vectors that have size zero and the -- of a head on top of a tail of size N which is a size six or so of N.
On search vectors, we might want to define values kind of functions, such as map or filter function. A filter function is a function that will take -- predicate on to element contained in the vector. A vector of some length N, and that will try to filter the list, the vector by this predicate. Allow do we do that?
In the case of the empty vector, we just return ventie vector directly, and if you have a head on top of a tail, then we check whether the predicate holds of a head and -- recurs on the tail or keep the head on top of zsh on the tail.
This function filter takes an argument of size, the size we know statically, even by length, but what's the size of a result of filtering this list by an arbitrary predicate?
Well, at first we cannot say, we cannot give a close formula immediately. We have to think a bit about, wait, how does it compute, and actually the programme for computing the size of a result is that this size of a result depends on the dynamic value of a list that we get as input.
So in order to prototype efficiently independently typed language, we might want to say, okay, for now I will just put something, some unknown value that I will write question mark, and I will try to evaluate my programme and see how it works. Let's see how things unfold.
So one thing I will not -- we have to consider as implementers of independently typed language and a proof assistant is that adding this kind of question mark to -- in the type checking procedure. For example, if I have something which should be a size vector of size 1 plus some unknown term, and I want it to be a size vector of a known, so I need to be able to optimistically assume that these two things are consistent.
So that's the kind of thing that dependently a gradually dependently typed proof assistant should be able to provide you.
Now on such an example, how do we run actually the thing? Let's try to see the example like filtering two different lists. In the first case I'm filtering the lists 1 to 4 by the predicate even, and in the second case I'm filtering venti list.
In both cases we can evaluate the results of filtering this list, and here I can both cases type check because we have result of type vector of unknown length, which is consistent with the successive length, so that we can apply the add function.
So reducing the filter function gives us in the first case head applied to a list which contain only two, and in the second case, we obtain only the nil list.
Now if the first case we can further reduce the list that only contains two and get a result that will be two.
In the second case, however, we have a nil constructor, which is supposed to be in the type successor of some unknown type, and this should be incompatible with the fact that things in a successor vector, successor size should be of a head on top of a tail.
And so we need to enrich our language with arrows to handle this kind of impossible case that happens dynamically.
And so here I will get --
For prototyping purpose, I could say this second case where I'm passing nil is not useful, so I might want to give a simple specification that refine this unknown bound. For example, saying if my input list is nil, then the resulting size will be zero, and otherwise I will over-approximate seeing that the resulting size is unknown.
In such a case, the second example where we try to take the head of filtering an empty list, will not type check anymore, so we will have gained a bit of static information on which programmes type check, and so that's the motivation for using -- dependent types for prototyping purpose.
So how do we achieve such a dependently gradual dependently typed language? Well, we propose in previous work to separate this language in two sub-language. A first sub-language which is actually gradual and a second language which is -- and the gradual language elaborates the -- while it provides a computational meaning for the gradual language.
The cast contains unknown types and terms. Every type is also inhabited by arrow terms, and we have arbitrary test between pairs of types.
In order to control this test, a notion of precision between types gives us the information about how precise the information of the type that we have is. So the most precise types are the static types while where we lose precision we get to an unknown type, which is actually maximal for the precision order.
Precision between types, controlled cast by enforcing the cast are valid in the following sense, meaning that if two types, if A is more precise than B, then casting from A to B is an injection and it has a retract, which is a cast from B to A.
And the fact that we can interpret the notion of precision as such -- projection pair will be the fundamental semantic criterion to build all our language.
So now in previous work we also observe that actually trying to get all properties that we want from such gradual dependently typed language is not possible. We cannot have at the same time the gradual guarantees that we want, and normalising language, and conservativity of the base language that we tried to gradualise.
And in order to have a proof assistant, normalisation cannot be trained. We want to have normalisation to be sure that we can actually have a decidable type checking algorithm and that we can actually implement our proof assistant.
Moreover, we cannot trade the -- of the base calculus because we want to be able to re-use all programmes from the existing literature, from existing programmes.
So in this work, I will -- so we had three different variants in previous work that were trading the different possibilities, and in this work I will concentrate on this variant, which has both an ambiting of CIC and that is normalising.
So now I will look at the cast calculus, which has this property. So I have this cast CICN which is both normalising and conservative over CIC, however it is not globally gradual, and the main question that this work tries to answer is how far it is from being gradual.
We are not able to show graduality globally for every types, but maybe it might be possible for sometimes to show some gradual properties. And how do we do that? Well, we will actually internalise the notion of precision inside the type theory so that we can actually reason inside the type theory on the precision between types.
So we could GRIP the resulting theory, which expands this calculus with a notion of precision, actually two families of precision. One for precision between types and a family for precision between terms.
And these types of precision are actually proposition that we -- that inhabit a pure universe of proposition, PROP, and this universe of proposition contains no errors, no casts and no unknown terms, which make it a good place to have sound reasoning for the type theory.
So what does this internal notion of precision actually means? Well, we can explain what it means in terms of dynamic gradual guarantee. So dynamic gradual guarantee is saying that if I have a term X, which is more precise than term Y, and a context for values in type A, applying the context to X should be more precise than applying the context to Y at Boolean types.
And we have the dynamic gradual guarantee will hold for a context C if and only if this context is related by precision inside the language at type of Boolean predicate on A, and if and only if this context is monitored.
In order to understand concretely what this formula actually means, we should look at precision on Boolean types, which contain true and false, as usual, but also the exceptional values provided by the gradual language, arrows, and unknown. And we have a lattice where arrow is the smallest element and unknown is the biggest one, meaning that the dynamic gradual guarantee actually means some kind of error approximation.
Meaning that if my value X plugged in the context C does not error, then I'm sure that value Y plugged in context C will not error either.
So now that we have some understanding of this notion of precision, how do we prove actually in our language the -- how do we derive a proof of precision? Well, let's have a look at two terms.
The first term is a function from natural to natural but add one to a natural number. And the second one takes an unknown -- a term of an unknown type and cast this element to natural numbers and then add one.
If we are able to prove that the first static term is more precise than the other known function, then this will mean that other known will not have a role on good input, and how do we do such a derivation? Well, we start from the judgment that we want to prove.
We look at function types, and at function types precision is actually extensional. It maps elements related by precision to elements related by precision.
Now we will look, we will intro the argument in the context and do some computation to unfold the add one and add unknown, and we need to compare two application of plus one of successor, and successor is actually monotone, so we can actually just forget about it and show it's enough to show that X is more precise than the cast.
Finally, to complete this proof, it's enough to know that whenever we have two types that are related by precision, the ambiting projection properties will allow us to get that this cast is actually a right so we can use it directly to prove that X is more precise than C at the type not more precise than unknown.
So now this proof that I just gave is made out of some principles that we have on precision, such that trancivity, quasi-reflexivity, the add junction properties that I just used, and also an important property that allows us to characterise precision between types but are not directly related by precision.
On the other hand, we cannot have everything in our language. In particular, the precision on types is not the same thing as the precision on the universe of types, and we have -- this allows us to keep a normalising language, but it makes reasoning a bit surprising, let's say, and we have to use some explicit cumulativity in order to regain enough expressivity in our language.
Really quickly I would like to outline that we have some interesting metatheory properties on this language. It is normalising. It is consistent, normalising, and moreover we can carve out some fragment of this language where we can automatically derive the terms are gradual.
So to conclude, we had a proof of concept of this type theory in Agda, and we have a model that validates the theory that I just alluded to.
We have non-gradual operations in this language, so for example we have a catch operation on inductive type, but I did not have time to talk more, much about. And this paper can also be seen as a way to address the question of catch in that gradual world.
And we add precision internally in order to reason posthoc on the gradual terms of this language.
So for future work, we would like to have Alex from a gradual source. Right now we are working only with a cast calculus, and we would like to have more general inductive type, maybe an actual implementation from the source language, but this will have to wait for another time.
Thank you.
[ Applause ].
>> Okay, we have time for a couple of questions.
>> Hi. So the internal precision, it looks a lot like directed equality from, like, a directed type theory. So what -- is there a relationship with that?
>> So I think -- first I think that there are many directed type theories, so I would need to have a more precise reference, I would say.
And I don't think I saw any instance of directed type theory that was exactly coinciding with this, in the sense that so I didn't have much time to explain the notion of the model that we are using, but we can actually interpret this whole theory in a model with types that are interpreted as partial pre-orders. So it's not a full pre-order, like some elements are not created to -- are not reflexive of the relation.
But in that sense, it has some kind of label of directed type theory.
>> Yeah, and then I guess related to that, you said that the ordering for the function type was extensional.
>> Yes.
>> Do you have to add that as an axiom or is that derivable?
>> I mean validated -- so the source language has it -- part of it -- the reduction role on the precision at function types, and then it's validated through the model, like in the model, the precision at function types is actually extensional.
>> Thank you.
>> More questions?
>> Okay, and then also the -- if you go back, this last one, the composition through upper bounds, is that equivalent to all of the cast being retractions?
>> All of the cast being retractions. So the thing is that here the important thing is that it allows to talk about cast between types that are not related by precision.
>> Yes.
>> Like, it allows us to -- this plus some composition operation allows us to describe the behaviour of cast outside the fragment which is directly related by precision.
>> Right, this allows us to reduce all of the reasoning about casts to up casts and down casts.
>> Yeah, mostly.
>> But what I'm saying is in my experience this kind of property is exactly what we need the retraction property for, from there being embedding projection pairs.
>> I don't know if it is equivalent, but I'm sure that I am using it in -- like, the fact that I can quantify over all upper bound X is indeed the fact that you have a retraction.
>> Okay.
>> One interesting observation around that is that we couldn't do that through an intersection. Like, it wouldn't work with a lower bound.
>> Yeah.
>> Okay. I have a quick question myself. So how does this compare with previous talk, and in particular do you support identity types?
>> So we don't support identity types.
>> Okay.
>> We are able to support inductive types with indexes only when we have enough decidability on this, for example, forceable indices or things like that.
And otherwise, no, I'm -- I don't know how a general identity type would fare in this.
>> Okay.
>> And I propose we thank the speaker again.
[ Applause ].
Our third speaker for this session is going to talk about a totally unique account of enumeration.
>> All right, so I will. So let's start with the very simple question. What is enumeration? And such a simple question deserves a very simple answer, and what we can say is it's a list for Booleans. This is true and false. And the enumeration for natural numbers contains zero, suc zero, and so on.
And at some point the conclusion we have to draw is that this sucks.
[LAUGHTER].
And is there some automation we can apply here, and that's what this paper is all about.
So to take a step back and set the stage a little bit, formal verification is great, but it's very expensive, this is one of the lightweight methods that we can use to catch errors early on in the process to reduce this cost.
But the key challenge of this approach is to find the test data to instantiate these properties with. And what we do in this paper is we use a generic approach to exhaustive enumeration of these test values, and what this gets us is a uniform enumeration of the inhabitants of algebraic data types and indexed families, and we can write some generic proofs that establish some notion of correctness for these generic enumerators.
Right, so let's dive in to the meat and potatoes of this talk, what is an enumerator.
So as we saw in the introduction, the basic idea is that an enumerator is a list, but that doesn't fare so well for infinite data types. Instead we have this enumerator data type that essentially defines one layer of recursion of these enumerators. It has an assumption list A that contains the occurrences and it produces an enumeration of a different type.
And an example of such an enumerator is the enumerator for natural numbers that we can define as follows, which just asks to enumerate the actual numbers we have zero at the head of the list or we map the successor over the re-cursively enumerated values.
And then to extract something usable from such an enumerator we can write the function that iterates these enumerators up to a given depth bound.
There are alternative interpretations of these enumerators, like co-inductors, but I won't go into that during this talk.
So why not define these enumerators directly, you might ask? This is something I would just like to quickly point out for binary trees, for example, you would write the following bounded enumeration to enumerate the infinite data types, but the obvious problem here is, of course, that the number of recursive calls grows exponentially with the depth of the type.
And the current setup avoids that.
So if we say that enumerator is correct, what do we mean by that? There are two things, actually, that we are interested in here. The first one is completeness. We say that an enumerator is complete if all the values of the type that it enumerates will eventually occur at some depth if we iterate over that enumerator, which is captured by this complete paragraph of enumerators. And we have a notion of uniqueness that says that an enumerator for type A is unique if all of the values of type A will occur at most once, and the way we formalize this is if there are two proofs -- that point to a value X in the enumeration, then actually these proofs, they point to the same location.
And then as a final thing, as a marker, I want to say for certain enumerators combinators we establish a notion of fairness that ensures that you don't enumerate one constructor first and only then get to the other constructor, but if you want to know more about that, you should read the paper.
To consider a few examples of these combinators, the simplest ones are empty and pure, which construct enumeration with no values or just list a single value into enumeration.
And then there's these operations which you might recognise as haskal's alternative type class, and we offer a choice between two enumerators. They allow you to map the function or apply the results of one enumerator to the values from another enumerator, and then the final combinator that we use, it's actually my favourite one, is for recursive, and it's just the identity function.
So let's see some examples of this. So yeah, for Booleans, a choice between either true or false, or for natural numbers, a choice between zero or successor applied to a recursive argument, and hopefully that you see theory immediately is there is a very clear correspondence between the structure of these data types and the way that we assemble these enumerators, and this correspondence goes further, because if we sit down and write the uniqueness and completeness proofs for these enumerators, we find that the proof structure also follows the data structure.
And this makes enumerators an obvious candidate for generic programming, meaning that we will define these enumerators by induction of the structure of these types. So a quick note about the setup that we have to use here. Obviously an -- value cannot pattern match on a type directly, so usually how these things are done is by creating some data type whose inhabitants correspond to Agda types, which is usually called the universe.
And this is something we can inspect, and then you write a reflection function that reflects this syntactic elements back into Agda sets, and then the enumerator writes such a description, and it enumerates into the semantic description of these.
I won't show the definition of the descriptions here, but it contains sort of the standard combinators for empty unit types and is -- on the products and co-products, and you can have -- recurrences.
And then once we have this setup fleshed out, then actually the generic enumerator is a very direct correspondence between the different constructors of this universe of types and the different combinators that I just showed you.
This is just a direct mapping and for sums and products it's also very straightforward. Please don't try to parse this, but try to appreciate the visual correspondence that hopefully is clear.
So now that we have this generic enumerator, how do we go about proving its correctness? Recall that completeness, for completeness we need to show that iterating produces every value eventually, and uniqueness then we need to show that iterating produces every value at most once.
And as it turns out, to prove these things, we have to show lemmas that mimic the introduction and elimination rule from propositional logic, and there's a brief example of that here.
So this is all nice and good, but then the question is how do we move to indexed families? So the idea here is that this recursive argument that we had in the definition of enumerators, that we can lift that to a function from index to list, which gives us this thing, the I enumerator which takes as its first search argument, which stands for the recursive occurrences, takes a function from index to set, and the assumption that it makes is going to be a function from that index to a list of values at that index.
And the way to think about this is that an I enumerator evaluates enumerators at -- but they vary with some index I.
And we can define the exact same combinators for this data type, and the idea is then that the enumerators themselves are defined as functions that computes on the index. So in this way you get a sort of two-tiered lifting of the structure that you define for regular types to the structure of index types.
And here's just a brief example for finite sets, which if the index is zero is empty and otherwise returns a choice, and notice that here we give the smaller index to this recursive combinator.
And to adapt the generic setup to index types, we use a very similar lifting from sets to I to set, and actually the completeness and uniqueness proofs in the index setting are -- they go through using very similar lemmas about the combinators about the use to implement the generic enumerator.
And then finally, so I remarked at the beginning that one of the reasons enumerators are set up this way is that you get a sharing between recursive calls for free, essentially, but that doesn't work so well for index types because the recursive occurrences are now functions, and every time we apply these functions with a new index, they recompute the result.
And in some cases this leads to excessive recomputation of these recursive enumerators. For example, if you have perfect binary trees, where for every node you have two recursive occurrences at the same index, or for bit factors where there is also a lot of sharing of substructures at the same index.
For this we can use generic memorisation technique, by re-using the generic structure that we defined for regular types, and the way this works is that we define a generic try, a co-indetective data structure, whose shape is dpept on the shape of the index of this function that compute the enumerator, and essentially these data structures represent a tabulation of this function, and if you call this function with the same index more than one time, then there will -- the results will be cached.
We have showed that this is equilt to the non-memoising enumerators, and under certain specific circumstances you get a performance gain if you enumerate index data types, which is critical, I think.
And then in conclusion, just to wrap up, if you need to take away a few things from this talk, it's going to be that you can define generic enumerators in terms of these very natural enumerator combinators. That the generic proofs follow from similar natural introduction and elimination lemmas for these combinators. This approach works for both algebraic data types and index data types, and we can even define generic memoisation on the index if this index is given by a regular description.
Thank you.
[ Applause ].
>> Yeah, thanks. We have some time for questions.
>> Hi. Over here. I wonder, is there any way to extend this abstraction to -- or is it already a monad? A lot of times when you're doing generation for testing, you have some kind of more interesting things that you -- interesting conditions that you need to make sure your values uphold.
>> Yes, that's an excellent question. In enumerators and index enumerators they are both monads as well. For algebraic data types you don't need this structure, but for the index families you use you do need this structure. That's in the paper if you are interested in that.
>> Can you show us the formula for unique again? I was a bit confused. It looked like there was a saying that there's a unique proof that the element at a certain position, and I don't quite understand how that means that there can't be the same element at different positions. Maybe just reading it wrong.
>> Yeah, so what we do here is we quantify over all possible proofs that X is included in this enumeration at depth, and we say that the enumerator is unique for all these possible proofs, that these proofs are equivalent, or equal.
>> And that works even if you have -- over a single N?
>> Sorry?
>> And that works even though you quantify over only one N? I would expect . . . I can take it offline. Maybe I'm not seeing it.
>> Yeah, sounds good.
>> I'm trying to understand exactly how you deal with size pounds on enumerated values. If you think of small check, then small check generates values up to a certain depth, whereas lean check, which I think you refer to, generates terms up to a certain size.
And that gives you a much finer division of terms. My experience, certainly, is that if you use the generator to find bugs, then the size-based approach that lean check uses is way better at finding bugs than the small check approach, which tends to run into the -- you know, the exponential explosion problem too early.
>> Yeah, so just to clarify, we used the small check interpretation of size, so this natural number denotes a certain recursive depth, and you are absolutely depth that this leads to an enormous explosion in the number of values, and is certainly something to look at in future work, to use the lean check notion of size.
>> So just to -- a follow-up question. The other thing that I found is that the lazy small check idea, if you adapt it to make a kind of lazy lean check, that works tremendously much better. Is there any prospect for including that current lazy small check idea in this work?
>> So you have to correct me on this if I'm wrong, but if I recall correctly, lazy small check uses -- pokes some holes in Haskell's pure interface, perform MYO, for example, and I think it would be challenging to include that in a formisation like this. That's one obstacle that I say that -- yeah, maybe there is a way forward.
>> For index only, is there any restriction on the type of indices?
>> Sorry?
>> Is there any restriction on the type of indices?
>> No. Unless you want to use this generic memoisation fracture that we have defined, and in that case the index has to be given by (CORRECTION) structure.
It has to be given by this -- data types because the -- yeah. You need something to match on, essentially.
>> Okay, thank you.
>> Umm, so yeah, so you can enumerate the non-index types and the index types. Can you enumerate sigma type?
>> In short, yes. But sigma is a little bit problematic because -- essentially it works the same as enumerating products.
>> Right.
>> That you have to alter things a little bit because it's now dependent, and this alteration includes that you have to rely on the numeraletic structure of enumerators.
The way we've set it up is that this first element of sigma is essentially a constant, and then you have to rely on some outside knowledge that tells you how to enumerate this constant.
An alternative way to set it up would be to say the first element of the sigma is a non-index description, and then you can sort of piggyback on to the existing you numeration infrastructure that was already there.
>> Maybe one more question, yeah.
>> Thanks for the talk. Can this -- I'm not sure, maybe you already covered it, but can this technique be used to generate sort of enumerate terms of mutually recursive data types? Thanks.
>> Yes, if you're willing to -- into sort of an indexed data type, because one of the tricks that you can use is just have a sigma where the first element sets in which data type you are, and then the second element is the -- yeah, defines the different families.
It doesn't work with these index enumerators. For mutual Li recursive index data types. I don't know. Maybe the same encoding would work. Maybe you run into problems. That's something to be tried in the future, I guess.
>> Okay. Let's thank Cas again.
[ Applause ].
And this is the end of this session. Please don't leave because in a couple of minutes there will be a fireside chat, and I think Andre will tell a bit more about it? Or, yeah. Or you will.
>> If you have to pee, you can go. Otherwise you stay.
We are going to have a panel discussion, which I think is going to be very interesting. I'm not the session chair. I'm here just to entertain you so that you don't walk away. So I'll entertain you by saying that there is a -- don't forget that there are two events. At 7.00 there is a [no audio].
.
.
.
.
If you don't know whether you registered for the guided tour, chances are you didn't. Every single person who has so far asked me this question, I checked, the answer was you didn't register. I can keep checking, but experience shows that you didn't register if you have to ask.
But I'll check Jesper now.
>> Arvind, Guy, I can see you both.
>> Good, we can see you.
>> Can you say something?
No, can't hear anything from Guy. Are you muted? No.
No, Guy, no, nothing, Guy.
>> I disconnected my mixer from the USB port and plugged it back in again.
>> Oh, hardware error. All right. Good. So we are here!
Welcome, everybody. I'm Simon Peyton Jones, and I'm thrilled to have you at this rather unusual conversation, so Arvind and Guy Steele are two giants of the functional programming community. Their influence has been profound, broad and sustained over at least five decades.
So I'm really pleased that they've agreed to have this informal conversation, which was going to be around a crackling fire in person with drinks, but alas it's somewhat remote, object their journeys through the world of functional programming and their reflections of what has happened and what has yet to happen.
Guy was always planned to be virtual from the beginning. Arvind made a long and somewhat tortuous journey and then on arriving has gone down with COVID. I'm so sorry, Arvind. If you fall over sideways on this conversation, we will know what happened. Thank you for joining us virtually.
So I think what I'm going to do is I'm going to engage in a kind of conversation with Guy and Arvind to get them to reflect a bit on their journeys, and then save up questions. I think we'll mostly have them at the end, unless there's anything super urgent. You can do that either in person, I'll try and leave some time, or by Airmeet questions.
I'm going to start off by taking us back to 1978. That was an important year, for me, anyway, that was the year on which John Hughes and I were in our final year at Cambridge. We had been studying -- John, we both started studying maths. After two years I decided it was just too difficult and went off to do electrical engineering. John, of course, cruised smoothly on to his maths degree. We both ended a postgraduate year in Cambridge. More importantly for this conversation, I think 1978 was the year you joined the faculty at MIT, Arvind. Jack Dennis hired you.
I think that was the same year at which you, Guy, published rabbit, a compiler for scheme, whose cleverness I was still decoding at least a decade later. You would enjoy reading that thesis if you haven't.
It was also the year or close to the year in which John backs won the Turing award, and in 1978 he gave the lecture which was entitled can programming be liberated from the van annoyman style, a functional style and its algebra programme.
It was saying to all of us have no truck with this dross of imperative -- functionality programming is the future, and we shall define new systems to execute it.
Pretty exciting. That for me as an undergraduate, and Guy and Arvind somewhat more established at that point, 1979 I think was the year that -- Conway published their book called introduction to the LSI systems, which was a kind of democratisation of VSLI design and moved it was out of the ambit of just at intel into at least universities.
So I was wondering if we could start maybe both of you by telling us a bit about the journey that led you to 1978, what you were trying to accomplish at this time, and what programming language research felt like at that time.
Maybe we'll go back a bit in time and lead up to 1978. Arvind, do you want to start with that?
>> Sure. I graduated -- I finished my PhD in '74 in California Irvine, and my PhD was -- there is echo here. Can you guys hear it?
>> Yeah, but we can hear you enough. Enough? We can pass, yeah.
>> All right.
>> Keep going.
>> Good. So I think my thesis was on modelling programme behaviour for operating systems evaluation and so on, and by the end of it I was convinced that this was a focus idea because the models were just so, so wrong, that anyway, the main decision I took at that point was I only want to be involved in the design of real things and evaluation of real things as opposed to models of things that may be done.
And at that time I came across some papers by Jack Dennis on data flow for signal processing, and they profoundly influenced me, so I thought that was cool, and I said I'm going to do parallel programming and languages, you know, for the next 10 years of my life, which I did, actually.
And then I also came across John backess's thing, and I don't know why I read thiz paper, but there was a paper in Bubble 2 or something, you know, on his reduction languages.
I pursued both of them, but I couldn't relate the two ideas together at all. What John was saying and what Jack was saying.
But the most interesting thing for me personally was that data flow graphs meant kind of what Jack was doing was you're to draw these graphs, and I think I was kind of a vulture on this. I was drawing very, very quick graphs, and very big is all relative.
I tried to do -- in data flow as defined by Jack Dennis. It was lots of space, and I said this is no way to write programmes. This is too verbose, and even if you were clever with visual tricks, it's not going to get there. I wanted to do a visual language, so that's how Irvine did a full language was born, and it directly translated into data flow graphs, so compiler was not an issue for me.
And this was also the time when structured programming was very big, so ID had very few of the constructs.
So on the way to MIT, I happened to meet, visit Tony at Oxford, and he said explain me, what is this ID language you're talking about, and I understood very quickly that he was least interested in data flow graphs. He just wanted to understand the properties of this particular language. And I tried and I tried, and it was very clear to me that nothing was getting across.
I mean, he was just kind of shaking his head and, you know, disgusted, or something. I don't know. When he said, stop, stop, what is the purpose of this language, and I constantly replied, he said don't you think a -- language is expressive enough. And I said, whoops, where is this going
