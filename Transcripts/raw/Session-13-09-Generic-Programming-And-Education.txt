Welcome to the session. We've got three talks in this session.
Our first speaker is Josh. I will just wait for the slides to come up. Josh will be talking about generic programming.
Thank you. Thank you for all coming. This is joint work and they're here as well. What our work is about is about libraries for dependently typed programming, especially in Agda because that's our favourite language. Usually we have lists in our library and a list say container that has some elements of subtype A. The library can provide this list and data type to be able to point to a particular list and say that satisfies a particular predicatepredicate. We point to the head of the list and say that satisfies P or we use the constructor to say the element is somewhere. Usually in the library we have some associated operations as well. For example, this lookup function that basically extracts the element pointed to and also prove it satisfies P. What's interesting here is that this construction is not specific to lists.
Whenever we have a container like data type, we expect that there is a corresponding anti data type and lookup function.
For example, the library has this data type of bindery trees that has elements stored in their leaves, then we would expect that the library should also provide this tree data type and lookup function. For generic, for general purpose data types such as list and tree we can expect that - well, it is okay to just put everything in the library. We don't have to do anything special, but we often write to our own special stage types and it can be pretty complex. We can't help to find them in the library. A typical example would be an embedded DSL. Here starting from here, we want to extend it to some mysterious language and we are putting well scoped variables, so we need dates of independent cease now, and additional construction such as led expressions and maybe a mysterious construct that is specific to this language and we may use various data type futures such as data type parameters and universe polymorphism. This is what we hope to find in a library, but we still want library support.
For example, here we might want to prove theory about this mysterious element, so we would want the library to give us basically from this definition, we should be able to get this missed any data type and look up function. We want library to give us in. What the library should provide is generic anti construction, that works for all container like data types.
Ideally, the definitions that give us should be something like this and native definition, a definition that looks like what you would have written by hand.
Why do we want niece definitions? When we go along with our development, so here we're proving the subtheorem, that says for any T, if it is well typed, then any mysterious element in it is nice in some way. The first thing we do is try to case split on the missed any argument, so listing all the constructors, and also when we try to find the gold type of hole, the definition look up understanding would show up nicely and those compute nicely.
So that's what we want. We want the libraries to give us generic constructions for deriving native definitions from our own data types. How do we develop such libraries? Since we want native data types and functions, we can use that using Elaborator reflection. The reflection API provides a set of data types representing Agda constructs so we can manipulate agda syntax within agda. This is an example.
We use the constructor to represent the variable in a form of Brian X which can be followed by arguments. We can represent lam expressions, literal and so on and also type expressions.
Here the pi constructor represents a dependent function type and the two arguments are domain dichotomy. Sometimes the API writes type instead of term to informally say that actually mere what we want is a type expression. Perhaps more importantly, the reflection API also provides a type check in to TC and a set of primitive TC computations. So these primitives, we use these primitives to write met aprograms or macros in a form of TC computations and, yeah, then these programs can be run during time checking. For example, we can use the first two primitives declare def and define fun to declare the function and give the definition to the function, and my co-authors have spent quite some time allowing the data types and defining data types as well. It is now in the development version, so you can try it out. What's special about Elaborator reflection is that it provides powerful primitives for that expose some of the functionalitys of the ee Elaborator. For example, we have unify, and there's two terms and solving met avariables appearing in those terms, and we also have this normalized primitive that normalizes term. It turns out to be very useful and we will see that later. Okay. That sounds good, but actually we don't like it because the representations are very imprecisely typed and as a programmer we hate that. So here the example is something like we're trying to write ray type, a pi type a type, a pi type. We might provide the wrong expression and supply an out of bounds index. This makes writing generic constructions using purely Elaborator reflection very difficult and very inappropriate, but there is a solution to this problem, and that is data type genetic programming. I'm not going into the details, but in dpat atype generic programming, you use constructions and proving them correct. Here data D, short for data type descriptions, is a representation of data types and inside we use this data type to represent constructor types.
Let's go back to the example.
Suppose I use ConD to write a function type that has at least one argument, then the argument type, we would use the Sigma construction because of the type of Sigma we definitely had to give a legitimate type as the argument type and later when we need to refer to the argument, we use an ordinary variable of the right type, so everything is under control by the type checker, which is good.
Actually, in our representation we also did something about universal level correctness. So we have a lot of universe levels appearing and we need to satisfy those constraints and we can actually require those constraints to be satisfied because the universe level are internalised so we with can actually reason about them, so that's a plus for Agda. That also looks nice, but still we don't like it. It is because traditionally, and especially in Agda, we were to use this new data type to convert data into types we program with. It looks ugly. I hope no-one is pausing itit. So don't worry about it.
What I want to draw your attention to is that it's actually very inconvenient to use. There's a generic destructor here that has the constructions of your data type.
If you try to case split, you get [indistinct] so long support is gone and you are left with this mess. So what do we do?
Actually, it's kind of obvious.
I mean, we just continue to use data type generic to write constructions, but at the end we don't ewe the new data type. We use ee elaborate cap is reflection to manufacture native data types and functions, so that's kind of simple, that is the basic idea, but if we want to go into some more detail, then suppose that in my library there are two constructions and I want to substantiate the constructions for my own data type myst. Then we need to write some interfacing code. First of all, we need to use a macro genDataD to generate a description of Myst. That's the input to AnyD, but AndD requires more information. It needs to know that MystD corresponds to Myst so it can refer to Myst in it's construction. In connection is as generated by a macro, so it's not a problem. Specifically for AnyD we need more information. We need to tell AnyD where we want to point to.
To specify that, we're giving a proof that MystD is a simple container. You can find the definition in the paper. Now we can invoke AnyD. That's the description of the Myst data type, which we can turn into an actual data type, a native data type by another macro. Usually we can stop here, but if you want to continue to do more, substantiations, then we should establish connection here as well. Felling, look up Any can be substantiated for MystAny.
Actually, I want to talk about this look up any because it is interesting. Look up any come Putins with algebra and it is turned into a native forward function by the macro defined thought. Inside the macro what we do is just write basically a definition like in this. The idea is to use mobilize to expand the definition and get a clean definition for look up MystAny. There is a catch because it doesn't expand unless you replace with actual constructors. So we have to use ee elaborator reflection to do case splitting, and to generate in kind of definition. It is actually not that hard. You can see that is actually very mechanical, so it is manageable.
Finally, we just normalize and you get the clean definition.
One of the questions now would be can we use the generic library now? The answer is, of course not yet. Why is that? We have experimented. We have experimented with some syntax generic [indistinct] kwhee talked about a couple of days ago, but there aren't many and I think we need many generic constructions to be able to call them a library. We need more.
Also we need to extend and refine our framework. We want to support more forms of data types and functions. We want to reduce the effort prior not infer face code because that's a lot. We want to spend more time with the macros because it is difficult to get them right because we simply don't have a very good way to deal with macro correctness at the moment.
Finally, everything is slow right now so some work has to be done. It is a lot of work, but, of course, we appreciate any help we can get, but hopefully eventually we will be able to get to a generic library that the practice al programmer will be happy to use. Thank you very much. Plus plus [APP L.A. U.S.
E]
I'd like to invite any questions that you might have.
One over there. There's a question just over there.
Thank you for that. It is a good talk. Do you support indexed inductives yet and if not, do you have any ideas of how maybe along the way you could have support for things like that?
Did you say index?
Indexed inductor families.
You mean data types with independent cease?
Yeah, that's is definitely supported. That is why I gave the example because that's indexed, yeah.
I'm a fan of elaborator and have hated it. It sounds like your programming is a step towards EDSL for expressing these types of elaborator reflection without getting into the nuts and bolts. Do you have plans to extend to general elaborator or just focused on these?
Focused on? Data types rather than generating general. You have a very precisely find representations of Agda code that doesn't exist in the provided library at the moment and this is screaming for a DSL that supports this in Agda in my opinion. I was wondering whether you were pushing for that.
You mean something like pushing this type of representation into elaborator reflection?
Sort of expanding on the set of definitions that you've got in order to provide a complete representation of Agda syntax with this precise typing.
That is actually a very good question because that touches on the nears. We're trying to represent data types within a type theory and so I think that's still ongoing research and we're basically targeting some more common data types right now, and yeah, of course, if there are some more theoretical advancements, we will be able to make use of that.
that. I think we should stop there. Thank you very much.
APPLA.U.SE I invite our next speaker who will be speaking about programming. Thank you. Snoo Hello. I'm going to give a talk about programming. It will sound very familiar to you.
There are two key words in this title that are very important, one is practical, so I am going to sflan what we mean by 'practical skwoets, and try to convince you that our -- explain what we mean by 'prakal', and the other one is native data types. Another one word that is missing is 'safe". What we focus on in this work is to have a way to define data construction safely that is once it works you know it will always work regardless of which data type you use. So I assume by now most know what data type generic programming is. It is the art of defining constructions that can be applied to any data type using a simple implementation.
The benefits are quickly visible, so you have codification and also you can get consistent behaviour by having a having a single implementation. One of the most common example in circles is the use of deriving ... most people that use it knows the benefits of deriving so many constructions for free, essentially, as soon as you introduce your own. It can be very simple, like the ability to debug, print values of the data type, but it can get intricate and especially with generics.
You are able to derive for free many quite involved constructions. So a fair question would be that is interesting in Haskell, but why would you need this in Agda? So a simple answer would be they are programmings in their own rights. There are actually many generated constructions that are very interesting and useful when you're trying to prove things of data types, so most of you probably know that you always have a principles that come with any data types, and there are actually other constructions that exist in the same kind of way, and some of them I introduced in a paper called a few constructions on stricters.
In this paper it explains that there is a recipe for defining them for any type, like there is a systematic way to define them, and then it is only a matter of having a system to derive them ally. Another common example is decidedable equality. It is way more interesting to get to witness two values are equal or proof that they are distinct. Of course, there are other examplesexamples. I've shown a bit of an example here. It is an index type. It is usual to want to [indistinct] you kind of would like to long wish to always give this to you. That is assuming some equality on A and ... by A.
Likewise, you can expect to have this principle and in cook you do. When you define, you will get this principal for free. So what is a solution? If your language doesn't give you any deriving primitive, there are two approaches. As you just saw of courses there is a thing called elaboration. A fair comparison would be Haskell. You are able to manipulate in syntax and under the conditions you can define almost anything. It is a very powerful mechanism. It tells you to produce definitions that look just like if they were ... but it is hard to implement using code. It is very diverse.
You have to be careful about the transformation you do on terms.
You have to use the brandishes and it is usually the hard to ensure that you don't do any mistakes. It is hard to guarantee that you have certain properties because you can only know if you fail once you institute it for a given prototype. It is hard. The other one approach is to use what is called universe of descriptions.
The idea is you internalise, you make explicit what it is to be a data type and you work inside a coding. You explain what is the description. You interpret the description as a shape and then you take the six points and you end up with a type, an Agda type that behaves pretty much like a native data type that you're trying to describe. When you use this kind of coding, it is very easy to define constructions.
You essentially quantify overall the descriptions. Generally constructions are regular Agda functions and there is no trick to it. Once it is implemented, you know it will be correct, but, of course, you are putting properties and not native data types. As soon as you want to derive generic properties, you need to work with this coding.
It is not very practical. So our goal is to find how to bring this kind of universal descriptions and the benefits of it closer to what native data types are so we are able to define generic constructions that do not operate on new but on nature data types and it is correct by definition. With this in mind we wanted to build a library. We wanted to provide safe primitives to implement constructions and, of course, we wanted to implement constructions to demonstrate that this is a practical approach for regular Agda users.
We also had pretty strict constraints. We wanted to avoid using reflection at all, to define generic constructions, and so this was a challenge that we had to face. We also wanted to be safe, to be in the safe fragment of Agda. This means that we really had to convince Agda that they were terminating, which is an issue that you have to deal with when you work with these codings. Also we wanted to have user friendly construction that is generally constructions that have the proper types that you would expect if you had written it by manned, and we wanted to have them to compute and reduce as you would expect them to -- written it by hand.
I'm glad to say that we succeeded. We have a library.
Assuming that you install it, you have to derive an encoding of data types you want to have constructions over. So this is a macro. This is the only place where you actually use reflection and then once you have this encoding, you can use all the constructions available in library. So the first one we implemented is, of course, the generation conduction principle.
I have to confess that maybe the name isn't quite what it should be. We are not generating code.
It is a regular Agda function that happens to take an encoding as an argument and will come Putin as the appropriate type once it gets its first fragment.
Applying to the description of natural numbers you will have a function which is precisely the eliminator that you would expect, applied to other descriptions you end up with a very systemic and actually useful type and all this means it is a regular function, and very important things that we wanted to enforce was proper computational behaviour. We see here that we defined the numbers, and we see it is able to make progress. If you see the S definition, so N is a free variable and if we apply add it is possible to move it out of the addition. This is very important because it allows you to actually use the constructions inside proofs, and there have been attempts to do this kind of attempts in Agda and it was computational always the missing part. I want to emphasize that here I have written the types explicit live, but you don't have to do it and you don't actually have to make - every time you want to use, you just have to give it a description. I think it is pretty useful. We implemented decidedable equality. We have a derived intervention. If you give it a description of natural numbers, you end up with what you expect. It is easy to fetch the appropriate depen residencies, so, for example, like on vectors, it will ask for equality on some parameter.
Another construction that is very useful is proof of jointness of constructors. You always have the properties that distant constructors are unequal, about you have to prove it yourself. This involves as many functions as you have pairs of constructors and here with a single implementation, no done fusion, we're able to prove all the properties. You see the two first definitions show that suc is different to 0. We are able to prove that suc is objective using the same implementation.
I'm just going to give some idea of the technical challenges that we have to face to implement. We in to find a very precise encoding of telescopes for what independent cease are. We ended up with the scope with recursion. We also had to enforce data type descriptions.
Then the question was how to bridge a gap between this encoding of data types and actual Agda native data types.
What we quickly realised was that you should avoid taking the six points and implementing construction on that. You see what was a constructor what was used and the arguments. What we derive is only shallow conversion functions that is for some value of type A we're able to know where it came from and we are able to do the reverse.
Another variant was accessibility predicate. If you are unable to split the value into its arguments and constructor, there is no relationship between the arguments that you get and the initial value and so Agda will not allow you to recurs this many times and so you have to find a way to convince Agda to do this kind of stuff. So it was to introduce accessibility predicate that would derive coding and so it guarantees that any value is well-founded. Then we had to implement the construction that I just showed you, again with no reflection and that are caught by the definition. This is the only bit that I'm really sure. This is the content of the record. It only contains a description, conversion functions and access inlity predicate. That's all that was needed to implement all the skrungs that I showcase. So, yeah, whatever is available and it mostly worked and I think it is a good example or at least it proves that user friendliness is not exclusively safe, but you can have safe and generic that still are useable, but there are, of course, more work to do.
We don't support ... nested types, induction-recursion, mutually defined data types. We are looking for a fine year encoding. Our test scope at the given position gives access to all values in the test scope, and I actually want something finer E we are happy that there is other work being done, so it is very interesting and we are very much looking forward to find how to merge the work that we did to maybe find a good library out of the two. So, yeah, Thank you for listening.
(APPLAUSE).
Thank you for your talk. Let's have some questions. There's a question at the front.
This is really nice. I really like this. I also want in. Is there anything I can run from this if I want to make this in Cork? Could you conjecture or speculate on that?
I think the main would be that there isn't any explicit handling of continuous levels in Cork, and so, for example, we had to use [indistinct] to be able to be able to represent [indistinct] in Cork I believe there isn't, around so it might be impossible to represent such a large customer data types. If you restrict yourself to set, it might be possible. I think domination shouldn't be an issue.
Now we have a feature. Thank you.
We have another question just down there.
Hello. Great talk. I have two quick questions. One, I saw that you said you avoid without K when possible. So are you using the K axiom?
Yes, for decidedable quality.
I'm not precisely sure why we needed it. I think it had to do with examination somehow, the ... checker interacts with it some weird way, so for the other destruction it can be used with conmm ...
A lot of what you're doing here looks similar what is done in equation in Cork, and so the no confusion, decidedable equality, der invitation of well-foundedness are similar to you might want to take a look.
Thank you.
I think we have time for one more.
You've shown some examples where there is niece instances that you can derive, like, show decidable quality. What if you tried to derive an instance that it shouldn't be able to derive.
Does that mean the function, the derive whatever function is a partial function or you can't even do the reflect function that gets the description in the first place?
Right. One good example is decidable equality. In the general case you cannot derive decidable equality for the types that I've in in the two arguments. So what we do is kind of a trick, but when we applied the extension, it will ask for an instance of some data type and in data type is defined possibly on what a description is and it will enforce that there is no circumstance for the two arguments. When you fry the function, it will say you could not find an argument of not allowed. Maybe the usual experience is not great, but we are able to constrain which kind of descriptions.
Thank you very much.
(APPLAUSE).
Our final talk for the day is actually a virtual talk. I hope that someone will appear on the screen in a second.
I'm a PhD candidate. I'm here to present my experience with different kinds of compositions of high-order functions. This experience report was born out of a separate research project seeking to understand how well students understand dpigss of high order functions in general.
As part of this property I was tasked with creating new problems for the beginning of the certained course at brown.
The problems needed to be solvable with the high function orders on list and needed to be of an appropriate difficulty.
This proved to be a much more challenging task than expected.
I can propose a number of problems but all of them appear to be on the two extremes of the difficulty speck frum. I had the ability to generate problems that looked to be easy or problems that seemed to be much to complex but nothing in between in this sweet spot. What really frustrated he is the fact that I couldn't explain to anyone why this was true or even the difference between the two collections of problems. Why were the challenging problems challenging? After sfaering at these problems for months I came up with the hypothesis to explain what makes some of these problems significantly more challenging. Over the course of this talk I will present the hypothesis and some evidence and how and why we're seeking to expand this experiment even further. Before I get started, we are currently restricting ourselves to higher order functions on lists. Specifically higher order functions with the type signature on screen. These were the types of high order functions that we're working with in the course which is why we use them. Students were, in fact, restricted to a small subset of these high-order functions which were presented in the course. Pop quizz.
Consider the two following ways of composing fight-order functions. I will give you a moment to read over them. Which of these would you consider easier for students to understand and use, and which of these represents a larger set of possible programs? Really stop and any. I will give you some more time. If you're watching this video on your own, pause and really think it through.
This is vital to the rest of our story. What does your gut say? I will call composition A pipeline and composition B structural.
Let's look at why, starting with structural. Let's just do a bit of type analysis on this example that we have. We will wrap that example in a function called example fun which takes in a list of A and produces some type B. Of course, that means that L must have that same type, a list of A. We also said that we're focusing on list higher order functions. So we know HOFC will take in a function and list and output something. In fact, since it takes in L, it must take in a list of A. Meaning that it's fun L emust take A. This is inner.
We can fill in those lines as well. We can also say that H 2 FC must have the same type as example fun. So let's fill that in. HOFD is one of the high-order functions. Let's put that shape in here as well.
Similar to before, we can constrain the types based on the output argument which here must be the same as the one here so let's do that. We can also constraint the types which is inner and has type A. I would love to put that straight in, but I lose of fact that this must be a list. Instead we do some substitution. We will anies say that this high-order function takes a list of type X and then will replace all instances of A with X. It is a jarring change at first, but notice what happened. L is now constrained to not only be a list, but a list of lists. Said another way, this composition only works if the type of input list, L, was a doubly-nested list, and a similar argument works for any number of high-order functions composed together in this way. I can only nest high-order functions in structural composition up to three. I cannot not have it if it's two dimensional. The type constraints don't work out. This is why we call it structural competition. The competition must mirror the structure of the input data. This in turn means that when given a lift of the high-order functions which students were, then for any given list, there is only a finite number of ways that I kon structurely compose. The list high-order functions impose dimensionality constraints and then the high-order functions nested inside must confess those constraints. Now, of course, I can fill in this eat the bottom with in one of any number of functions. The set of possible solutions is a bit misleading, but still there is a significant constraint imposed on the space and synthesizers or students exploring the space searching for solutions should have a relatively easy time but easy relative to what. Let's it do the analysis for the pipeline.
Example fun once again takes in a list of A and outputs B which once again means that L has type list of A. HOFA must still have the type signature of a list high-order function and must still output the final result of type B. HOFB must be a high-order function and take in L which is a list of A. This A then constrains the type of its funarg. We know HOFA and HOFB are equivalent. A list of X.
That means that the funarg must take in an X. We're done. We see the key distinction between these two types of compositions.
I am no longer constrained by my input type in the same way I was in structural composition. I can, in fact, keep chaining pipeline composition indefinitely similar to the piping operator in U nix and other processing pipelines, hence the name. As long as I cannot to line-up the out put type of one to the input of the next. This means that that student as a synthesizer is likely going to have a more difficult time as a solution they're looking for can have unbodied size and loser type constraints. These list of X types can be anything as long as the next step in the pipeline accepts that same type of thing.
Does this difficulty match your intuition? It certainly didn't match when we started and it went against the intuition of many other experts we spoke to, but it does line-up with the experiences we spoke about earlier. In almost all of the examples of this gap I mentioned earlier, the easy problems were structural in nature and the more difficult problems were pipelines. Observing this apparent distinction in the wild we set about creating a more controlled instrument, a course assignment that would test my hypothesis that students have more difficulty with pipeline composition than with structural xomp sigs. So what is in an assignment? Well, for any assignment, a student takes in a problem statement and produces a solution as out put. So to design our assignment we need to both pay attention to problem statements and to solution structure. Let's look at the latter of those two. There are a lot of challenges involved in wrilg full working code and not all of knows challenges are germane to our question about composition. So instead of asking students to write code for in assignment, we gave them a prompt for text in the hopes of seeing their high-level planning strung tour. The full text of this assignment can be found in the paper but the instructions are roughly summarised here. You may have noticed that the instructions mentioned a box with examples.
We will come back to that in just a moment as we talk about how to present problem statements, the other piece of this puzzle. Here is one standard example of a problem statement. Given a list of lists of numbers, I will put the same lists of list of numbers but with the number five removed from each inner list. The issue is that most people who have had some experience with high-order functions will quickly realise patterns, like remove probably means to filter, from each indicates a map and we're basically already done. It is a map of a filter. What is even left, we have given away the statement. We tried alternatives. Specifically, we took inspiration from a previous education paper on high-order functions which used in put output pairs as the problem statement. This is what this looks like for the previous problem. The assignment gives them problem statements that look like examples like this and asks them to generalise to those plain text solutions or explain why that is difficult or impossible. Our students had, in fact, completed this task from the previous paper that we mentioned before starting our task so that we're already familiar with this input, output pair format with both our problem statement and our solution formats decided and with hypothesis in mind, we set about our experiment. Students were given seven problems total, three each with structural and pipeline only solutions, and one impossible problem. The problems were presented in a random order except the first problem which was always fixed to avoid giving the impossible problem first.
How did they do? The detailed numeric results are in the paper but they are summarised in the chart. We see that nearly all students correctly identified the structural composition questions and less than 75% of them correctly identified the pipe hin composition questions.
There is also a high degree of uncertainty represented by the imprel owe portions of the bars.
This is reflengtd further in the written text results. More students had the correct answer for the correct rope on the structural problems than on the pipeline problems. There's a more detailed breakdown of the written responses in the paper.
We know that in isn't perfect.
There are valid challenges to our choice of representation and this is a small sample at one university. Our goal with this work is not to claim a definitive answer, but to be a starting point to inspire this community to investigate this phenomenon further. To that end we are currently actively coordinating a follow-up study to this work across multiple institutions in multiple nations working in different programming languages. If in work seems interesting and you or someone you know might like to come on board, please reach out. You can find a detailed write-up of our current research plan on the screen. You are welcome to reach out to me personally via email offer on Twitter. We have owned an interesting distinction between different ways of composing high-order functions and a perceive difficulty gap between them. We have given the names structural and pipeline to these two kinds of composition and we have provided a small sample of evidence suggesting that students do actually struggle more with pipeline composition than with structural. We are coordinate aing follow-up study and I look forward to hearing from you and answer any questions you may have. Thank you.
I would like to invite any questions that anyone has.
I thought it was a really interesting presentation. You have a restriction on your high-order functions. Because of that, that's what causes this structural composition to unfortunately have the depth of the nested lists. Have you explored removing that restriction on the high-order functions, because I think intuitively when you ask the question which has a larger search space, I thought the second one because I wasn't considering that structure of function, but any kind of high-order function. If you do allow any kind of high-order function, does the structural one become more difficult?
That is an interesting questionquestion. The question of how do we handle different high-order functions in this, with this idea of okay, does composing high-order functions that are not list functions make them more or less difficult to work with and how does that affect the search space. Is first of all the focus of the ongoing work and so stay tuned. With that said, we played around with the types for a few different other common high-order things that you do with high-order functions. Specifically in the data signs and data processing world, not general, like, things that you see here at ICLP but things that were done in the science classroom, and so we're working with that on tables, for example. We're looking at how it applies to trees as well. All of them have similar structural restrictions, but the interesting thing is actually for high-order functions which take in a function and then some sort of individual item. If you need to get a hold of me afterwards, there is an example that I have of walking through what this looks like in that case and the structural composition still holds while you're working with an item that has structure and soon as you bottom out to a primitive, anything goes.
In the two example codes you put up there, in one of them you have two spots where people are fill in code and in the other only one spot to fill in the code. Is there a way to eliminate that as a confounding variable?
I can share my screen, so I will put that on the screen if I can. In here there's composition they have funarg and another one. Composition B has the one funarg, the pipeline and structural. I don't see a way to eliminate that as a confounding factor so to speak, considering that the whole nature of structural composition is that one of the funargs is another high-order function, right? I think that that is part of what potentially may make it easier, but I think that - I'm not sure that that is a confounding factor to in observation as much as part of the observation steflt. But maybe I'm not quite understanding your question.
Is there a way to measure the effect of that independent of pipeline versus structural? Like measure that specific aspect of the two parts versus one part and then you can kind of see which one is causing - whether it is causing the effect or whether others even more effect and you have that on top of pipeline versus structural.
This is an interesting thing to know and maybe something that will go as we're coordinating this follow-up study. I can't off the top of my head construct an example that teases this apart while still comparing, like, without explicit live using one of niece types versus the other, compare one funarg versus two, but I'm happy to think about that more and get back to you.
I think that's all we've got time for. Thank you to the speakers again.
(APPLAUSE).
We now have a ten-minute break.
I already announced in on in course. You're not going to be too surprised that we do have at least one confidence case, so I urge everybody to improve on their mask wearing and there is a channel on confidence so if you have confidence or you suspect that you have confidence, please communicate with the organisers and we have some self-tests at the registration desk and we will get more as needed. Thank you.
