>> Hi, so I'm going to begin with a disclaimer, is that this talk isn't really going to be about algebraic effects, but about something at least as cool about probabilistic models. So the best place to start is tell you what a probabilistic model is. I find it help to view it as set of relationship between certain random variables; that is the inputs and outputs of a model. And some parameters that describe how they are related.
For example, linear regression, which models, a straight line relationship between inputs X and output Y. Given X, We first declare how the slope intercept and noise around the line are distributed. And then the final line, so it's about our output is generated according to the normal distribution using these as parameters. To implement and arrest act with models as regular programmes, We have these things called probabilistic programming languages, or ppl. So what might linear regression look like as a program. That's the thing, in a lot of PPL depends on how you want to use the model. In monad bayes one way is to simulate from it.
And for this we provide an input some fix model parameters, and we then sample the output, according to its distribution.
On the other hand if we already new the input and outputs, we could be interested in inferring what the program FERs are. So in this instead we sample each of our parameters and this line says that we condition against the likelihood that our parameters have indeed generatered our output. So I had to define two different programs for the same model. If you look at other PPLs you will encounter the same thing, the main difference is whether we sample for a random variable or whether we observe.
And this then effects what we consider the inputs of our programme and outputs. But the number of ways we can interpret a model isn't just limited to simulation and inference. For example, maybe I want to explore the behavior of certain variables by isolating which ones we sample from.
Or, if your training data was incomplete we could first want to simulate some of the data from the model, and then loop it back in to do inference on the same model.
And so, this gives rise to a frustration is that: Every time we want to interact with a model in a new way we need to define a new programme. This motivates the first topic of our reef --
research, ises, which is that we want to define, just one model, and then defer until later, wherever we sample or observe from its random variables.
I can find an existing term for this. So, an exciting point in my research l I got to coin a new phrase, which apparently is already taken. But these a called multimodal models, and, in Haskell called prop effects, which uses algebraic effects to support multi modal models, and that's the only time you'll hear me say algebraic effects in this presentation. So let's take linear regression again, on the riding hand side this is what might look like in programme effects. Tightly correspond to the notation on the left. And notice that we don't have any sample or observe operations. In fact this representation is entirely syntactic all we're saying is mu is distributed according to the normal distribution and nothing else.
An important feature of the language are these hash variables here. And we call them observable variables. They indicate as a user you are able to later provide observations when executing the model.
They directly correspond to these type-level streams up here in the observables constraint, and these constraint say mu, c, and sigma and y are observable variables type double, and may been provided in observations in something called model environment. So will become clearer interact with the model.
So let's do simulation first.
Here are declare a list of inputs from zero to 100. And then I'll assign values3, 0, 1 to our parameters and not to our output, and this he is expresses to observe our parameters but sample for our output, if we sent simulate under the environment this generates model output as seen in this graph, and conversely can do inference by specifying different environment.
So with this we sample parameters providing no values and observe against outputs we can just loop in back values from simulation.
So you might have noticed I assigned list of values for each observable variable. And this allows them to have multiple dynamic instances. So every time hash Y is referred to at runtime, what happens is first Val from the list is read, and permanently consumed. If we Len run likelihood weighting inference, each iteration will produce weighted out put environmenters, and an output environment has exactly the same structure as an input environment. The only difference being, it now value sampled during model execution. So if you are interested in Val sample from mu, you could map forget function, and sorting values along side the likelihood give this graph here, you see values around Mu = 3, having a higher like LOOSHGhood, which is consistent with what we originally provided.
Okay, so this underlies for basic ideas of what you could do with mostly modality. But at this point, I should note that there are already a handful of great languages that support this, such as Gen Turing, and Stan.
But in these languages. This is typically done by having models exist, as some form of special language constructs and these constructs normally can't be manipulated as first class values, or when they get close to it, we still don't any static type guarantees about our model.
So we can do things like referred to random variables, but I don't actually exist. .
So... this makes it difficult to do two things in particular.
The first is to define models in terms of other models, and the second is to treat models in higher order way. By embedding into Haskell models are not only first class values, but also type safe which opens the door thinking about models compositionally, and to show you why this way of thinking can be really pragmatic, a cool example is an HMM or hidden Markov model. The idea is series of latent states X which we don't know anything about. But we doe know they're related in some way to set of observations, y. Our goal is to try and learn about X, given Y.
So if we were thinking about this compositionally, big brain, then we can see HMM as being decomposed into the two submodels. And first transition model, tells us how X transitions to the next sun, and second is an observation model, which tells us how to project an observation from the transition from a latent state. We can then define the HMM as a higher order function, which is parameterized by both of these. O do this, we just define the behavior of a single node. This just performs one transition and projects out an observation. And then the last line creates a chain of end of these nodes, replicate and folds over them with... and what this does is propagate each nodes output to the next node along in the chain.
And this is it. I think this is quite an elegant abstraction but we can take modularity even further. So for this I'm going to wrap up by showing you a really cool real world application of HMMs. The spread of disease during an epidemic.
This particular example is called the SIR model. The idea is that we assume a fixed population partitions into three groups. Those susceptible to disease and those who become infected and those who recovered. Given the outbreak of a new disease, the SR model is interested in tracking how these values vary over a series of days. So there are large number of reasons it's infeasible to know these true values especially for a large population, so for these you can view these as latent states of HMM but do know there are number of reported infection cases.
Defining the rest of a model, it's then a matter of just defining the transition and observation model. So, let's do the observation model first.
This is going to be quite straightforward. We're going to assume that the number of reported infections depends only on the number of true infected individuals I.
And use report on distribution, to say that on average a fraction row will reports for infections.
The transition model is a bit more interesting, because now we can show off some composition. So, will first say, but the number of susceptible individuals who become infected is given on delta SI, and this depends on the contact rate betta between them will then appropriately, subtract this away from us, and add it to I, follow a really similar pattern when modelling the transition from infected to recovered. And what's neat is that we can then compose these two models to give an overall transition, taking us from Si to I, and I to R. Okay, so we have our observation and our transition models. If you recall, the higher order function I defined before the final SAR model is simply this, we jjust pass it off to sub models. And this is great because we can now simulate the end of the world. Assuming the world has a fixed social population. So we'll give it an initial latent state of one patient 0. And everyone else is This shows the number of sue susceptible, and recovered over a series of 100 days. But the story doesn't end there. The setting of composition makes it straightforward to incrementally extend our model. Which can be really useful on trying to strike a balance on how we should model a certain problem.
And at what point is the model too DMREKS to being useful any more. Let's say we realize that people can be resue susceptible to the disease again. We can just implement this as a new transition model very simulated beforebefore. Suppose this. And this gives the graph up here. And let's say a vaccine is introduced. We can implement this as new type of population, and create new transition submodel. And compose it, and this gives this graph.
And so, reasoning about models compositionally like this is quite uncommon from what I seen, and I tend to see models and PPLs written monolithicy and from scratch. And from talking a few statisticians, I gathered anecdotally this is partially for historic reasonses, one being that when we formalize models. We care about things being self contained.
And so what might happen is that these models are just translated from this mathematically style presentation to a program. But I'm sure you are all aware the needs of programming is very different, and probabilistic programming languages, I think this translates to the need to reuse and maintain existing models.
What I'm hoping is that the ideas behind this language, or even just by taking functional programming approach can help support these needs. Yep, thank you so much for listening to me.
[APPLAUSE]
>> We have time for questions.
In the middle here.
>> Thank you I really liked your presentation, and really liked the example of the epidemic.
>> Sorry, I'm half Deaf could you speak up a bit. In the epidemic model there is transition from the sue susceptible to infected and to recovered.
>> Carry on...
>> I was going to... yeah, so about the order. Yeah, you really want the infected transition first and then sue susceptible. But you really also want some people to stay infected for a few days, so maybe you might need some kind of state to remember some information. So my question is how easy is it to combine this probability effect with other effects such as state effect or any other kind of effect?
>> Yeah, good question.
So it's extensionable. So you can have news or effect, but just have to be handled first before the actual effects of this language are handled. So this is the actual type of a model.
Which consists of two core effects, and what happens is you will just as a user call the custom handlers and run these two effects afterwards.
>> So composition of handlers rather than running two at the same time.
>> Yes.
>> Thank you.
>> We have time for one more question if anyone has one.
Great, let's thank the speaker then.
[APPLAUSE]
>> Next up we have Frank on constraint based type inference for FreezeML.
>> FRANK EMRICH: Hello, everyone, indeed I'm frank and going to talk about constraint based type inference for FreezeML. Once I found the clicker, there is the clicker.
How does the clicker work?
Like this.
And this is joint work with Jan Stolarek James Cheney Sam Lindley Sitting over there. I'm taking you on a journey visiting the past revisiting the joyful decades that have coined the technologies or other techniques we're going to be using. And we're going to start in the 70's. Because something happened back there many of you are familiar, Hindley and Milner developed the, type system, and everyone was very happy because suddenly you had a statically typed language without the need to write type annotations, and you still got to complete type inference type inference for it thanks moving into the 80s and 90s. And now, people did a lot of foundational work on constraint based systems. That means systems that have constraints in the type system itself and using constraints to perform inference for your language.
And nowadays these systems are considered much easier, to be much more ex-sentenceable of type inference, than ad hoc, and DEM STRAITed in theory, by constraint frameworks.
And which is pair meet parameterized in the concrete consequently next. And then once you plug in your own accent that you get a lot of kind of nice properties for free. And in practice, this is demonstrated by GHC and many extensions being based on constraint base. And there is another line of work.
And want to highlight these boxes here. And in no way claiming people stopped working on these at the end of the 90's, but showing the time our work is based on.
But, the other line of work I want to talk about is the work of the people who were unhappy with the limitations of the ML or the HM type system. And those limitations are two ones, first of all, in the Hindley Milner type system, you only get pre next polymorphism That means that all of your quantifiers in your types need to appear at the outermost level. T That means, if you want to have a type such as this one, which could be for example list of polymorphic list permutation functions, you can't have this type in an HM system because it hasa quantifier nested in another type. Did the microphone just go away?
That's bad. Can you still hear me this way?
>> Shout.
[Laughter]
>> Hello, 1, 2, 3.
>> Yes.
>> Hello, hello.
>> 1, 2, 1.
>> Speak loudly and we'll listen carefully.
>> People in the stream will feel discriminated I assume.
Shall we fix this now or continue. Hello, hello.
Yes, does this still say it's on.
>> If you can't fix it now have to continue, and loop in the people on the stream once they fix the microphone.
Ooh, fun. Okay, so... another limitation, with ML, you only got one amorphic instantiation, that means, if you want to instantiate when you were quantified variables such as thethe -- here, not allowed to do in HM system, so the idea people try to extend the HM type system, first class polymorphism GHR first class polymorphism means allowing these two features here that the HM system itself doesn't give you. His led to a relatively large design space, because you can make a lot of trade offs between expressivity. While still trying to retain type inference, and once this design spice FreezeML, the system we published 2020. Focused on simplicity, but the downside, the type inference algorithm was another ad hoc adaptation of AlgorithmW. There was a big scary click, is the microphone back.
[Click]
[Laughter]
>> Well the starting point, basically we had this system based on algorithm from the 70's, and goal for this work is move FreezeML into the future, and by the future I mean the 80's, and 90's. So it's the future relative to the 70's.
Okay, I'm going to continue by giving you brief 1 on 1 on FreezeML, because it doesn't make a lot of sense to talk inference for a language where you haven't seen how the language works. Class for dimorphism to like language one of these kind of questions you have to answer is how you want to deal with instantiation.
[Clicking]
>> I'm not sure I'm getting fireworks already for how the talk is.
[Laughter] [APPLAUSE]
>> But... [click].
[Laughter] >> assume we have function single and creates single list, and identity function with the type you would expect [clicking].
[Laughter] >> would at least set consistent rhythm could get use to it. And now want to think about what happens with single to ID basic LFSHG have two options and, that we instantiate ID, and then we get list of A to A for some type A.
Alternatively, now that we have first class polymorphism we may want to keep ID polymorphic which gives us a list of something polymorphic. And FreezeML answer to the question is freezing.
FreezeML is designed to be conservative extension of ML and normal variables they're always instantiated. Basically works like a syntax directed presentation of a male, where all of the top level quantifiers of any variable are instantiated Once you touch it, the difference being that the instantiation can be polymorphically now. And the key inGRENTs are the frozen variables. Prevent this instantiation. So, in other words, frozen variables are best basically system F variables that keep the type just as you would get it out of the environment. And finally the last ingredient generalization FreezeML, syntax accurate presentation of ML, that is at lead bindings. So, in terms of syntax, what we add to and kind of an calculus, frozen variables and type annotations, optionally, on binders. So let's look at examples with the single function, and first example I had earlier, first plain unfrozen ID and there are both instantiated and on the other hand, if you take the frozen version this is how you get the other type. The one where you keep the polymorphismand FreezeML. And one other thing I want to point out is that if you try to do this, you apply ID, frozen ID then this doesn't work. If you freeze ID that type is polymorphic. And we're basically treating these types justice system effort again, and the polymorphic type that FreezeML is simply notes... type and can't be aflied.
And another thing that deserves some attention let bindings already mentioned that they are the mechanism that provide generalization in FreezeML. And now we're going to look an example and this is the one where we define our own identity function, and then refreeze it and apply two three and first observe, give ID, mention the polymorphism type earlier. So if you give IDpolymorphic type.
This doesn't type check, now the question is, what a type check, if we give ID the type into end In which case the freezing doesn't do anything and you could apply to three. This is something ordinary ML system would allow you to do. The answer is no.
>> We need to be careful to ensure we retain the existence of principle type in our system.
And because of this kind of the system F types that we use, there is no direct relationship betweenn. So for example consider variation of example, we return the frozen ID and body and that's it. Then you immediately... the allowed both of these types the polymorphic one, and some instantiation, as the types of X. Could be either of those, and there is no more general type and personnel that subsequent service. So, this would mean we lose principal types. The solution to this problem when you have let binding of the form, we have principle type of N, and Jennize that. And our concrete example that simply means we're ruling out the program earlier, the type of ID is going to be polymorphic one, and the body late binding just doesn't type check, take the step over to type inference. In general, constraints type inference has this nice separation of concerns you independently define how you translate the term to a corresponding constraint, and then independently define how to solve such a constraint, and then is satisfiable if the original term was well typed.
And concretely in FreezeML, the most naive approach would be to say, well, we need to reason about types, we have type variables in constraint based system that's just allow them to be polymorphic. We also need to be reading about lets an instantiation constraints and be done with it, and you won't be surprised to hear that this doesn't work. Our system with these unique ingredients, immediately lose the existence of principles solutions for this constraint language. So unsurprisingly, everyone who does constraint based in system needs to come up with a kind of system to contain it respect the polymorphism That means that what we want to do is that the only reason about instantiation of types, where we fully know the polymorphism, or in other words uniquely determined where all of the quantifiers appear and the type. And though, those are the only types that were allowed to be instantiated, and practically in order to do this in our constraint language we adopt an idea that is probably best known by being used by Pottier and Remy, where they also follow this idea of having turned rebels in the Constraint Language. So we have these four constraint forms that I'm going to talk over in greater detail in a bit. T's important that these kind of term variables are the only ones for which we may impose instantiation. And finally, type variables in our system are allowed to be spent for polymorphic types in general, but there are certain situations where this a certain variable may be forced to morphic.
So let's look at these four constraint forms in greater detail. I already mentioned, we have these instantiation constraints that must mean the type of X instantiates to A, then we have this freeze constraints which are asically the same as equality constraint, ... oh, wow, surprise.
[APPLAUSE]
>> This is going to be confusing talk for anyone that missed entire half of it. Now to constraint solving, last and most interesting constraint, where the idea we use constraint to determine the type of variable X, so let's look at those in greater detail.
General form looks like this.
And again, the idea is that we, the constraints, X and then bound in C1. Sorry, in C2. And again, we want to preserve the invariant we fully know where the polymorphism curse in the type of X and concrete semantics of these constraints as follows.
Funny operator down here is like an existential quantify variable, bound in C1, nd this type that we choose for this principle type is choose for a this then generalize to give you the type of x, which is then bound to C2.
So what we can see is that this principality condition from the terms translates over to the Constraint Language as well. Now in terms of how we concretely do inference, what we do is we translate a term and a type.
To a constraint. Where the capital A here is basically the expected type. And doing inference, so we don't know the actual type ahead of time, so assume capital A here, type variable in general. And put these on the slide thinking they would be tiny and wouldn't be able to read them. So I'm only going to tell you one thing, let terms translate directly to let constraints in the it system.
And because we have a bit of time, the rest is relatively standard, use freeze constraints for frozen variables, and the rest deals with Val restriction, which I don't want to talk about here. So in terms of actual solving, defined constraint sol VER, that works as state machine, and if runs successfully, then you can read of the principle type on the final day of sol VER, and essay would hope for, doing translation and solving, gives you Sound and complete type inference algorithm for the original specification of reasonable. So in conclusion, you'll be unsurprised to hear that If you want to do first class type entrance for first class polymorphism, you need to be very careful, restricting polymorphic types may occur and restricting on what types you can impose instantiation constraints. Our approach to this problem is introduction of term verbals in the constraint language, and relying again on the fact that FreezeML type system whenever a term variable enters it is enretirement, the type is uniquely determined, o monomorphic instantiation meaning, we know where all the quantifiers. And then we only allow instantiating the term variables. And as also, as I already mentioned this principle that we had term level, carries over to the Constraint Language, as well, and to the best of my knowledge this is the first constraint based. -- inference approach, that has principality in constraint language.... Went over a lot of details in the talk, I mentioned that type variables are polymorphic by default but sometimes they aren't, a mitted when they need to be polymorephic.
Also a bit of a wrinkle in the semantics of let constraints that has to do with free type variables occurring in this first sub constraint and how these also needs to be tamed, in order to retain principal types and its future work. We would like to kind of actually get the benefits based type inference by actually being able to more easily handle extends. So one way would be to actually handle extensions, but something else would be to actually go all the way and define our own kind of constraint framework in the style of HMX, something like --
like --which is then been kind of fully generalized over the concrete constraint domain, but that again is future work is something we haven't done yet.
So thank you, that's it.
>> We do have time for a quick question.
>> Yeah. In the middle.
Richard.
>> Apologize if this was in the middle of the talk had a hard time hearing then. Are there programs, so I understand you took the FreezeML and translated that to constraint based... and implementation of this are there now new programs accepted and what's the nature of the new program.
>> Sorry, now I'm having a hard time hearing you. Are you asking if there are programs now we couldn't type check before.
>> Yes.
>> The answer is no, the current new inference al algorithm is supposed to be exactly the same as the old one.
The hope being that it is now easier to analytics and sentence we have for example, at least a brief look at for example, how row polymorphism could be handled. But of course, I would like to go to an excellent framework that kind of is extensible by design, but we're not there yet, however, was really just to get something that is constraint based and equivalent to the current system.
>> Thanks.
>> Let's thank the speaker.
Thank you.
[APPLAUSE]
>> Next up we have Arnaud on linearly qualified types.
>> ARNAUD SPIWACK: Hello. Is the mic working fantastic. Hello, everybody.
So first off I have to say this article we're talking about is part of the linear Haskell project, which, despite the name isn't really about Haskell, more about the linear part. It's Haskell because we have to choose a language, because I Val my sanity.
Usually in my first slide I put picture of all the people that worked on the project, and quite a lot now, and I kind of gave up.
So instead I put a little timeline. And today for scale.
So what's linear Haskell. The goal of linear has call to add linear types, and we made a lot of progress, to Haskell, or any programming language that already exists in a certain sense. So we're focusing on Haskell. So I'm not going to mention any other programming languages. I'm sure it works in anyway.
So explain for a second what a linear type is, the idea is to add an extra function type.
There is a number of ways to do that. More or lessee equivalent, and the way we done that, is just add a new arrow, which we write, a potent, Aerio.
And function type linear function, read that way.
Promise, use the argument only once. And say, if it's consumed only once, then the argument and consumed exactly once.
And, oh, New York City you would say, why do you need something like this.
Why would anybody need something like this. That's an excellent question. do you need something like this.
Why would anybody need something like this. That's an excellent question.
The thing is producing a value, linear function from A to B doesn't buy you. Not just, okay, made a promise, but it's not really useful to make that promise. What is useful is to require linear function from A to B it let's you constrain the API from A to B. You can instance force some resources to single thread fashion. And once you force single threading, then you can do all sorts of thing.
Like allocating data with Maloc directly, bypassing GC and still memory safe. Not going to quite do that today, because it's a bit involved for slides. So we're going to do simpler version to have immutable data structure, so I'm going to put up some code for mutable array.
What's interesting about that is, normally you don't have a mutable array but I have a pure interface so really what I'm saying is I have a data structure that is pure array, but under the hood, I'm going to implement that mutation is presumably that is much more efficient. Really expends what are you doing. So give up sharing, so not free. But that's not the point I'm getting at.
The code on the right is really ugly.
And if you look at the code on the left, written in. Mezzo language, you know exactly what I written naturally. And here swap to values, and all the examples in this talk extremely contrived because... slides.
But... yeah. I'm a bit jealous.
On the one hand it's understandable, Mezzo is language best to speak about mutation of data structures, includng arrays, and dedicated type system, and I have general syrups mechanism, single type, I would expect some degree of boilerplate.
On the other hand jealous and the syntax highlighter, named arrays A's, and pass that as the keyword as, doesn't exist in Haskell, first it's very funny and highlights how ugly the code on the right is. And how many times I wrote A's, and so made the effort to make it look as silly as possible with the prime signs. General idea, I named the same array 4 times, we're accustomed as a pure functional programmers to return a new data structure when we modify it. But in that case we even need to return a a data structure when we read it. So considering this, this is just not very nice.
Like, half the code here is renaming the array. -- not half the code. But significant proportion, obscures the idea, and makes the experience not very pleasant. And so my users -- that's you by the way -- I'm not going to use that. And honestly, just figuring out something is single threaded and any computer can do. Doesn't need me to tell you yeah, yeah, because it's prime and not A.
So I wanted better.
There was, by the way, four other authors by the paper, I don't mean to represent their motivation, everyone has their own, that was mine. It runs a bit deeper than this. And today focus on that. And if you want to know more, you see me tomorrow.
The important point, and frank did a lot of my job for me.
Thank you.
Is how does GC in that case.
Handlele. And how does GC do work for me.
So I'm going to clean up this so these things are called constraint they appear both in in the type inference algorithm and also n terms, which is going to be okay. Just clean it up so that we can see. So when I'm doing inference, much like in the previous talk. So GC talks down the term, and license plate aL I occasion, and see constraint, and say by the way if you call nub you better be able to satisfy the EQ constraints, and second phase that... and on the other hand, every constraint more or less can be manifested to the left of far arrow like this. So I even if I don't 100% need to. And I'm saying when you use API, you need to do that and that's exactly what we want, we dealt with it or yell. So that's the core idea -- getting ahead of myself. Important point we need, is how it this fat arrow implemented. Ultimately, to a thinner a perfectly normal Eric, so in the language of GAC it becomes an argument.
And so... that's where the ID come from. Let's add a new fat arrow with a person one in front. And it's meaning, is given by the fact inside need to pull that off give me a minute, write code, with linear constraints in it.
And the meaning of this is just given by the linear arrow. So don't really need to add any real power to GC. Except to tell GC, you can infer that. So the type of set doesn't fit the slide. And try to make it smaller. Size so big probably going to pull it off. Realized that this morning. So I wrote set, and there are things you can write code, the same function, and now, there is boilerplate.
But not like silly boilerplate.
Something you notice, there is single as on the slide. It's really convenient, there is single A's on the slide, this is not the linear quantity any more. The function doesn't say, I'm going to use a single M...
knead it Instead what the only linear quantity that we have is this readwrite constraint, which says I can read and write on my way.
And because it's completely implicit, to see it, I just get an error message. If I do something silly. That's what we set out to do efforts, as it happens. ... there was another thing in linear Haskell that was displeasing users. If I'm allowed to come back to what I said very early on. Producing linear function is not very useful, what uh-uh need is require. So if I am write a mutable array library have to ask for linear fashion at some point in the way, it typically happens is, when you create an array you don't return that the array, because it would be unconstrained. I don't really have time to go over this. You have to take a function that takes an array as an argument, consumed it linearly and return something that is not linear, that's what the earthing means.
Yeah do you like that. Most people don't. Not as bad as it looks in a way. You will have to kind of scope, the resource exists in this scope, that's why I call a scope function, and why not have the scope be de limited by a function. The thing is does really cause problems. I gave an example here, very tiny, ugly, but does cause problem down the line, especially with several arrays, and want them to interact, and quite BESy.
Nova Scotia have to go after the, after the talk.
Clear.
>> So it caused a lot of problem, and not pleasant, and not the way people want to look at array. And a lot of people so far that have designed linear API has gotten this wrong at least once. Theyirect style, and the API was not safe.
So at the very least is not intuitive. So enter the linearly constrained. It's hard to implement the linearly constrained than just in your conference in general. I just a special constraint that has some properties because it needs to change and weakening. To be completely useful. But then you say okay, I'm creating a new array, licensely, and just say that. And I think it fit before and it was a bit too big. It looks okay.
And then, I wrote the from list function in the direct style, what we want to write from from list, and also this is not the right for form list, and said all the examples were really contrived. That's the best example of examples being contrived. I mean.
And so how do you pull that off?
Because so far -- it's not that hard. So you can design declarative testimony with constraints and the system is close enough, and the linear Haskell without constraints so that the sugaring becomes becomes obvious. You elaborate that with a constraint generation algorithm. There's a few things to think about because of linearity. You saw the constraint. Some people have already done that glad that they do work for me and I will have to read the paper, this 15 pages of technical developments so goes into more detail. Until then, can you follow the tweag blog where I blog about the subject from time to time. And here are the slides and QR code also brings you there [APPLAUSE]
>> We have time for questions?
On the side. The swap example, you have to use C bindings that intuitively make sense
>> Which version.
>> The one here.
>> Intuitively make sense why you need to do that. My question is the type checker, yelling as you if you forget to do that. Or silently disguard the effects.
>> It's not completely implemented yet. So the thing is, there is similar restriction already in GHC. You put the pattern on, lazily, the constraint it pacts will not be available to you. So to have such a nice syntax, you need some modicum of essential types also coming to JC hopefully.
Other disease WIES you use -- in which case get that for free.
Don't tell you strict pattern, tell you don't have read write access on the second get, I think.
>> Yep, on this side.
>> Can you tell us a little bit more about the implementation of linear Haskell.
>> Linear Haskell.
>> I think I had a slide at the very beginning, you can use it today. More recent there's more pleasant or maybe the less unpleasant, it's going to be, I think so there's not completely polished. There's more coming in the next version,.
But it exists as library called linear base in which collaborators try to build a standard library forgetting started with linear types.
Something that is missing today and bit of trap if you type let or where, the variable will be unrestricted. And that's change in the next version of GAC.
Work being done, so mostly polish.
>> Can you turn to the slide with the linearly constraint.
>> Linearly constraint. This one.
>> So Haskell, obviously uses -- extensively. If you have a multiple argument function, and partially apply it, linear constraints exist at the very front. So, with that, if you were to just partially apply it, and use that once, it would discharge the constraint, if you pa partially apply it.
>> And the type checker will ask you, please, please.
>> And/or might happen, consider you use linearly more than once, and tell you, that you used linearly more than once, and not super helpful, that needs to be next frontier for the system.
>> So in the sense the linearly constraint encodes the idea that there is some Val that needs to be linearly consumed.
>> And the magic is just in the type inference.
Because at the end, just creates a function, the way you generate linear constraints, and insert scope, and that limits your usage than normal linear type stuff.
>> Let's leave it there. And take the rest of the break.
Let's thank the speaker.
[APPLAUSE]
>> We have a short break right now and resume at 5 o'clock.
