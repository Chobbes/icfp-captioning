>> Hello, everybody, welcome to the program verification and synthesis session, I'm Arthur Azevedo de Amorim, and first speaker is going to be Steven Keuchel, who's going to tell us about verified symbolic execution with crypkey specification monads. .
>> Thank you for the introduction. And thanks for attending my talk. So this is the program verification session, and that's also the application of symbolic execution than I have in mind.
So the general setting maybe something like this given an implementation , we want to derive, if it's conforming to specification. For instance giving pre, postconditions. If you are serious about it might want to do somewhat formally, and if you are very serious might want to mechanize it in approve system. FT the general approach, or the common approach is something like this we embed the syntax of our language, deeply in the proof assistant, for assurance we use shallow embedding, and rep sent assertions somehow using the proposition of the system. We then use facilities it gives us for instance to use simplification, and reasoning root of logic and system will check each step is actually sound, so that means program verifying is essentially metaprogram, and each run of the metaprogram is verified to be sound individually.
What if we actually use the deep embedding for assertions at well, instead of implementing a metaprogram, we implement a normal program into the proven system.
And so that means that well, with the goal of also proving soundness, but proving soundness once and for all for every run, so this is what the talk is about. It's systematic approach of how to use symbolic execution to build such program clarifiers, semi automatic, in general might be in a case it's undecidable, and need some... we need some help from user to actually perform verifications.
And other contribution I also want to talk briefly about, Katamaran, this is program verifying that we're building specific language for instruction sets. The structure we're proposing is as follows:
Essentially build the system twice, once you implement a symbolic executer, and implement shallow executer with the same structure, and same behavior, and using shallow embedding, and we want to make this proof, so we want to reduce the proof of the symbolic executer for the shallow executer, and make as systemic and hopefully pain-free as possible.
Once you are in the shallow world you are in familiar waters, and you can use the normal techniques for instance, to show that this yields derivatis on in the programme logic, and Katamaran specifically specify the, axiomatic program logic, we proveexecute a sound with respect to that one, we instantiated the iris framework with the operational semantics, language, and we proved that the axioms of the program of the code in the iris model. So this talk talk with colour codes, will all be symbolic executions and for the shallow one, and use capitalized and noncapitalize letters, so you don't lose orientation, let's start with shallow execution first. Preand post condition. Bit clunky, and want to change the setting a bit. So in general want to talk about verification conditions which are just formula sufficient to actually prove derivation in the program logic.
Most SPIKly want to talk about -- precondition one way to implement verification condition traditional naive way. And we look at team of weakest precon operator for instance, in statement of imperative language, it maps out puts to predicates. Maps postcondition to condition, and weakest precondition. In a proof system, we might actually want to take predicates to be just mappings to the propositional types, If we use this definitions we can SHUFal the type around a bit and mapping the input to the cop out put. And, continuation monad with answer type prop can result type output. You might have seen this already. So in the past there have been flurry of research papers exercising such monad, and promising to give them to us for free for all and forever. And the idea is we use these monad to write your specifications in. And if you implement the computations specific monad. You can derive a verification condition generator and you can use that to reason about monadic code. So that's very common in thesystem where lots of authors work on.
So what if our monad code is monk interpreter, just what we did previously, that means this interpreter basically implements the is he Manitobasive object language as weakest predicate transformer and use that to implement a verification condition generator.
If you want to add side effects to your interpreter that's fine, just use monad transformers, and it will give you transformed continuation monad that has the right shape of postconditions.
Okay let's do this.
So let's maybe interpret an expression language, and derive something that computes a Val in such a monad. So if you look at if case. We want to evaluate the condition first and depending if it's true or false use one of the branches, so here implementing the object level if using the metaLEBL. So using pattern matching and have to think a little bit ahead, and want to... THAET that one do in this case.
And so somehow represent pattern matches also explicity, or go traditionally rout, and avoid pattern matches and translate them away. So the rules reasoning route so we could preconditions, which use the logical connectives of your social language to implement these. Can abstract away from these a bit. These features if we really want to work at the level of abstraction of monad corNOND to actually angelic, and demonic nondeterminismm features.
So we can implement invalid non determinism with the underlying exist, quantify your demonic with a formal binary non determinism using dysfunctional in conjunction, and then we can also implement guards, for instance. And can assume certain proposition for the remainder of execution path.
So if we use this to... and seems something we also can implement in symbolic setting.
Just note here the control flow constraints that assumed are qualities that will generally be the case for almost every language.
So look at symbolic executer, talk about what we want to represent first, so the propositions that away want to deeply embed, instead of shallow embedded propositions.
So symbolic values. We using existential and universal quantification, we have to represent them somehow we are defining a set of logic variables. And then we also defining a set of symbolic values are symbolic expressions which have all the formats of values, values but also I have a new case for the symbolic variable. The control for constraint specifically as I said, our equations between symbolic expressions, but we can also add user defined theories that come from the key impose conditions, which are done in a user defined subset which really can be anything. For symbolic proposition, we have to implement something that actually allows, angelic and demonic non determinism features and provide version of logical connective that we use constructors, and so monad uses symbolic representation as an answer type and implement weakest preconditions. And then, generate using that. And those are challenges, and so now all sudden have to deal with variable binds ourselves and ensure all the propositions we generate are well spoke and formed and well typed. And we have to reason about this, right?
Fortunately we already have the machinery to actually do that.
This is very similar to the case of dynamic allocation of memory.
For which we use possible word semantics, and can also use in this case to reason about logic variables, and another challenge that is coming exponential explosion because of more or less naive implementation. So what do real listic systems do.
They try to do an allowed to avoid explosion of execution paths.
And so might want to for instance, detect when assertion is actually unsatisfiable. So for instance, if false = true.
In this case, in the second component hereof the con junction we have whole subtree which we would need to compute which of course we will just throw away in the end. So when we already detect this unsatisfiable constraint, we want to detect and put a false there. Not only do that locally, but keep track of all the past control flows constraints seen and maybe restrictions and constraints that come from the precondition, from postcondition. And how can we incorporate that in our reasoning. And quick example, where we pattern much on same variable twice, and execution branches is of course impossible. Talking about symbolic execution without a trace of the tree.
So if not here in the in the T con corresponds to a control flow point in the program. And as you can see in the middle, there is the point, where x, and left in the other one. Sorry, left and right at the same time which is inconsistent.
So people in symbolic execution, not only represent the problem state, but also contextual information, so the control constraints from a note to root of the tree and all the logic variables that have been introduced.
So what if we just took these constraints and put them in our worlds as well. So use possible world semantics to use the binding and keeping track of logic variables, and also use to manage our constraints, so define, like that. And accessibility means we can add more constraints and more variables, but never take anything out.
So, how do we actually program this?
We can... so in order to do that we need to keep somehow track of these worlds.
So it would be best if we could abstract away from this. And show you how we describer define some of the... that will more or less hide bit of the details, but essentially, will keep passing worlds to... and dealing with them explicitly, and further more, we will not only pass them in implementation, we also index everything that comes after by the world. So we work with families of World Index types. And we do that, to make sure that, well, all of our propositions in all of our symbolic values are well scoped and well typed. Which other people have done before, and also make sure the constraints, are consistently moved. And can't move Val generate ated, under a certain constraint for instance, with sibling sibling in the execution path. Or to the parent in the execution path in the execution, sorry, because that would be unsound. So we can define form of function types, so world W. This is just function of corresponding instances of family members, so we go here, monoo logic rout.
And define explicit box operator, and make sure type or specifically functions monoo tone. Which we do by just universal quantifying, accessible world.
So basically what we want, what we have seen here previously, we have 3 leafs or 3 subtrees, in which we use to post condition.
And these are in different worlds, but at the very beginning of the top of this.
The condition tree, the postcondition was in single world. So we define the predicate, and we show post conditions are actually monoo tone. So for the programming, we can define all of the abstractions so assume, assume cards and so on. Using these primitives, and world passing here, accessibility... with us, and otherwise the progress really has same structure. So pruning states. Sometimes we can do more, having access to the world. So you might take the current world. And formula that we are sorting and running a silver, which will tell us, the formula is valid, or ininconsistent with the constraints we seen previously, so could get the execution this point.
And so actually skip this. The keyword here is define Kripke index logical relation. That actually links the implementation sounds very much like an equivalence proof. T's sort of an equivalence, you want to show that the symbolic computation is actually stronger than than the shallow one . For implementation Katamaran, as I previously said it's separate from the clarifier implemented in it's not fully automatic and semi automatic. And we also have some escape hatches so we not, don't do everything using Katamaran, mostly really everything on boiling plate.
Where you just want to apply very basic rules very basic and so on. For the very hard an interest in cases, these we'd actually do manually. In, in the iris framework. So semi automatic. We allow the user to provide some automation himself, so whole system is parametrized by user theory, and able they could reasonable steps, those...
residual proof obligations.
>> Just as example, separation logic, a case which has been implemented in some other systems, keep an f systems or Raynolds.
Heap allocated linkless so mutable data structures. So we've tested that the verification times in via C in the separation of recommendations and bedrock, which you forward you will find the numbers only in the paper.
So here are the results. As you can see for the verification times Katamaran is 1 to 2 orders magnitude faster. Which is consistent kind of like with other observations or computational reflection speeds up proofs and with this magnitude. Means that we couldn't actually find implementation in the codebase of these also lemmas, hat we have to prove consensus folding and unfolding of the linked list, .
>> So in the paper you will find more information about symbolic separation logic about adding debug information and about case study, which is a bit larger. So in conclusion possible semantics reason, about logic variables, and symbolic execution and have program verifier with competitive performance, thank you for attention.
[APPLAUSE]
>> All right we have time for one quick question from the audience.
Here. Gabrielet.
>> Do we have volunteers.
There.
>> Okay, so I think in more theoryyetical part of the world one is use Kripke keyword, for secure handing of logic, and one mind blowing trick, and conditions and passing style, and get symbolic executer, it's beautiful and very complex. So my question is can you do without that part, writing symbolic executor, would it work, would it look the same, or do you really need the continuation monad.
>> You don't need the continuation monad. You need some way to represent control branching, and continuation are in my opinion easiest way to do that.
>> When we write evaluator, we don't do it in continuation passing style.
>> Yes, but... I mean, you might locally branch right. But you might still have somebody you called to which you have to return to multiple times.
So have some form of representing that.
>> Let's thank our speaker again.
[APPLAUSE]
>> The next speaker of the session is Lisa Vasilenko, who is going to tell us about safe couplings.
>> Hello everyone, my name is Lisa Vasilenko I'm going to present joint work with Niki Vazou, and imagine we have two basket ballplayers throwing ball to basket with different chance of success. And we ask them to perform and throw each. And interested about two comparison questions about the results.
So if we knew inequality between chance of success, does it necessarily imply inequality between the results. Or even more, if we knew the difference between chance of success, can we know the difference between results in the end.
So this sort of question can be called a relational question, and the basketballplayers here are the illustration of probabilistic process. And so in this talk, I'm going to present a library for verification of probabilistic programs in Haskell, which I do in relation style, and also do in Haskell itself. And I continue with the balls and example of verification, and using safe coupling, and then explained we use library for more real world examples from ML. To define a probabilistic program, I will need a bunch of primitives at the word list. I want to simulate the practice of throwing about to event, which is going to be a Bernoulli distribution. It's a flip the coin distribution, it takes the first argument probability p, and returns the integer of uncertainuncertain value. And zero probability one minus. So the CRM, I use as a type for uncertainty. And since they promised to use Haskell as verification means, I need more elaborate type system. Use refinement types, which is provided by Haskell plugin liquid Haskell. So the new probability type here is a typical deployment type, it has two components, the first one is usual Haskell, type and double will give it a name. And then the second component of requirement type is a predicate that says that this double always has to be between zero and one. Then return type of Bernoulli also changes. We had done certain integer, and now we also know that it's necessarily 001. Define a monad interface for my type zero. So the pure is a single on distribution it takes one element of type A, and produces a distribution which is So I have a distribution of elements a and a function that can operate on a single sample that distribution, and result is the result of something one element from the first distribution and applying the function. .
So now that we define all the primitives, we're going to define the probabilistic formula.
Which is going to simulate one basketball player. Here I use a do notation which is a shorthand for to bind the operations and the function takes the number of throws the probability of success, and returns the score, which is probabilistic in the base case we didn't do any ...
throws so the result is constant zero.
In the inductive case we sample from recursive call to sample from a distribution flip the coin, and the results at the results and that's our result So, remember we had two basketball players, and relation property about it becomes the relation property between two programs. Now, interested to find out how the result of...
relates to bins nq. And there is he will can't way to verify programs, called relational types, so what typing judgment of we can compare left hand to it's type, and with some relational assertion. Which is very commonly, one is in sense better than the other, expression. Or oops, sorry, need to go back.
So the other common way to define assertion, American assertionassertion. And so in my system, I combine this two common approaches with a single connective, which consist of real valK, and bool predicate P.
And so numeric types for primitiverequire that there is a distance function. In this example, withstand some natural numbers absolute Val of the difference between two natural numbers. And meaning of judgment, the interpretation is the distance less or equal to numeric, and predicate and should relate to TROOU. So now that I defined the meaning for primitive TIEPTS, I go and for expressions that are distributions I stayed at judgment, there must exist, two distribution represented by left and right expression. What is coupling, imagine our left expression was flip of coin, glanced silver coin, and the right expression is the coin that commonly, more frequently ends up heads up and so the couplealing will be distribution on players. And flip both coins at the same time, and we want duration. How probable it is that one coin is 0 and other ises 1.
It's A, no, it's C.
So in addition to be correct coupling, these distribution pairs, need to be satisfied by more.
Which means if you ignore, one coin... if you ignore the golden coin, we just add the probabilities in the rows, and it should be one half and one.
Now, the can be in general case, more than one solution to the system of equations, and we put additional constraints on the requirement for coupling. First of them, every player in the coupling. Samples of the players that are now primitives, should satisfy the predicate if they have nonzero probability.
The second requirement is expectation of distance, which is again on sample from new, which are primitive, finishation of this distance should be less or equal to K. Now as promised, we're going to verify or system in high school so I need to translate it to our type system.
So first of all, I translate our typing rules which I have for each of the primitives such as for example Bernoulli distribution. This typing rule says that distribution it's disstance between Bernoulli is less than or equal to absolute value of the difference of the arguments, and the lifted relation, the predicate is less or equal, if we knew ... so it translates to two assumptions, using the eword assume, and swung on the slide, and distant bound, and type of assumption, you can see inequality that we required for the meaning of the judgment.
And so having the building blocks as typing rules, we're now ready to prove our statement. So this is how you write the theoremal statement, pretty much the same but doesn't have... which means we actually need to prove the statement, so what the statement tells is the distance between two runs of the program, P and Q, can be upper bounded, multiplied by q minus p, this is what we're going to prove, and in the assumptions, as we had before .
>> Going to the proof, I will start with sentence fashion. If the program you have two cases and start with two cases in my prof. And first one one, base case, where the program was distribution. Luckily have I typing rule for the primitive.
That says disstance, between two singleton distributions, is the same as distance between their elements. In our case, zero, and zero, .
So this case is proofeded and showcase, we showcase that we also support non syntax directed proofs. Inductive case will be a little bit different. In inductive case, I still want to prove the distance between B and being skewed and do it through the intermediate three for the function in the middle, the cost function thatconstruct, and I upper bounds the difference between initial functions by two pairs of distances, mathematically, called triangular inequality. And loss function estimate, and the components, result in boundary I expected it translates Haskell, almost literally using the operators for inequality it's equal less.
And I can also supply the exemplary statements such as the triangular inequality which we have for distance class and theorems about left comparison and right comparison with a question mark. So that concludes that proof for bins. I'm going to explain stochastic gradient decent, very widely uses in machine learning, . So its algorithm function optimization, which takes four arguments.
First of them is a good function. Called Lipschitz function. It has two parameters, the data point and the vector of weights. So our goal is to make the function output, better results on data points, and we can achieve that by adjusting rates to we want to find a good weight for the function. We are given a data set on which we can try a wrong function. And then we are given the job, which we can pray to optimise the function as performance steps of optimization. Each of them has certain influence on the outcome, which is supplied by the final argument, the list of steps. What they would like to proof about that is that the function performs good on all data points, not just the data points from the dataset. So it would be good for any other data set, the result would be roughly the same as applied. So what I say in the stability property, if I have two different data sets, that differ in exactly one element, then the distance between two cause of that SGD that have upper bounded constant, nondependent on the data set. So right hand side we see that expression that depends on Lipschitz function on F, and Val dependent on number of iterations and step sizes so does not dependent on data set, good for machine learning algorithm it is, and what we use for libraries, and examples here. And the SGD, represented now, and bin spec, is that component proving that one call to... always return something was referred to the second offense. I skip this proof, interesting part about this table which compares the size of programs two sides of proof to the combination time And for base days, I presented the non, which was interesting and good to have. But the point is particularly good for proof.
This is where relational types, train, so they are significantly improve the size of the proof.
Showing SGD. We have a mix of both, so the proof is shorter faster. And it's interesting how P and K components of connective relate to each other, and how we use this interaction between them prove our different case study, and convergence, of certain reinforcement learning call greater than our highlight is fine through since we are higher order system, which allows us to infer rules for applicative interfaces as well.
So with that present higher order relation probabilistic system. I encoded in assumptions of Liquid Haskell, which allowed me both syntax directed and onto their roofs and explore the usability of library with several case studies. Thanks, feel free to contact us, and from liquid Haskell.
[APPLAUSE]
>> Okay, we have time for a couple questions from the audience.
Let's see... here.
>> Thanks for the great talk, I was wondering usually Liquid Haskell is used in automated manner, here writing manual proofs in Liquid Haskell. So could you compare how much effort was this compared to doing in full prove assistance, or are you still benefitting from the automation built in liquid Haskell.
>> Certainly do, don't have exactly the same proofs in other group assistant, but I know in general, this ... but the transition I had here hugely facilitated by... liquid Haskell is translated to a very simple sequence of steps, comparing to what you medical have with other purposes. So for example for SDG this requirement of what that data sets differ in exactly one point is proofed very simply.
>> In the case it's inStacks direct you get a lot of automation. And other cases you have to to...
>> Directive proof for elation prove. And expect the upgrate audimakings from SMG.
>> Thanks.
>> Have time for another quick question, if any.
>> Thanks for the talk.
>> So maybe I missed this part. Just wondering in our proof about relational property for bins, it seems on both inputs the control flow of both programs is across the same, so just wondering can you handle cases where the programs take different control flows, and is that covered in SDG.
>> Our assumptions they only cover cases, one factor of the program is exactly the same on both side, but can be DIVENT --
the rules only cover those case, but we can use, liquid Haskell to alter the structure of the program. Using the quality and for cases where the relationship proof is desired, but the STRAUR of the program is not quite the same. You can use, and then apply. So with that, the right extension of the system, with synchronous rules, in our library, we keep it to the rules with the same structure.
PSH .
>> All right, let's thank our speaker then.
[APPLAUSE]
>> Our next speaker is Son Ho, verification framework for functional functional translation. .
>> So I'm going to talk about Ineas. Which is verification for work safe rust programs.
Bit of context first, rust, low level memory safe language.
Good way to have understanding rust is compare to C.
So for instance in C, you can write the following function, takes Boolean, and depending on the Val of the Boolean, returns one of the two pointers, and then can use by giving pointers to the variables, X and Y, retrieve point of Z and use to in place of dates. We see the type system doesn't give any guarantees by the pointers you manipulate from here. You don't know how they are, and this is also true when you... for instance, with X and Y. This can relate to a lot of bugs in relation to memory management.
In rust... the true function is the same that instead of using pointers use burrows,. So...
here X and Y, unnecessarily...... nondangling, exclusive access, they cannot against each other. There's also another important thing which is that we have enough time here, which appears in which is you analyze the different burrows, and out put, and give constraint... and if we caliberos, And because it has the same implement burros Z, as long as variable are borrowed, which means they can be accessed.
Because... then you can use for dates like on the left, and of course later retrieve access to X and Y, but when you do you need to evaluate the house. So here the left hand goes to here, and from own words, retrieve access to X and Y, the the fact you use correctly, lust...
checker. And this type of system was designed to initially provide memory safety, but also provides a lot more. In particular, in a sense you don't have any liasing in program. So what happens here, the important... that you can't choose X and Z at the same time.
So as long as Z is valid, and retrieve access to X and Y, and that moment on ward, Z is invalid.
So this gives you very STRAURD management of memory. So literal question from there.
Can we leverage this very structured and disciplined management of memory to simplify memory reasoning, in the context of program verification, particularly interesting in the con TESHGS that you do have program verification, reasoning and memory is... difficult problem. And used more and more for programming. And so this provides opportunity for scaling more on the preample will Iification, and question of leveraging, and is not new, and explored a lot in the past few years, and led to quite a few frameworks, interesting that most of the frameworks SPLAP one point in the design space.
Typically, require the user to take the risk programs.
Typically conditions and use those programs to generate formula, by rev languaging research centre, and yes, everything is fine, I don't FRM that I get proofs, you direct, will date practice ams. And...
finishsting proofs by which the programs, and then do the proofs in one group, and delivered your huge But there are other points in the design space that you may want to explore. So for example, what about extrinsic proofs by which you have a model of your program. And then on the sign your rights and privileges, and this gives you more modularity And what are they having less automation, but the principal, automation and more direction.
And this is the direction is decided to explore a new framework, which, is called Aeneas. Rust program, leverages the rest suggesting to translate it to a pure executable model, which can then be extracted to value and then you can write and problem us this model. The important point is that because it's pure. You don't have when you stay in progress to reason about memory energetic, and you can instead focus on the functionality of your program.
So what does this look like.
This is similar to he is one previous project, and trying to have have... so, what are these formally look like. Let's go back to the truth example, we can do the translation step by step, and the beginning is quite simple, so for the beginning, what we fleed to say, is we introduceintroduce ... need to say, we introduce, X y&z for those here We use a purified version of our choose function, which is typically choose but we remove burrows and comes being flexible. By using this date, we modify X, as I said previously, as long as length of leaves we don't access X and Y, and don't have to take into the account this date yet. And, What we have to say is that z gives us access to some value. And at this moment, this value gets ability is 2.
And then comes the interesting point. Here, we get access back to x and y which entity lifetime alpha And at this moment when you observe the fact that we have made modifications. But, so, at this moment, we retrieve access to x and y, we can observe those modifications.
It's important at this point we have to propagate the changes that have been performed. We have to say, we get new values, for X and Y, now what are those new Vals. It's actually quite simple. As the Boolean head it's true. And so X. And why is left unchanged. So the new Val for why Y, is so the Bool case is true. If it was true, we have... and X would be unchanged and Y would be updated. So if we want to be more precise we can write the following. And here we see something with the case. When we could choose we write z. And so we did some forward computation. And then when we left the left there. We go the other way, that we loan the function, we do backward computation. Now, if we write things a bit, get the following.
When we call the truce function in the transmission we use a choose the forward function. We see the same functions before, which is the purefied version.
And then when the last time we need to propagate the changes back to x and y, and record to choose backward function, back into the environment.
Now those two work a bit like lenses actually.
And this is the whole idea around translation, when we have a translation, we bring to one ford function, and, and we use as changes to impact back into the into the environment, Now something interesting about this permutation is that it is actually very modular. The reason is that in order to generate this snippet of code.
You don't need to know the body of truth here need to know is signature. And you can see that from the fact that the and the truth backward functions can be derived by looking at only to be more precise, what should forward, we need to do is look at the inputs and the outputs, and purify them, typically by removing the boroughs, so it comes up backward. We said the same inputs, forward, because we need them for example for the control flow and we need to remember it for x and y. Then we look at the output and see what happens is that for every borough that return time alpha, we need to back. So we need to provide these you backwards. So here we take additional input.
And returns Vals for loans we received. And so we have...
emotional translation. And today, useful to handle external dependencies or features we don't support yet. Just have to treat them as opaque declarations. And we believe with YIGSthis version features and traits and function pointers. Now, this gives us the following framework. On the left we had thethe Rust compiler. And we wrote the plugin to retrieve the output of this compiler and convert it to an AC we call the role of a broken plus, and the reason why we dothat is the Rust compiler, is extremely easy to use malarkey So the reason behind this compiler plugin called Calico is to provide an easy to use interface to generate a simple ASD that can then its own use. And then comes the transmission mechanism itself. Yes, which customer CSD and generates a pure model, which can then be extracted.
As of today we have one backend for instance. But we also are also working on backend for God and artful, and of course we're interested in having more back ends, transaction works through is symbolic execution. We actually have we need to translate your function. We need to know every point in the function. Who borrows what from home. And because of that have to check the borrowers are quietly used. Which means don't have to trust the borrow checker. Also along the way to getting this to work, we have, we had to come up with some operational semantics forest, and this is no vehicle, in particular for explorer mechanism. And this is another contribution of this paper.
Has quite a few limitations and only support safest, only use the leverage, we belief it shall not. And don't have loops, this is on going work, and I don't have interparticular aty. Why we don't have national goals, and it's important to know in the context of the plantation.
So engineering point of view.
The pluggin, whose goal is provide once again, user interface compiler. Should be completely independent from Aeneases. And also supports unsafe good, and should be reasonable for the projects, and we have used for framework, so we can also handle... finally, when the code is publicly available on GitHub. Thank you for your attention.
[APPLAUSE]
>> Let's see if we have questions from the audience. PL down here.
>> Thank you for the excellent talk, classical program analysis, such as I'll I can't say analysis for example, often,nential blow up of paths through their flow and Pat sensitivity. And I was wondering if the backward translation say analysis for example, often, rust functions into the pure calculus, also suffers from this performance degradation and if not, how this problem is avoided.
>> Well, so far we don't have the problem because of the backward translation. And I think the reason is that the backward functions have exactly the same structure.
So the forward functions and backward functions have exactly the same function as original loss function, so you have same number of paths.
So there is some... whenever we have branching, we don't have a joint operation, so we duplicate the paths, and currently working on general impression to remove that problem.
>> Let's see who else. You have a question there.
>> Interact with unsafe, isn't complete wildwest, and guarantees and nice to use something that things don't blow up when you use unsafe code.
>> Sorry... I don't get the question.
>> The question is: Okay, obviously relies on the borrow checker. And unsafe you can circumvent that. Does that break too many guarantees in order to do the transformation?
>> Well, if you want to...
unsafe code, have to make sure the unsafe code satisfies the contract given by the signature you put in the function.
So... and in particular have to make sure that... we connect this to framework, and have to state what it means to have signature, and prove you satisfy the signature. In our particular case, there is actually, actually quite straightforward to state what means for a function to satisfy, because the backward functions are very easily expressible.
The refinement can be easily expressible.
And so prove have signature,
>> All right. Thank you.
>> Okay, let's take maybe...
let's see. A quick question.
You point out competitors or colleagues, And this sounds like actually good idea.
Verification approach. Macro translation to enhance your tool condition. Have you thought about that.
>> Because we have these pure translation. You in the theorem you state the preconditions. For instance the case of loops, would generate, we don't have that yet. But the loops would be translated to some recursive functions. So by saying some, you know pre, and postcon. And.
>> Let's take next questions offline, and thanks our speaker again.
[APPLAUSE]
>> Our next speaker is James Koppel who will tell us about searching entangled program spaces.
>> Hello everyone I'm Jimmy Koppel, with Zheng Guo, and Edsko de Vries. So, I want to write down 10-100 programs and find the one that satisfies my goals, needle in a hay stack.
So twist is putting extra constraints on the spaces. So before I show what that means, show what we can do this with.
2 and a half years ago, authors created... for Haskell. So you give it a type, and says something like: Going to...
type says somethings like I'm going to take function and argument, and apply N times argument, and gives you the program for that.
So with S and T encoding with Hoogle. And we create ad better version called hectare. How much better. More in less time, this is comparison how long it takes, and top left, hectare auto new...is very slow. You see it's all very much the left, eight times faster than average, and zoom in a little bit and see Hoogle +, times out... and hectare under 3's.
And not only does same stuff faster, but does more.
Several times we're benchmarks in other datasets which has more high order functions, and my favorite comparison. And uses less code. So we have constraints offer. I bring to you ECTAs, equality con trained tree automata. So what are these equality constrained tree automita.
I have bunch of... things to the left and right, and represents 100 million possible programs, and big circle or, and have one choice, it's a +. And point to PS let's argument from this bubble here. And add new things to that. And Y, and third thing, and now the data structure is representing 9 possible programs, and keep doing this and add more nodes.
Can be exponentially large number of programs, that's great, amazing stuff with you.
Now, I'm going to add constraint.
2 times A, and I want to represent all the programs that match this.
It's a nonlinear pattern. So we can't use the same rotation.
And then we have X + Y, I don't want, I want X + F. Ive P.
And this is not so compact any more.
In fact one of the first works 20 years ago, on one these kind of spaces, it was very similar.
Wrote solution of programs, with choice. And today I present the solution.
So let's go back to the domain of type safe programs. So here we have four programs. And I'm going to introduce you to...
another way we'll be working on first one tree automana. And at the top we have one choice, application, two choice F and will G, and TT KWARS are transitions. Those are aunts.
So... what does it represent, accept thing run, made one choice for each of the circles.
And that's X.
And now, let's add some constraints. And type everything.
So add more information here.
Every single variable here is given a child. And type, and need argument type to lunch the argument. This is constraints.
You read this, it's a path.
You see that dollar sign has two children called function arguments, the arguments, the function after I made the choice. They'll have a TR o RG and T So the return type of the function has to match up the type arguments is ...
>> So create any time, to say the only three types in the universe, I need them to about the same its int not bool.
And have to line up.
I want more than 3 types of universe, just add some circles.
>> And now going to represent the map function And I just use this from here inside the map function.
Now we have list to B and more constraints... and now I shown can represent the we can divvy ,o back to this invitation earlier of of our tool, all it is it builds ECA he space of things it wants, and kits, giving you the satisfying terms, but doing that, Nachi not so trivial. So See it represents a lot of invalidates on h one H can bookable can apply tax value different constraints, with one each placed xh intent satisfies.
So this ECTA. Only 20% of runs satisfy constraints, so need a way, to satisfying things very quickly. Because it grows can be very, very few of its athletic constraints. And so, actually I'm going to spend a few minutes explain our enumeration out of them, as it's quite a clever, but also because it's at the heart of how we get such great performance results. So the idea is we're going to resolve constraints lazily. And going to big dollar only choice. And I don't know what types. I'm going to decide later and split into two subproblems. And both sides have title lines up. And choice, function, and can put H.
And constraints merged with low constraints, and all the program that must be the same.
And pushed out again, now need to choose what the input is and, gonna defer that because that makes sense or somewhere else, and you make a choice of what the archetype is. I'm gonna defer that. O now I know I'm gonna make the same choice or both sides when I make it go the other side, let's look at x, and long behold tells us that this type. This time, I want to make a choice in all three places has to be ends, and the magic of a common intersection just tells me I found satisfying term without backtracking. Only thing can make at a beach. Go back to the top. She was at this time.
Once again similarly, we're going to find all use of F. And so this is fast enumeration is a secret, how we get to the great results showed you earlier. And so very littlecode. That's hectare, GHC pluggin, you can download and try thanks to...
somewhere in the audience.
So very fancy data structure, and generate housing programs.
And all they ... more broad domains we're going to apply it.
>> So something we're excited about e graphs are very bad as sharing text. Somethings that can be... ETAss we have a term, reverse brown notation built in.
And here we have submission with Matthias, and student from San Diego.
So this problem with where I give you functions and you want to find laws in the functions use property based testing.
And previous work, give 3 functions, and we built something, and entire modules, so two domains. Hectare XHAN, and spectacular, and you hit run and solves it way faster than everever, thankfully, they're also more activewords idictionary. So you have such a domain that you think might be amenable, sent We're looking for collaborators and maybe you'll be the person to create nectarine or delectable. But hopefully not rejectable. .
[Laughter]
So I'm free to check the paper as a lot of cool stuff in there especially want To beat details about how we make cycles work during testing.
That's thank you and I'll take questions as little mostly all figures have shown so far has just been drawings and the top right That is actual with an actual ECA. Generated by a tool looks like. Thank you.
[APPLAUSE]
>> So we have time for a few questions.
>> Specifically about the Haskell program generation.
Over here, sorry.
I had a question about what kind of language features do you support. For example, if you have some JDT's, can you also generate programs for that?
>> So we're looking at components built out of functions, not say constructors, I can't think of... we don't have components in the library.
Standard components.... Big thing with sports... we do have 7 codings to get type glass to work, and same as original Hoogle + paper. Something we don't do is generate programs with ham DAs. Thatgenerate programs with lambdas. And the reason for that is that we used to have a restriction on how many cycles can be in an ECA.
They can still process and shortly before the paper was submitted and you have to remove that restriction, but we dig around and see you using our new technology to actually implement support lamda.
>> Thank you.
>> F he question on the scatter plot, of Hoogle + versus hectare, and few points on far right corner in the bottom, can you tell us more about those.
>> One of those points is both.
And the other is the ampersand operator, both these things are operations that need to operate on both components of inheritance input, and hectare is going to be kind of blind in capacity to try and put second and first of something in expression. Hoogle + actually has built in.
>> Hoogle actually has built in support and need to structure it. And we're not able, we were thinking about this before the ability to support lamdas. And without lam DAS, did not figure out how to embed that with inductive bias in hectare, and yeah, that's why only 8 times faster from what the graph looks like.
>> Another question from the audience?
[Off mic]
>> Yes, um...
[Laughter]
>> It's actually... I have a deleted slide in here about an application, someone who joined the paper and then dropped out.
And that was... application.
Found... representing the staging constraints, and stage language, and some on runtime.
So preliminariry result that he implemented. d.... With EPAs, and much more compact than what you can do with E graphs. But then still the work of makic it fast, and programming, extraction based on IOP different sol VER, which we have not done.
The other thing doing rewrites, it is actually challenging to rewrite, efficiently, you can do the rewrite but will break all the constraints above it.
Talk more with you about online oall the problems with that.
And how we think we have solved them, but have not done that in our first paper.
>> Any more questions from the audience?
>> All right, let's thank our speaker again.
[APPLAUSE]
>> Limsoon around?
Or somebody that will give the next talk?
Our next speaker is Limsoon Wong and tell us about iterating on multiple collections in synchrony
>> Thank you, good afternoon.
So in this talk I'm looking at a problem where, you know, in many modern programming languages, comprehension syntax is used to, or encouraged to be used for many manipulating writing programs, for structures. And comprehensive executed directly is usually not very efficient. Yet those programs you write comprehensive syntax the function they defined always, always, almost always have alternative implementation that is much more efficient. So this represents what I call intentional expressive power gap. So the function is there, but the algorithm is not there, so how do you fix that. The objective of this paper. That is published in JFP and presenting here. So let me give you an example to illustrate what I mean.
So in this example, let me use event as simple data type. So event has a starting point, and ending point. And some extra stuff.
And then, on event, you can define ordering. Like one event before the other event, if it starts earlier, or if both events start at the same time, then the one that ends earlier is the one that comes before.
And we can have different kinds of operations of events, for example, overlap, two event LOEFRlap d overlap. So you see, the comprehensive syntax, give me two event, X and Y, overlapped, very. And OB2.
Another function, possibly quick glance or look at it for a while, you probably easily get what it's doing. But. OB1, and OB2, same function, and condition that X and Y are sorted according to the ordering it's before. So as long as it's... according to it's before.
before.... Of course, the comprehensive syntax OB1er, quadratic, and OB2 surprise, the complexity is linear. In the sense that it's X + KLK is number of ivents that will wants in Y can overlap events. Typically in real life application that case very small relative to the size of x and y.
Okay so it's effectively linear.
So that is what I want to talk about today. OV1. And it's very easy. You know what it means.
But there is no... you know, OV2 exists. It's a function that is much more efficient.
But there is no... let me say...
so if you are restricted to only using comprehensive syntax, I don't know if it's obvious to you efficient function can not be defined if you radio restricted to only are using...
like this. For example, this function that you have return, to taxpayers, and X... or pay more tax that Y. You have two... you see very clear.
How to prior to efficient program for them, is not obvious. The reason they're not so obvious, because there is no equality testing. If you think of equality, there is no equality. Everything is inequality.
So let me come first to the result which is to show you that actually the intentional gap exists, is real. And I do that in the context of comprehension syntax restricted first order setting, this mini language is very simple, it has atomic types, couples, and simple list of couples, and aam toic test it is.... I don't have a pointer.
And so concatenation of two lists, and then, but if...b b b But it's actually enough to encode all the or flat relation algebraic operation. Okay, so they can do everything your database system can do. The other thing I need is this concept of guymer. Guys when graph, So your input can be very complex, but what can be represented as a government So the notes are the atomic values in the input, and then you put an edge between two nodes, if this value appear in the same Okay, so that's one graph And because I can have constants, and also things inside your lists, so I caught a constant level zero atoms and those things in a list, level one atoms. Okay. Now, you can very simple theorem. If you have any expression in this restrict restrict that language. And if this expression has the complexity that is linear, then the following happens: For any edge in the graph of the output when auto I ply the function to the input, this edge has to either already exist in the caspian graph of input, or one ledge of this hero fge has to either already exists in the cache line graph of input or one end of this edge is a level zero atom. The other end is a level of one atoms. It cannot have both ends being level one Okay, so this is called a limited mixing counterpart of the locality theorem. So, what does it means you cannot qu you cannot mix That means you cannot construct two level atoms into the same type of output. So it straight away tells you that, for example, this cannot be done right because taxpayer will be level one input lists, and you cannot put the X and Y together in output. So this function is not definable efficiently in linear time. So have I given you a very quick proof this intentional expressive power gap is real. So are things you can define in comprehensive syntax.
But you can not it... but you cannot define efficiently Therefore, we need to fix this gap right completely we all have compression syntax, but it doesn't let us program efficiently. So what is missing, what do we need to add it as additional constraint or as a function into our function library, so that you can fill this gap. Additional thing.
It's not something that you want to be thoroughly so that you can define everything under the sky, so it should not introduce new function. But let's you define your algorithms, and this missing ingredient, several ways to fill it. The way introduce, synchronized iteration to define that had introduced some additional concepts. The first concept is monotonicity,what you seen earlier, two list sorted in some ordering, and before, ordering that bridge this too.
So two list, you name them together. How do you put them together. And this is before, and reaccidented by... the more interesting thing on the condition, is a you applied to overlap predicate, and it's rather simple, and so, first concept is that if for example, you have this wife here. And it can not see this X, so would not be able to... anything that comes, for X.
And continue is that if this WIES after this DPCHLT annot overlap this x, then this x will not overlap by me, why that come after very simple conditions, negation is a bit complicated You can also flip it around. And what this first one say is condition says basically that this can see predicate is right sided convexity, what it means, that if this Y can see this X2.
It will be able to see every X between this X2 and this Y.
Okay. And similarly, if you...
X1. And X2. Can be seen by...
right side complexity.
And what does this two concept bias. Suppose you are X and Y.
Partner ifs what does this do concept bias Suppose your x and y.
And the can see part again, is before predicate satisfied and time and monotonicity, then you can easily write this program.
The program is actually does the same thing, the only two that you've seen earlier. It's a bit small.
small.how monotcity, is used.
Maybe easier oif I explain in this chart.
So if you have for example this y1 can not see this X2 therefore I know can not see everything that comes off the X why so what does it mean I'm doing a join overlap, once I see this condition, and can not see this x1. I can gore fete this one already. Because don't need to see anything is it else. The Y1 is finished and can move on to the next, y2. And similarly this after X1. And can not see the X1. And then, this X1 is not going to be able to can Y2.
And so I cover. So these two condition let me decide when to shift and discuss things in my two list: and that's what these precisely two license are doing,what these two lines are doing.
Alright, so this is what I call it the synchronous generator thing the synchrony generator is efficient. So just to illustrate over previous query, and are we for is basically the same query but now you think synchrony generator. Okay, it's very short, and, and the complexity is again, linear, So this is what a synchronous generator will buy you. By the way it's an extension in the sense that you can not use it to define functions that you can not already define using simple comprehensive syntax, does not add any function, just algo rhythmic power. So in this sense fills that intentional gap. And also has a relationship to data base joins.
So currently data base systems joins implemented by many, many different algorithms to get efficiency, depending on what the join looks like. If it looks like X.A = Y.b, then usually the optimizer in your database system which was a hash join or merge join or something like that, execute it. And if you have inequality test, a single inequality tests. Then there'll be someexecuted a join.
Can with a range join. And if you have something like x.a less than y.b, less than xc, that would be band join, and last one,one, times and all that. Okay, then they will use a specialized interval joint algorithm. So, to that. But more recently people discover that actually this can be executed using a union. If you use specialized algorithm, are cause. What is very interesting here. Join has is satisfied conditions here, so if you join reflectionive, and all convex and reflexive, interval join is not convention. I already told you, correspond to right side convexity, if you convex, both left and right side. So therefore all variety of joins are monotonic and they can be excused using synchronous generator. And very efficienttly.
So this is one important thing, and also you can actually rewrite synchrony generator, in the same form the data base is written. So those not familiar with the data base system, groups on the left side.
Simplified way of writing data base.
So basically you have two lists, Y and Y you want to join.
And so basically if you process Y, and keep trying things until you get Y equal to your X, at this point you can do the joint of X and Y, yep, and keep doing this. Until you hit a Y that is different. Then you can stop, and move on to the next X, so that's what standard join is doing.
In the synchron join on the right side, slightly different, when you process the Y, process of Y is before X, and list of things that matches your X that can see your list.
And then from this list you extract those that meet the condition CS, those that you can see. And no will be all the rest of the Y, and what you return will be X why yes's. But the rest of the list is not the no any more. It's yes, con connated with the no.
What is the difference, what will buy you the extra -- extra general capability, because the traditional must join require CS, get to be equally join. Whereas with the small modification, CS join predicate is now monotonic doesn't have syntax
requirement..
It just required to be a terminal, so is a tremendous generalization and simplification, in terms of concepts and algorithms. So this is the next part of the paper, because this synchrony generator, ery convenient if you're joining to relation. Now if you want to join 3, 4, 5 relations, synthetically a bit ugly. I like it. So I tried to work on how to modify it so that you can get back your competence something as close to comprehension syntax as possible. And it turns out that it is possible to decompose a synchrony generator into two parts. One part, the iterater and sec is synchronize with the iterater, and composed into the two parts, and that's easy. And so you have also at synchrony citrater, into small library , function into your standard Collection. Library. And. Okay, for those who have access to a compiler and you can modify it.
You can even introduce a simple syntax generator syntax like this. And that can be the...
into the synchrony iterater in the second piece there. And this is what show can look like.
This program can join... 4 different lists you can think of this as free time slot of four different people and want to find common meeting slot for that. So... the naive program will look like this and, iterator so you can see the bottom half. After you create the order iterators. The bottom bar is exactly the same as the comprehension syntax, so if you use that synta drysugar, it becomes like this. Okay, so very close to the comprehension syntax and it is very fast. This is a stress test. A very advanced genomic query system for to do it carries on the genome, and carries on the genome so very complex, 100,000 lines of code. When I re implemented this system using synchrony iterator is like, less than 4000 lines. You see left bar chart, and blue bar, execute ugh synchronc LISHGS, ash. . c iterater, This synchrony iterator executor and sample parallel.
Okay, so the origin is completely parallel slowly sample panel allow it also chop the genome into small pieces and run in parallel. So, you can see that the performance is very impressive. I will stop here.
[APPLAUSE]
>> Have time for one quick question from the audience?
There.
>> Thank you very much for your talk, it doesn't disappoint at all.
I wonder if this synchrony extension list KOFRN hence can be added as feedback of Bool ean, and whether the match perform depending on this, could move forward. Or more complex, and needs more effort. The code is not very long, it's 0 lines, so CLIB rattly, to use deliberately like this so that not only you can add it to a score of new languages also have a much more primitive languages.
>> Let's thank our speaker again.
>> And we have arrived at the end of the session, so thank you very much for attending.
