>> Okay, Welcome to the section on effects -- not effects that's the wrong section, welcome to effects on programming and reasoning about effects. So our first talk, Patrick Bahr, telling us about how to calculate compilers directly from their specifications, when they have diverging, or at least non converging behavior. Take it away Patrick.
>> PATRICK BAHR: Thank you, this is joint talk. And we're going to show how to calculate compilers with the help of monads. Xpress diverging behavior in our source language.
So that's the plan. So here's the setup that we have. So we have given the syntax and semantics of source language, and is he southeast Manitobaics given as interpreter, and here for very simple language.
And what we want to have is derive from that a compiler.
Systemically. And we also want to have assurance the pilar is correct. So also want to have a proof of it's correctness to exexpress some form of equation shown here. How are we going to do this. So the calculation approach just as my starting at the proof. So what we're going to do is I'm going to start proving this correctness theorem here before we even have the compiler. And as we do this proof, definition will fall out of the calculation.
This might seem like black magic, I hope by the end of the talk I convinced you this actually works out.
The nice thing about this is uses very SIFRP reasoning techniques only induction and equational reasoning which partly is one of it's problems here.
Because we only have induction it makes it difficult to account for nondetermination, so what we do here is use an monad to model the diverging behavior and use simulation for equality here.
So that's the plan, so what's the partiality monad. We have two cases now case, just going to return, and return value type A, and also defer computation with additional time step. And indefinitely, because it's a conductivity defined type, so that we get also divergent computations. And we give this operations and these indeed satisfy the monadic laws by similarity. So in principle two choicess bisimilarity. Weak kind and strong king. Let's look at weak version, Since the one that usually find in compiler verification light because it does not care about the number of steps on both sides,now 42 weak is similar to now. And generally expect in your compiler that thedoes not necessarily, necessarily take the same number of steps, as the source code. But unfortunately, it's no good for our purposes for for calculation, because we cannot reason both using transitivity and by coindeduction, f this makes it completely useless for for calculation. So let's look at bisimilarity. And so now 42 is not to later now 42. But turns out this is not a problem for calculation. Actually the calculation process will not only produce the compiler, that will also produce the semantics of our target language. And in particular enter just the right number of latent steps, so the two sides source language and target language will measure.
So we're going to go with strong bisimilarity here. And let's see by example how this works here. And again, the simple expression language from before, and now going to extend with silly loop construct just to make the language nontotal.
So we're going to extend the interpreter who just interprets loop S evalue loop, and just loop forever. And automate definition now total and partially monad, and code monad style, using return and do block here, and crucially, now this, this recursive call eval loop is now guarded by this later. And importantly so first of all, this makes this definition now total as far as the meta language is concerned, right so we can do this in coq no problem. Definition now mixes co recursion recursion. So in particular, we have here is to evaluate their recursive calls to structurally smaller x and y, arguments, and have here eval call here. And does not call on structurely smaller argument, but is guarded by later. This is well-defined semantics here.
So this is our semantics here on the left.
And now, as I said, we're going to derive this compiler here that turns our expression language to some code, and not sure what the code is derived by the calculation as will be the virtual machine, the semantics of the target language that takes a piece of code and will take a stack, which is just a list of integers and produce the stack that we have after we executed the code we have given.
So that's the setup we want to derive, and we also as I said, we want to have proof of it's correctness of the compiler.
Okay. So now the plan is to calculate this compiler using this specification we're using strong by similarity, to make it go through, we need to generalize it slightly and ask compiler to take continuation argument here.
So for given expression, continuation, produces code for the expression and followed by the code that we give here, and we accordingly generalizes, specification here right now says that if we evaluate the expression e stick the results on top of the stack, execute the continuation and let us the same as executing the compiled code on the right here. All right, and the procedure is as outlined before we prove this property, and we get the definition on the other end, that's the plan, let's do this for simple language. And this is the proof and specification. And go through induction, and coinjection, Right so this matches What we've seen earlier the semantics, which was recursive And we also got to make heavy use of the monad laws which hold up to bisimilarity.
And let's do by case of integers. And plan to start on left hand side of the equation, and work our way manipulating the term to the right hand side.
So on the left hand side specification. And transform step by step to the right hand side. I say right-hand side, but we don't know yet what the right hand side is because the compiler is not defined yet, but what we're going to do is find some C prime, some term of type code here, and then we just make the proof go through by force.
Just by defining the compiler to exactly return our C prime that we here calculated.
So this is where the definition of the compiler will indeed fall out of the calculation proof, that's what I meant earlier.
So that's the plan, let's go through step by step. First step we're going to apply the definition of interpreter, evaluate LN, and return N. And apply the monad laws. And substitute for V, and then stuck here, and want to get to form given down here, but we are up here. So the stack doesn't match up. So what this means essentially is we have to solve an equation. This one here.
Replicated here on the right.
And the way we're going to solve this equation, is again by force. By definition, we're going to introduce this as a definition, as a clause in the definition of our function exec and how do we do this?
We make sure all the variables on the right hand side, in particularly N and C are bound on the left hand side. And we're going to do this by introducing a new constructer of our code type. And takes these two arguments N and C so they're bound on the left-hand side. So just call push because that's what it does. It pushes the integer in top of the stack, and falls out of the calculation, and namely our goal to get to solve this equation here.
And not only that we introduced additional constructer in the code data type, and discovered instruction we need for our compiler.
And this completes the proof for this case.
All right, so now we can simply read off the definition of our compiler by just comparing this here to this up here, and so we have the compiler turns Val N, and continuation. And that's the trick we have here. And remaining two cases now, addition, again we start on the LEFD hand side of our equation we want to proof. And apply semantics of addition, and apply this out, and a ply monad laws and allow us to reorder the nested do block and get rid of return here, and now appear to be stuck because this does not match the left hand side of our left induction hypothesis, we may use, but trick see this as equation we need to solve, and again, solve namely by introducing new definition for the production machine, which puts in the right form to apply the induction hypothesis, so we do the induction hypothesis for X and Y and as it happens, we can again in the right shape and can apply the induction hypothesis for X, and complete the proof and in this case, can read off the definition for our compiler. And one last case we see coinduction, that's loop case, start on the left hand side, and apply the definition of our semantics and apply the definition of the bind operation that allows us to move this later here. Outside of the do block, and now we see that this term here, the left hand side of our coinduction hypothesis, which allow to apply because we're guarded by this later.
No problem can apply induction hypothesis. And now the target is to get a term of this shape here, and it's again equation we need to solve, which we again solve by definition, by defining the virile machine exec to be exactly in the right form to solve our problem here, and note it introduces the later here, right because the later appears up here.
And we can read of the definition of the compiler here which is now: Loop. The loop compiles to this loop code. And completes the calculation. We now derived compiler for the simple expression language, and derived the target code and derived the compiler, and derived the semantics of the target code. And most importantly, we also proved along the way it's actually the correct the compiler and indeed it was driving force for deriving in the first place, t this was the trick about this, but very simple language, toy language, and also applied to more interesting languages.
So we did it for a number of different lamda calculi, both in cabo value and name form. And lambda calculus, and did with language with loops and biloops all with the partialality monad.
And use the language with interrupts, and there we had to extend the partiality monad.
And all these calculations have been formalized in Agda.
All right few words about future work. A couple of things would like to do... extend to register machines, stack machines to register machines should be straightforward, I believe.
And this might be quite challenging, but I think the use of monads now allow us to consider more interesting monad we can use to structure our effects, so if you replace, for example the partiality monad with the interaction tree monad where you have additional algebraic effects I can imagine you can then peel off effects step by step doing the calculation, and get essentially multistage compiler.
So this is something that we're looking into at the moment.
And... with that I'll shut up and thank you very much.
[APPLAUSE]
>> All right, we have time for questions.
Yes.
>> One there and one there.
>> Thank you for the talk.
Parts of those calculations you performed seemed like they can be quite mechanical. So how mechanizeded can this get?
>> PATRICK BAHR: Right in deed so this is all done in... so missing the tools to do mechanically, but in deed most of the steps we seen should be automated. Because it's just mechanically applying the definition. And mechanically applying the monad laws to simplify our expressions and applying induction hypothesis, so should be opportunity for appropriate proof search of this kind. And only small number of steps where you really need some user interaction, where there is truly a choice, where the user or the calculator -- as it were -- has to make a design decision that will influence the resulting compiler, but only few of those.
>> Thank you.
>> Other questions?
>> Go ahead.
>> I'm kind of curious, so here you have this kind of mixed inductive and coinductive definition, and challenging to have this work well, they're complicated how practical to use these?
>> PATRICK BAHR: I cheated a bit on the slides so what is happening here on the slides, the bisimilarity relation we have here is defined step indexed. So what we're actually going to do here instead of saying approve this by induction and coinduction, imagine there is an index here I -- and then do induction on both E and step index I, and this gets rid of all the nasty issues how to nest.
>> So you don't have KOIN coin deduction in there.
>> There is no coindeduction in there.
>> In the middle there.
>> Hi, can you go back to the final slide with the compiler complete with the execution.
>> PATRICK BAHR: Yeah.
>> Yeah, that one.
>> Don't you need some sort of final code, like done to run your program.
>> PATRICK BAHR: You do. I have a slide for that. Thank you, another slide. This was the original compiler we have here that does not take cocontinuation, and express the property of that compiler, as well, and then can calculate the same way we calculated the more general compiler, and that will indeed introduce this halt instruction, which is the one that halts execution. So I didn't show this part, but, yes.
>> One more question up front.
>> So, when I see this something I want to do, tell you about it so you do it. Instead of designing -- opcode pieces of data and exact function at the same time interprets them you can have high order presentation of opcdee is a function with the argument and is has effect on the stack. So you would have like one up code using several cases. And simple executive function and wonder if you thought about that.
>> PATRICK BAHR: In deed on the original paper of compiler calculation, that was the first approach we went with. The idea that we're not calculating this type of codes but instead we calculate combinations high order come binatrs that manipulate the stack, and apply the functionalization, which then results on this. So what we did instead because it sort several steps we just fuse this together, this, this, this calculation that you propose, and the functionalization, so we get one equation we have to solve only.
>> Okay, let's thank our speaker and take the rest of the questions offline.
[APPLAUSE]
>> Next we have i i .
>> Does this work, it's not working
>> I unmuted. Apparently need to turn things on for it to work. I, I'm Irene Yoon, PhD student at the University of Pennsylvania, and I'm going to tell you about formal reasoning about layered monadic interpreters, this is joint work with... and Steven Keuchel. If thes so today we're gonna talk about free monads there, a data structure for representing a cycle programs here we consider a computation was interactions with the environment, our reads and writes to a state intuitively or a tree where here you have a program, interacting with the environment with the event where you do a read from a reference cell X for example, and brand continuation, and first is continued with result R, and rest is continuation, and event with right to reference Y with different natural number values. So typically in a monad, you have a parameterization of monads are of type two type parameterize without result type power of free monad you're also parameterize by a family of events, which describe the operators and the language. So in this example, the type of this would be free memory E, not where memory E or the read and writes to reference cells. So given this data structure, how to actually give us semantics to it. Using monadic interpreters given this roughly OCaml snippet saying that you have a right to a local variable x and a right to a reference cell Y y which should be ready to memory. You can encode this as a free monad that may possibly return a natural number of value where you encode the reference cell by saying you have summary. And then the local variables are encoded with multiple events.
Now we can give us semantics to this in a layered fashion. If you know how to fold over the structure of a particular free monad, you can have INTERP constructor, which folds over a monad and given a handler that is a concrete semantics to your memory events, you can then go to a state transformer which represents the memory on the heap. And then you're left with a free monad with the remaining events which are local environments. Then we can give a full interpretation to the event by giving another layer which, given that you know how to fold over the structure of a state t apply to a free mo monad, you can have an internal state function, get a handle of the environment. And then the resulting monad would be nesting of some state transformers and then now you have the remaining free monad with no events left.
.
And at each layer of interpretation, new notion of equality, you can imagine, though ou have the syntactic notion of equivalence. Whereas on the free monad you can have a free monadic prevalence, assuming that you have an equivalent free monad. You can imagine that you can define some kind of notion of exceptional quality, which is an equivalence up to the resulting environment of state. So the goal of this paper is how to bring modular reasoning and semantics for such layered interprets. So just to situate this work, I'm going to give you birds eye view on all the related works this has been built upon rich tradition of nestss nested interpreters since the 90s where people have used nested interpreters to give us solution to the expression problem in very popular programming languages such as Haskell and nested types are very useful and free monads allow us to use free monads and the total type theories such as CoQ.
So, this particular work is built on top of interaction trees which is a Coq library, a data structure which is technically a reader monad so what is the problem that we're trying to solve, as part of the vellum development team hich is a formal semantics of LLVM IR in Cocke, we've developed a formal semantics for LLVM IR a realistic production language of production level language using interactive trees which has this layered form of interpretation. So when we do this, we get interpreters at each level which is extremely useful because, as you can see this diagram of stack is very complicated and see the last layer have a choice of interpretation we can give to non determinism which we have other last layer here. And on each layer, there's also a lot of equational theory involved monad laws and more. And the difficult thing is there is a lot of boilerplate metatheory, where there should intuitively be structural rules preserved through the layers of interpretation. So previously we've identified that the semantics is somewhat extensible kind of limited, but the reasoning principles are not reusable and in this work. I'm introducing that we're going to remove all these extraneous words so in practice what this looks like, in our case study we've done, simple imperative language to assembly language compiler correctness proof and then the middle of our proof we've identified that we've had to continuously apply the same structure rules but different layers which incurs different proof obligation on each layer, and also redundant boilerplate whereas the key idea is that in our new proof, we can actually identify the structural rules, which are preserved throughout interpretation and we develop the necessary meta theory to do this. The contribution is that we have a new meta theory to reduce the boilerplate and formal reasoning about their monadic interpreters and then the way we also introduce new abstractions of interpretation.
And using this we can do algebraic and heterogeneous relational reconcile reasoning which is suited for reasons such as compiler corrective statements. And this is all a oq library that is used, sending the traffic increase framework and if you didn't read all this, the takeaway is extensible meta theory. O kind of going back to the initial example right, what's the problem, have the right layer of abstraction? So once we have this free monad you say Oh, as long as we know how to fold over the structure of a free monad, we can write an interrupt function over it. But then once we have a state to apply for a free monad ctually use a different function called interstate because the -- so we need to introduce the right kind of abstraction, This interpretation, we want to introduce a new abstractions so that this last layer, really, is the same kind of interpretation so that we can lift the structural rules. So how do we build layered interpreters, this is just type signature of the interpreter function of the original entry library. Given an interpretation function for a given free monad your event type, just to remind you, that's the family of events, type, type, that describe the operators of your program, and tar gets monad, and target of interpretation, so given handler give you semantics for each event in your programme, you can take the free monad, and of these events and interpret into the target monad. The problem here is that the interpretation function as parametric and its target monad. So, the key idea is we want to interpretation function that can be layered.
So we generalize the notion of a free monad and define something called an interpretable monad, and we try the layers of interpretation, using higher order functors, right. So now we can help a new definition of interp. And actually two new types we see here on the definition, we first have a layer type and index monad, I'll explain throughout the slide, a monad transformer and a higher order functor, which lists the boilerplate meta theory. And what we call an interpreted monad is the now newly parametrized source monad, , which is a layer rigger events which I will explain. So given the this layer of interpretation but we've seen earlier. Concretely, the type of layer will be the set state transformer with the state of nheap and then the index monad, which is a higher order functor representing respecting iteration, and the index monad is an index monad with events with what we characterize as having operation called trigger which represents a monad that has minimal, computation for performing an uninterpreted eveevents. And then the target.
Here is a state T transformer applied to the free monad with no events left. So, the layer domain should respect the necessary structures rules.
Right so now that we have new abstraction for defining the interpretation function, how do we actually get a he OOE conversational theory that can compose. Right, so relational reasoning on monad is not new.
The most like common laws monad laws as we seen in the previous talk, you have the expected unit, and return operator, and bind operator is associative.
And we introduce a relation called equivalence moduloR, theory monads with a parameterize relation r. O this not only generalizes the monad laws but it offers a relational program logic for monad so in practice we can reason about program transformations, with postconditions on the resulting values where R post condition or the resulting values.
Furthermore, we have a we have some structural laws that we characterize this equivalent to hold. So, in order for this to be a useful equational theory, there's some expected laws such as monotonicity over the parametrized relation. And if the relation that you're parameterizing over as a partial equivalence relation to the results be also be a partial equivalence relation. And then also the relations compose a somewhat expected manner. You can find interpretation laws, enfores laws, and in the paper if you are interested. So, this kind of parametrize relational reasoning I just wanted to show you a couple more rules are nice for kind of having this kind of heterogeneous reasoning about programs transformations. So you can do reasoning on pure values were. If you were to want to relate a monad that returns an element change with another one at that returns an element, when the related results should be related to each other and it's heterogeneous in the sense that A1 A2 not necessarily have to be the same type. So it can be monads returning different values. And then we have the closure property or if you're more familiar with logic side like this is basically a property for buying, where we can relate to monads which are and they want to want me to do X, relate off of per condition RB if we know that the prefix is related on RA and continuation related postcondition for all related RA. And as a result we have composable equivalence for layered monads and we say that each monad can have a particular notion of equivalence with all the structure of properties and additionally the structural rules for iteration and interpretation should be preserved. Pointer, that rule, that rule, and basically saying interp, monad morphism that commutes with fine and then if it's an interpretation of a return that should be equivalent to return. And then, this is kind of the behavior of what it means to be trigger and to remind you of the definition of trigger trigger is the minimum minimal impure complication of an interpretable monad. And these laws compose in the sense it's unnecessary to introduce new definition of equivalence and proof new structural rules for every combination of layered interpretation. So in conclusion, we introduce and accessible meta theory for acceptable effects, and then the paper and the artifacts. We got the badges. We have a Coq library on top of the attraction tree framework. And we also talked about during this talk, we have a generalization of automatic injection of handlers with combinators called --
subprime over and we have various instances of the equational theory of monads and monad transformers including like state and the weekend strong by simulations haracterization of interpretable monads respecting the theory of iteration and and also notion of an image of a monadic computation which characterize the name of a monad case study which is and to ASM compiler this is kind of a sort of a diagram of the tech cost dependencies of this project and the green part is new part red part, are some of the dependencies from the tree library, thank you for listening I'll take any questions.
[APPLAUSE]
>> Do we have any questions?
>> One down here.
>> Okay, so something basic, maybe you said it and I missed it. In general, when you have a monad the way to lift in relation to values and regulation and is not unique, in your library, for example, do you make one choice per monad or several choices how do you deal with that.
>> Very good remark, the idea is we have these acclimatized structural rules for your monads but you can really choose your equivalences. So in practice you can define various notions of equality over any monad and in practice we do actually have to use it in our en compiler correctness, because the base free monad in our interaction tree which has a notion of both strong bisimulation, and weak bisimulation, stack of equational theories that are basically different in practice and we use that in our case study. .
>> Other questions.
>> Can't get airmeet to load, and don't know if we have questions online.
>> There are no more questions, so let's thank our speaker again.
[APPLAUSE]
>> Okay, next up we have a virtual talk for the distinguished paper telling us about program advebs, s a term they've coined to describe a certain set of reasoning principles that are very composable, so hopefully that talk will appear shortly. And should mention --
>> hello everyone I hope everyone is enjoying ICFP and to those of you who are attending ICFP in person I hope you are also enjoying Ljubljana .
unfortunately, I cannot attend this year's ICFP in person but i'm notheless happy to be here virtually and present our paper Program Adverbs and Tlön Embeddings my name is Yao Li an assistant professor at Portland State University this is a work that was done while I was a PhD student at the University of Pennsylvania in collaboration with my advisor Stephanie who is attending ICFP in person so let But happy to be here virtually and present our paper, My name is Yao Li an assistant professor at Portland State University this is a work that was done while I was a PhD student at the University of Pennsylvania in collaboration with my advisor Stephanie who is attending ICFP in person so let Us start with the talk. Suppose you want to verify an existing program the original program might be written in C, Haskell Verilog or Makefile etc.
Which are all mainstream languages that do not support mechanized reasons the first step you need to do is embedding such a programming language that is available to mechanized reasoning such as Isabelle, HOL, Lean, F, or Coq this step is known as semantic embedding or simply embedding it turns out in bagging is with many challenges first we would like our embedding to be simple so both developing the embedding procedures and working with the embedding might be easier for example let's consider the following arithmetic expression one plus two plus three in Haskell let's embeds the expression in Coq.
We can either do a shadow embedding which would look like the same expression or a deep embedding which would be an abstract syntax tree (AST) of the expression .
The shallow embedding is simpler and it makes reasoning about certain properties simpler as well .
For example we can easily show that 1 plus 2 plus 3 equals to 2 plus 3 plus 1 and also equals to 6.
This is because these expressions are direct computable arithmetic expressions in Coq as well so we get the theories of numbers etc.
Built in Coq for free on the n the other hand we need either some extra theory for the AST or interpretation from the AST to a shallow embedding to show the same thing for a deep embedding however what is convenient in one situation can become inconvenience in another supposepose we want to reason about properties related to the syntax structures of the original programming step. Such as how many plus operations does an expression contain we would not be able to do so with the shallow embedding . we cannot tell the difference between 1 plus 2 plus 3 and 6 in our shallow embedding.
Instead we need deep embeddings this is the second challenge of embedding.
We want our embeddings to be simple but also be suitable for the properties we want to reason about.
Finally we want our embedding to be modular .
This is because many programming languages contains similar language constructs, features, or computation patterns.
Let's consider makefile and FPGA they are very different languages but they share some common characteristics first t First they both enforce a static control flow in a Makefile file dependency are statically fixed and an FPGA is hard wired furthermore due to the nature of this static control flow we can parallelize certain computations in them as seen in "make -j" and in an FPGA furthermore Haxl is a library that automatically runs certain operations in parallel it is written in Haskell, a language more expressive than Makefile and FPGA as it allows dynamical control flows but a particular piece of code in Haskell may only contain static control flows and Haxl makes use of exactly that part to run parallel computation .
in other words how Haxl also has a subset that shares these characteristics we would like to model these shared characteristics in a modular way so we don't need to repeat ourselves .
this paper tries to address exactly these challenges but in the context of mechanized reasoning about the effectful languages in other words we aim to find a way that allows us to reason about as many properties as of an effectful program as possible in a modular and simple way.
Let's start with an example consider a circle that can read from some external devices in this circuit we show on the screen we read from two external devices x and y---or just one device if they point to the same device---and join the values read from them with an end gate this circuit has a static control flow and it runs into parallel.
In addition we might want to model the reading effects from external devices abstractly external devices abstractly.
Either because we want to make them modular so it works with multiple different types of external devices or simply because we do not know anything about these external devices note that reading effects reading effects on these external devices might do anything depending on the device.
For example each reading from our device might change the values on the device or the values change by themselves every second or the value returned are purely random random.
How do we model such a computation pattern our inspiration comes from many previous works on free or freer monads and their variants such as interaction trees which allows us to embed effects using an uninterpreted data type.
We have discussed the earlier shallow embeddings and deep embeddings and their trade-offs in terms of properties.
These two styles of embedding are common choices when we embed the program but they are not the only two.
In fact as many recent research works show the styles of embedding should be viewed as a spectrum rather than a binary choice.
Free monads and their variants offer a pretty good mixture for embedding effectful programs in this spectrum.
The mixture is embedding effects deeply as uninterpreted algebraic data types and embedding pure computation shallowly.
In this paper we generalize the key idea of free monads and propose a class of mixed imaging that capture general properties of computation patterns.
we call this class of mixed embeddingsTlön Embeddings.
More specifically to circuits free monads are too expressive to properly represent them.
For example monads would allow dynamic control flows .
So using them would allow us to write exotic terms that can be cannot be expressed in the original language and would get in our way if we would like to reason about properties related to this static nature.
Now before we move on some of you might be wondering how to pronounce the word Tlön and some of you might be wondering what Tlön even means you can pronounce Tlön however you want as it's a fictional word from a short story of Borges.
If you want to learn why we use this word to describe this class of embeddings I promise you that there is an answer in our paper Now instead of using free monads in the circuits we derive a structure from a different class of functors namely applicative functors which are Shown here, we derive the new structure by reifying applicative functors first we declare a data type parameterized by an effect data type called e and a return type Called R. And then we take every method from the class and make it a constructor and change the Types accordingly.
In the end, we add an Embed constructor for embedding effects in it.
The part that demands special attention here is the LiftA2 constructor which contains two computations that do not rely on each other.
There is no way to run a different computation or invoke different effects depending on the result of another computation or effects.
this structure is called a program adverb with this program adverb we can represent circuits shown earlier as follows .
First we use the lift a2 constructor to represent the join of two reads.
We use the Boolean and function in Coq to represent the and gate so we're using Embed to well, embed the read effect which is merely represented by uninterpreted constructors this gives us the embedding of the circuit which represents the pure computation shallowly (and deep embedding) and the effects deeply .
program adverbs are not just data types they also contain some general theories that can be used for reasoning about semantic properties such as some parallel properties of the circuits.
For the sake of time we do not show adverb theories here but you can find them in our paper.
I find it useful to understand program adverbs in comparison with effects by effects I mean new operations that provide our program interaction with external environment .
Like reading from external devices in your circuits, I/O, exceptions, mutable states, mutually exclusive locks, etc.
if we make an analogy between programming languages and natural languages effects are like verbs in a natural language .
However verbs themselves do not decide how our action is done because they can also be modified by adverbs.
It is the same in programming languages we have some computation patterns or general mechanisms that work on the control flow or data flow that would modify how any effect is invoked in a general way.
This computation pattern includes static or dynamical control flows parallel execution and nondeterminism etc.
program adverbs are structures that capture exactly this type of computation patterns.
So how do we find other program adverbs.
It turns out we can derive many program adverbs by reifying the many classes of functors that are commonly used in Haskell such as functors, applicative functors, which we have shown earlier, selective applicative functions, and monads.
We give names in adverbial forms for each of them as shown on the screen.
Okay so far we've been talking about standalone adverbs but what about the modularity concern we were talking about earlier.
In particular in a Haxl program there is a subset that has static control flows and parallelization and another subset that has dynamic control flows.
To model programs like Haxl we also need a way to compose different program adverbs .
This pursuit however would lead us to the expression problem.
In our paper we overcome this problem by making use of the techniques presented in by Delaware et al.
In their meta-theory à la carte paper which builds on church encodings.
By making program adverbs composable we can actually achieve a computation pattern à la carte that allow us to add existing program adverbs as needed.
Furthermore since program adverbs do not need to be standalone we can find a few more program adverbs that represent repeated computation and nondeterminism that are mostly useful when combined with other computation patterns.
We discuss composable program adverbs in more detail in our paper along with two case studies using composable program adverbs .
One case study is based on the key ideas of Haxl and the other one is about a networked server.
You can also find our Coq source code in the artifact of our paper.
In conclusion in our paper we present a class of mixed embeddings called Tlön embedding that allow us to embed program using program adverbs.
Program adverbs are structures that are parameterized by effects and can be interpreted into shallow embeddings.
They model some common computation patterns and they are composable enabling composing computation patterns as needed according to the original language.
Program adverbs can be used to reason about syntactic properties as well as semantic properties about the general computation patterns and they can also be used to reason about semantic properties via an interpretation to shallow embeddings.
You can find more detail about these in our paper .
That is all for my talk today thanks for listening and I'm happy to take questions.
>> Greatings.
>> Take questions from the audience, do we have any questions in the room?
So we had one question about the bit of audio that we lost in the talk. This was an explanation of the origin of the terminology of the paper. Can you fill us in on the origin we lost there.
>> You didn't actually miss much, because the answer in the video was there is an explanation in the paper.
>> And the PC awarded this paper distinguished paper award, so if you have more questions about it should be very readable compared to our intermittent airmeet issues.
>> My question is that your refi... are they constructed such that they have the same equalities as the relevant type class rules, predescribed. Or do you accidentally see difference between two applicative structures that are actually semantically the same because of rules.
>> So maybe can I answer both questions here. Refi... counter I show here,..
applicativefunctor and not normalized, so normalize, normalize the free applicative functor only the recursive definition, both on normalized form of functor and we do that to keep the tree structure of the original program, and...
sometimes you may have like, two... you may have associated issue, that don't have the same tree shape and that's the question here.
And our work, we do not use the Coq quality... we develop theory for equations for some equational theory, in which case two structures are semantically equal.
And for starting itself, with the associatve role, and explanation how to derive this equation in our paper. And and in parallel. That...
perilization, but at the moment don't know if there is safe way to derive, the role. Hope that answers the question.
>> Okay, I see one more question on Airmeet. It says, the description in the talk was fairly abstract, can we see a code example.
>> Probably not an easy way to show here. But we do have artifact that are available on GitHub and ACM digital library, if you are interested in materials I also recommend reading section 2 of the paper that contains very concrete example and working through different types of embeddings and ad VER Bs and bringing to the table.
>> All right, thank you.. any other questions?
Don't see any more in the room, and don't see any more on Airmeet, so let's thank our speaker [APPLAUSE]
>> Thank you.
[APPLAUSE]
>> Okay, next up is my mike on, Dylan McDermott will be telling us about flexible graded monad, extending the classical effects that we can use that we can capture using monads, take it away Dylan.
>> Thank you. Does the microphone work?
Now?
Do you hear anything?
I flipped the switch. Testing.
Do you hear anything now. Okay, good.
So I'm going to talk about models of computation effects.
And to tell you specifically what I'm going to talk about I'll start by giving you an example, here is example of computational effect relate to modelal. O let's suppose that we have programs, which can use a operations. So in particular, I have an operation called All she's binary so it takes to kind of programs as arguments, and it says hoose between the two programs have another operation called fail with non deterministic, and a third operation called cut. The way to understand these if I draw as a tree. And read off the values returned by the computation.
But then if we ever see a cut we stop and prune everything to the right of the cut.
So here we get 11, 12, 13, and the computation stops. So there is some kind of OI way implementing back tracking computations with these things, won't talk about in detail, but keep this picture in mind.
So... we want these terms to satisfy some equations. So when we interpret the terms inside the model we like the equations to be validated by the model.
So for example, we might like to say that if I do choice between M and N, where M is computation that cuts, this is equivalent to doing M, because everything to the right of M will just be pruned. Or write down other equations like this one on the bottom of the slide.
Perhaps less obviously why this is true. It says under some circumstances we can compute these computations N1, N2. So the question how do we actually construct a model of these effects in such a way to validate equations like these.
So there is existing story of constructing models of effects.
Way it goes is say something like, we can model effects usually using monad, or strong MORN, I'll forget about strength for this talk.
And these in some way come from presentations. And by presentation mean operations together with collection of axioms, equations we would like the model to validate. So nice story that says, once we have the presentation we can construct the monad in a way.
And so say more than this because every creation of the presentation will induce an algebraic operation And these are kind of the functions. In order to actually interpret the operations that autos the effect. So there are examples, interpret these computations with cuts. So we say something like, I computation there's a list of results together with some tag that says where the computation cuts. And this comes from the presentation, the presentation, says have free operations one called R, and one fail, 0 arguments and one called cuts taking 0 arguments. And then bunch of axioms require to satisfy. And R associative, fail as unit, and cut is left 0.
And then the nice thing is from the presentation, in addition to getting this monad in economical way, we have algebraic operations thing we use to interpret and fail and cut.
And so family functions take two computations and merge in some way, looking at tags and making some lists.
And so now let me go back to the equations.
Because every kind of model effect using monad, and algebraic operations this doesn't really give us a good way proving equations hold. The reason is these have side conditions, that say only computations that satisfy system property, computations that cut for example. And there's no really good way of expressing the conditions using this kind of previous story about ordinary monad.
>> Hopefully this will give you enough idea of what they are.
And so we can say something --
here are examples. You can say something like, I have this monad, created by natural numbers. E is a natural number.
And computation of grade E is list whose length is exactly E.
So list of length exactly 4th for example, don't form monad with concatenaition, but collectively create list called graded monad, are graded monad, bit like a monad accepts, we just kind of assign appropriate grade to the bind operator entered the return. This is in fact why I'm asking for an for an ordered monoid here. The monoid gives us the appropriate grade for the bank and return, all, for example, we could say, well great my list monad by taking list of length moth E for example. So there is MORN we can use for the cut example.
Essentially the way it works we take the ordinary monad few slides ago, and say impose side conditions on the computations.
So here have free raids, called bottom 1 and top. And can say, computation will cut, or either return something. And stop says, we don't know anything.
So we say for example, E where E is bottom, is list of X together with tag that we require actually to be cut, and compensation grade bottom is computation that actually cuts, form of graded monad I won't go through details, not important for the talk, but we have monad and model these computations showed in beginning. Reason this is good, we have way to express the side condition equations, so we take the equation. And now says, M has grade bottom. Which means M actually cuts. So in fact, if you calculate for a while, we can interpret the computations using the graded monad and this equation is actually validated.
So now the question I want to ask is there a story about presentations for graded monads?
So some stories I have graded presentation, that consist of created operations with some equations. And somehow want to extract canonical graded monad, together with some algebraic operations that can use to interpret my effects. Actually there is existing notion of graded presentation, which in fact would allows us to present this particularly graded monad.
The problem is we can not present these algebraic operations using the exist notion of operations. But if we only care about the... monad this is fine, but if we care abouts algebraic operations, and we do because we want to interpret you know, programs that involve all for example, then we actually can't use this existing notion. We do not get the correct algebraic operations. It is reason by the way in case of R we have several arguments that J NN have different grades, D1, and D2.
This is actually not permitted in the existing notion of gradedgraded -- there is condition that says, all the arguments have to have the same grade and can't extract operation doing this. The goal of this essentially develop nice notion of graded presentation, and call flexibly graded presentation,s, in such a way that we can actually extract economical graded monad together with algebraic operations and suitable a suitable notion of algebraic operation to interpret effect like this, he cut example, and do this by generalizing existing notion in some way.
And then, yeah, we kind of do tall the proofs and show we get graded monad and so on.
Okay, so for the remaining time I'll kind of go over this in detail and say what these things look like and how everything works, I'm not going to give full details of everything, not enough time to do this. But start by saying what flexibly graded presentation is.
Presentation is first signature.
So collection of operations.
And in this case, every operations has a list of input grades, and here D prime. And also some output grade D.
And given such a signature I can say, generate terms of the signature, by kind of applying operations to over terms. And so if we have operation whose impulse have D prime, and out put grade D we can actually apply the operations to here.
And we get... not obvious why the E is here, but to get the bind of the grade monad. And once we have terms, have collection of axiom and axiom is pair of term, pair of terms he how how want to be equated. And so in the nondeterminism example, say, have a bunch of operations in particular, these are operations, we'd have appropriate And then also a bunch of axioms I'm not writing down all of them but there's some associativity and things And in particular, something that saves our effect on y is the same as x, whatever X has bottom. When X definitely cuts.
So the rest of the story we want nice graded monad, with algebraic operations first step to doing this. First define the notion of algebra, a space together with operations. And subpoena that interpretations satisfy the axioms of the presentation.
So the notion of space here, I'm calling graded set, this is collection of sets one for every grade. So you think of the set AE as computation of grade E, interpretations bunch of function satisfying properties. ind of lift the interpretations of operations to interpretations of terms, and then ask are all for the interpretation function satisfying properties. Of each side of the axiom to be equal.
And come up with notion of equational logic, this essentially just says which equations we can derive from the axioms so a bunch of rules deriving equations this is rule for applying axiom and rule for congruence, and this is appropriate equation logic, in the sense it's sound and complete. Equation is derive NL exactly when it's true in every algebra. Okay, now can construct graded monad, easiest way to do that take the terms and KWOEGS by the the equational logic, not writing the whole details of this. But we have the monad, which is canonical in some sense, so precisely it satisfies some Universal property what this means is essentially the greatest monad is a folks as we can possibly get to these algebrasPor uses graded monad, and actually gives sense of uniqueness of graded monad. Final thing is we can actually extract some notion of algebraic operation. The algebraic operations are essentially interpretations of terms inside algebras. And can extract one such thing for every operation in presentation, these are the things we actually use to interpret the effects. Now I'm almost done, just want to mention one last thing, we can say exactly which graded monad can be presented in this way.
It turns out to be the greatest monad, using the existing notion of graded presentation, we do Jennize the notion, not in the sense we present more graded monad, but only in the sense we present more algebraic operations.
So actually, this is in some sense a nice note to the presentation and that we get large class of graded monads, but we actually get some nice algebraic operations that we can use to interpret effects, which is somehow the point of what we're trying to do here.
And can use the presentation to validate equation, to validate equations like the things I showed in the beginning.
But that's all I wanted to say, so I'll stop there. Thanks.
[APPLAUSE]
Okay, I'm going to give the first question to Airmeet some discussion during the talk, how is cut different to fail?
So I think this was around slide 8.
>> Okay, so here I have a fail inside the computation. The things right of the fail are kept, so here 11, 12, 13, and that's it. Everything to the right of cut is pruned from competition, everything to the right of fail is not. I produce no results. So cut is I want to forget any results that might come afterwards.
>> All right thank you, any questions from the room. One here in the middle.
>> What is the purpose of your equations being graded?
>> What precisely do you mean.
>> I mean that, yeah, there gamma entails T = U, and seems to be graded or have a type of D. And also seem to be...
>> The grade here is not so important, it's just the grade of each term in the syntax of this you can only write down equations between terms of the same grade, so the grade of each term, so in sense not so important to write it, just prefer syntactically.
>> Okay, so other questions?
All right, no more questions, then let us thank our speaker.
[APPLAUSE]
>> Okay, next up we'll have Patrick Thomson, and Rob Rix, and have two speakers for this so give us a second and make sure the audio is working.
>> Check, check, there we go.
Perfect.
>> Want to go with the podium mic there?
Yeah.
>> Oh, this one as well.
>> Hello?
No
>> That one.
>> It appears so.
>> Test.
>> It works.
>> Thank you.
>> We're ready.
>> Hey, everyone, it's a real pleasure to be here at ICFP. My name is Patrick Thomson.
>> And I'm apparently Rob Rix.
>> And we're here from GitHub, show of hands quick, who uses GitHub regularly. How many people sometimes use GitHub once or twice. Excellent, not a lot of source FUJ FUJ users here, glad we're in the safe place.
>> Patrick and I have known each other forever. And been fans of typed functioning programming, and Patrick one that taught me apolitictive, and devicing CFP in particular, for quite a long time so we personally are quite strongly motivated to be speaking to this audience, but additionally for from GitHub perspective. Well, there is a lot of code on GitHub, in fact that almost everybody here uses GitHub is probably an indicator of why that might be, given that fact, it'd be nice if we knew anything about it. So, who are you going to ask but folks who know something about programming languages.
>> PATRICK BAHR:
>> PATRICK THOMSON: So one of the tools that we've built to try and understand and comprehend the of code on GitHub is called semantic it's written in Haskell and it supports nine separate programming languages. We'll get into those later. And we built it to prototype and the power real actual honest to goodness GitHub products that actual people use, like for example the Table of Contents. Now sadly you can't actually find this one in the app anymore. If you're familiar with pull requests on GitHub. However, in the pull request or PR view used to be a way to go and find a little drop down that would show you a table of contents of what had changed in the file. Now you'll notice some of the entries here are in green summer and yellow.
Well we have inserted deleted changed and yellow files and symbols within those files to help you navigate your way around what might be quite a large pull request code review process. And this again, computed almost entirely with the guts of semantic.
>> And another product and perhaps our main semantic code teams main product offering is code navigation, originally this was prototyped in the Rubion rails monadlith, powers github.com via shelling out to the venerable C tags executives will, it may not shock you to learn that that approach doesn't really work at the scale that GitHub is at so we took semantic and hooked it up to an RPC framework servant with all it's fancy types and allowed people to navigate code, jump to definition and find all references on GitHub and the 9 different supported programming languages. Now, at this point, this code doesn't current the code navigation service is built on a DSL. This was not written this is not reflective necessarily of Haskell being the wrong tool for the job or semantic underperforming semantic did a killer job in practice. But what we want is community involvement. Semantic code team is small team, 7 people at max, and sometimes even 4 people. And we want language communities to maintain their own rules and heuristics for code navigation so we've implemented this as a DSL and we just passed the DSL off to the elixir programming language community who implemented, and now maintain their own code navigation rules on GitHub. We think that's pretty cool.
>> So, again, small team, big problem. Lot of languages we want to support, and supporting stuff across languages, and trying to find commonalty between them is an interesting challenge in and of itself, functional program something concerned solving large problems by breaking them down into the smaller problems and this is what we were hoping to do.
So with that in mind. Let's take a look at four case studies how that played out.
>> Before you do anything with any program text, you need to parse it. And the unpleasant truth is that in a lot of corners people consider parsing a solve problem. And in real world, there is just myriad approaches of parsing, Ruby uses parser and Python uses their own homegrown parsing parsing expression grammar framework, and most of the GCC projects parsers are actually hand written. I hope you enjoyed and maintaining a handwritten C++ parserrthat job must fall on somebody, I wish NEM the best of luck. In ideal world would use native tooling for every language. Obviously nothing understand the language better than the tools that underpin it and written by it's authors that's too operationally complex to fly in environment like GitHub, and can't maintain language stacks for languages.
We need some sort of interface, that let's us part whole variety of different languages in sufficient time and space at GitHub scale.
>> One of the ways we built on -- I guess that comes later.
So tree-sitter developed by one of our teammates, for integration into the atom code editor. And integrated successfully into atom but also into VS code into Neo vim I think Patrick mentioned emacs todaytoday. That's MRET cool. Let's you write grammar in DSL, in JavaScript that ends up looking a lot like parser combinators so quite familiar to t myself and many others in the functional programming community, and perhaps most importantly, it gives us a consistent API that we can use regardless of what language we're actually parsing.
And grammar taken and compiled down to C file, and compile and link against that and talk to that through the Haskell FFI.
>> Now tree sitter is based on the generallized left right algorithm developed by language in 74. Which interestingly enough took a decade plus to be implemented practiced. This came out of the natural language processing world, natural languagings, as we all know are riddleds with ambiguity, and possibly even nondeterministic interpretations, and these techniques were developed to have a framework, and algorithm that is capable of unction even the most byzantine and COMPLEGS grammars, and speaking of byzantine and complex grammars ruby is measuring stick for something capable of handling real world language, Ruby though on the surface may seem very beautiful syntactic language is fiendishly complicated, but we're happy to report that tree sitter, combined with a custom lexer.
That's a fairly small c program is capable f handling all the nine languages that we've chosen to arse that the generated C code is fast enough to be integrated into a high performance service you know it's servicing the entirety of GitHub user base.
>> As a quick sidenote Ruby is also the most complex in every other way in addition to syntax that's true that language source of many joys and many pains. Anyway, so as the second case I mentioned earlier that the complexity of trying to do all the various things we're doing across nine different languages. Led us to want to consider how we can factor that work out. For example, to, printing for one example of integers, integer literals, the same way regardless of whether it's in Ruby or JavaScript or go or, etc. That isn't necessarily going to give you source level printing but way to share some of that structure that work.
Trying to produce ultimately M features across by N languages.
Up from the huge mbnpolynomial down to simple m + n line, that's what we want to be looking at. One attempt was to factor out bits of syntax to individual datatypes we then combine into the ala carte style sum type. And here have binary sum, and principle take the union of all the different shapes of syntax, and gives us injection into it and KRUSHly safe projection back out, and ompatible with working with recursion schemes, you could throw fix or free around this and have something that you know how to fold over. .
>> It's also worth mentioning here, with the little language, we include parse error case, case, and that's crucial because at GitHub, we can't assume that all the code in a repository is well formed, maybe somebody made a syntax error somewhere, or maybe introduce new language syntax or haven't integrated properly into the parser yet.
Tree sitter is capable of best effort parts of possibly mall formed syntax three, threout able to... the property and make visible in our syntax trees.
This is a classic paper. And I can't recommend reading it highly enough, if you haven't yet. But it has problem in that the standard encoding of the ala carte of injection, and subsumption relation is expressed with recursive overlapping type classes. And if all this works for your smaller languages with maybe 20 or 30 different syntax types.
Languages complexes TypeScript has over and I'm not getting 140 different distinct syntax notes and when you feed a GHG type level list containing 140 different items. Tends to be unhappy, so the solution was to do some loop unrolling in the type system, which sounds mad inducing, but a bit of throw back to 70s with C programmers and dust device.
[Laughter] >> I heard some knowing laughs there.
When that's unrolled with some template Haskell that would make your eyes bleed. It worked fast enough and took the approach and published on library on Hackage called fast some and open source, and people out there using it we're using it.
>> So the way this all started, in fact, the reason that I work for GitHub period was because I used to work on a diff tool called Kaleidoscope well now I work at GitHub so how can we do diffing differently. If you are not familiar with the product, GitHub is code hosting and collaboration for communities and users of arbitrary size from small teams to quite large companies, or external company teams. Communities rather, and one of the key atoms of the UI is changes to code. You made some change and suggesting I adopt that change in my projects, I want to look through and review that, and have some discussion with you why it's for, and why you made the choice, and ask you to change something perhaps. And hopefully make improvement to it. That everyone gets to benefit from.
However, diffes are limited.
Mostly we look at textual diffes if we're looking at textual diffs, that's fine with lines of text, usually not, usually the structure we're trying to compare and much richer than that. And to that end, we developed algorithms doing syntax aware diffing, and these were are is it refined somewhat, and conversation with folks in this community. And victor Meraldo in particular, and some of the resulting comparisons we got were kind of cool.
If you move something in textual dif deleting lines and inserting lines, but no notional comparison or relationship between the two, or for example, if you are doing the thing where you insert a function, with curly braces, one of those languages uses those. And it's shown it as the first line within the braces down to the last line inserted. I forget the right structure of this.
But basically gets the braces in the wrong spot simply because the algorithm happened to end on 1 or 0 or 0 or 1. Frustrating way to look at program and understand the changes.
So we developed this, and works really quite well as far as we're concerned. But we found it difficult trying to build good products on top of this.
Our first attempt was a notion of summery which was to try and compute human readable summery of what changed within a given diff. This turns out to be terrible if you do it the naive way that we approach it because what ends up happening is you say, deleted three, inserted two or replace, replace three with two this is not going to be an improvement on just looking at you know, minus three plus 2 in the diff so that particular product feature didn't sail.
But as a whole capability, we're pretty happy with structural diffing and.
>> What fell out of first approaches with diffing, was really gratifying, which is the utility of recursion schemes in a real world context diffing is the is the process of comparing chunk by chunk syntax trees and that's recursive operations over syntax trees and turns out in practice most things you want to do with syntax tree can be expressed as syntax stream.
They've turned out to be a magnificently useful tool in our toolbox. As programmers, it's they're compatible with our ala carte approach. You can see on the right here is very simple recursion scheme over the language we showed earlier. And the application of the catamorphism there. And cata, aka, the standard fold is fairly innocuous even the more exotic recursion seems, history morphisms came in handy when we're doing did we do Zygo morphismss.
>> The pre formorphisms.
>> And finally the perhaps most exciting to me personally application trying to do here is program analysis. So we have all the source code in the universe verse and really KOOL to know anything about it So therefore, we look to the literature again. For ways that we could try to analyze and understand, answer questions about our programs, our users programs, abstract interpretation was a natural starting place just because it's so general and so powerful. But in fact it's same property that is kept us there. This is really open ended for us.
This is a really considering question offer research, we don't know what questions are important to ask. We don't have a way to engage questions are going to be important to ask next week.
So if we make ourselves into something that's a little too tightly focused on one particular kind of analysis that's, that's going to be a shame, we're going to we're going to regret that. So we built on abstracting definitional interpreters. If you haven't read this paper by the way it's one of my all time favourites and always will be.
This is on going basis the primary focus on semantic and my personal joy, and my personal focus in my work where ever possible. It did pose some interesting problems though.
For example, it wasn't exactly obvious how to take the approach as described as implemented in the paper, and implement that in Haskell with the tools we have commonly available to us, for example MTL.
>> In deed the paper in the abstract definition interpreter paper artifacts are implemented in Monk style but not in Haskell. Implement racket. To overcome the, the problems that a traditional NTL focused view of the world. The problems and traditional NTL view of the world encounters when implementing Adi. We turned again, the literature on the papers of our co authors, Nick and Tom and Rob went into the shed with a great deal of clanging and bashing emerged two weeks later, slightly blood idea but with an effect system, a practical real world effect system and performance enough for the GitHub use case that allows having multiple state types are multiple return types in the same computation hich is not possible with the standard MTL approach, and that allows one of the state it types to have a differing interpretation from the other which is really crucial and non determinism enters the picture and you want one state to have all possible path reflected changes in it.
And another state you only want winning path propagated. And there are people out there building businesseses and websites which is humbling and honoring, it's got more than 10,000 downloads at this point on GitHub. That's pretty cool.
>> On hacket.
>> Oh, goodness, yes.
>> And as quick extra note about fused effects great ICFP story, is it started it's first commit occurred on the steps of the building outside... can't remember which building it was in St. Louis.
>> 2018.
>> Yeah, 2018 and.
>> Immediately postconversation with Nick. So thank you Nick.
>> So in conclusion we have done some things, and some of those things were hard for some reasons. And because of those reasons, we applied some other things, and things we applied, would not have been available without the work of ICFP, and the ICFP community, functional programming community, and academic community, and the research oriented industry programming community as well.
So I personally would like to say incredibly thankful to everybody for your work.
>> You all rock.
>> However in the grand tradition of people with mixed news, I'm going to break the bad news first, some techniques worked BUFly the first time, and some encountereded some ups and downs, the traditional encoding of ala carte syntax that we showed earlier, is very cool ery fluent, to express in Haskell, but there's one large drawback and that you must go, units give out a little bit of type safety, because the sub terms of when we express yntax with func d the sub terms are of the type variable associated with that functor so in the standard approach you're not able to say Alright, in this particular term we know will be one of these 3 types, really could be anything, there are approaches with Patrick, with the comp data library, that work around this, you do pay a pretty heavy cost in terms of complexity. And what we actually realized was that our hypothesis that we get good code resues handwriting and reusing data types. The degree of code reuse there, is less than the code reuse, we did when he started generating, Haskell syntax types directly from the grammar descriptions and this was a really interesting result and that suggests to me that the higher level things that you can reuse, even if they lead to proliferation or perhaps a word of data types that gets you more compelling code reuse. With large Haskell, become a problem. We've used the basil build system to work around those editor tooling. Nowadays, the Haskell language server is in a really terrific place that back in 2018 it was it shakier, especially with large template Haskell slices prices that we have something we do run into is that generic programming in Haskell can be very difficult with the standard GHC generics library you need to the programmer needs to have a scenario with type families with poly kind of types and taking a novice Haskell programmer and sitting them down in front of that interface is a little bit difficult. We also had a brief.
Experiment with type file paths to catch any errors we might be making when we tried to append a file to a file. A file to a path whereace allowing pending file path to directory, this did point out places we were having creative semantics, but didn't really catch any critical bugs, and posed a good deal of computation complexity and reading complexity on opof that.
>> In contrast, at one point in time, any push to GitHub of any code, where at least one of the files was in one of the 9 languages we mentioned, or showed anyway, went through HAFK Haskell code. That's pretty damn cool. That seems to me to be a pretty tremendous win and we're talking about a lot a lot of runs a lot instantiations of the process so pretty fantastic . Tree sitter, ikewise became foundational not just for us, but for anybody doing parsing for developer productivity in any way, shape, or form. It's really been tremendous to see tree sitter take off the community that's grown around it has just blossomed. Likewise recursion schemes folds, who doesn't love a fold Come on, full degree. And it doesn't matter, matter if, if you call them folds or catamorphisms still beautiful.
Algebraic effects like why does I will never not love algebraic effects as well, the tremendous, to be able to decompose programs into DSL is like this, and it's pretty neat, also that we were able to find several places in which we could composed part of our system into an open source package that other people could use and sometimes even did.
>> Fantastic brief anecdote, that I wanted to tell about our scanning requirements and operability, that our deployment of production, never actually hit any crashes or errors, except for one. When something deep inside the GHC event loop croaked and died, and submitted on the GHC bug tracker and got reply back in couple of hours, and said, are you running Dell servers and no way, and I get on the horn and find out what brand of servers, they were Dell servers and configuration of BIO is that made Linux is E poll fall down in a given critical situation. It's horrifying at school. But most of all it's not our fault, and that's just terrific.
[APPLAUSE]
>> We got around 5 minutes for questions.
>> We have tons of questions.
Let me start with one from Airmeet very aggressive question. Does GitHub know how to fund academics when it would need their help to put their work in production.
>> Anyways a great question, and happy to report that GitHub is sponsor of the Haskell foundation, and tapped in the community and very GRAFRL for what it's given us, I hope that in the future, we can continue working with academic community and ask them, What is the most important thing, which is funding. .
>> All right one question down here.
>> Hello, thanks for taking my question. I am a blind user of GitHub, so I hope we can take time during the coffee break to discuss accessibility, which is not bad, but could be improved.
And already reported several problems and it was ignored, so really hope this will change.
Meanwhile I have a question about diffs. Have you considered trying to compute the diffs in collaboration with the user who submit the changes.
Because after all they know the semantics behind the changes, or at least they're supposed to.
So they could enter the changes, or they could verify the changes computed. And see what exactly they wanted to do.
And then after a while you would have data base of semantic diff and do machine learning on it to improve this. So what do you think about all this.
>> First off. Thank you for filing bugs of accessibility issues that's important to me as well. For different reasons, but important full stop.
Regardless, regarding the semantic diffs that is something we considered. To your point it's an interesting problem, if you try to compute the difference between two things, you are developing proxy for what the user actually intended so for example if I delete something and insert the same thing back again, the full diff is null, nothing changed in full. But some cycles were learned. Potentially that they may want to be able to communicate forward. So yeah, That is an excellent idea, it is something that I think we should build. Unfortunately it's not likely be on my plate, at least the next while but I would like to circle up with you at minimum get your email address, so if that changes I can get in touch.
>> All right. Let's take one more question.
>> We'll be outside during the coffee break.
>> So any more questions, hit us up.
>> So you supported 9 different programming languages. Haskell wasn't one of them.
[Laughter] >> little bit of shame.
>> Always get this one.
>> Maybe there aren't enough Haskell projects to really motivate that. And so you did all this work, and don't get the benefit of the cool stuff.
>> Show makers children have no shoes. What hold us back from having Haskell as core offering not that there aren't Haskell programmers but the tree sitter grammar for Haskell at that point was not particularly good place. Those who SAR in the GHC Haskell parser for long enough, knows that's a gnarly parser, and it's a gnarly languagelanguage. It's not the world's hardest thing to build Haskell 2010 parser, but when you bring in many GHC umpteen different blocker -- we got into building the Haskell parser, and then we realize, oh, there's that block arguments extension. We have to start over. Now.
Luckily have been outside developers working on, rees that are Haskell parser and have gotten it into a really fantastic state. They did this by writing a custom lexer using C++ 17 lambdas. And we're hoping to bring house go into our supported operation. And if you'd like to help contribute some code navigation rules with our DSLmeet us during the coffee break we would love that.
>> All right, I see there are a bunch more questions, but we're already in the coffee break, and let's take the rest of these offline, and thank our speakers once more.
[APPLAUSE]
