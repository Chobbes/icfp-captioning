>> Welcome to the Wednesday morning keynote session, and delighted to have KC here whose going to talk about his work on OCaml 5.0l. KC did his Ph.D. at performance dueon a parallel version of Milton.
After that he went to Cambridge and worked as part of the OCaml labs project, and in particular, he led the development of multicore.e.OCaml, which is an extremely exciting project for me personally because it gives a really efficient implementation of effect handlers which are extremely versatile abstraction that allow you to build all kinds of features in particular concurrency features, which I think KC will tell us a lot more about. While he was there 1851 fellow and research fellow Darwin College in Cambridge.
Had a distinguished paper award at ICFP. Subsequently to that he also has been involved with various OCaml-related companies.
Helped set up 3 companies that all merged into Tarides. And also at IIT in Madras. And anyway, not going to say much more, going to let him take the stage, and tell us about Retrofitting Concurrency in OCaml.
[APPLAUSE]
>> KC SIVARAMAKRISHNAN: Thank you for the introduction, Sam.
So in the wonderful PLDI keynote earlier this year. Gave the wonderful Simpson team talk, and joked about making talk with AI generated images.
So OCaml 5.0l is the next major elease of OCaml programming language, and brings in native support for concurrency and parallelism into OCaml. By concurrency we mean overlap execution of tasks in time. And we define parallelism as simultaneous execution of tasks, overlap in time. So we have the separation between concurrency and parallelism in OCaml five, and we provide distinct mechanisms for expressing concurrency and parallelism or concurrency we express them through effect handlers, as Sam mentioned and in this talk talk about how went from OCaml 4, to OCaml 5. With the OCaml project I had led. This journey wouldn't be fun if there wasn't any adventure and had our fair share of adversity, and what I want to tell you is the challenges we faced to retrofit concurrency and parallelism to industrial strength language of OCaml and how that shaped our approach. And talk about take aways you can take away from the experience, if you are designing your own programming language abstractions, and brave/foolish enough to retrofit them. So I split the talk into the two.
First going to talk about the journey of OCaml and then talk about the takeaways. So let's go to the year 2014. OCaml was an 18 year old industrial strength functional programming language, reasonably popular in the industry and academia.
OCaml was particularly favoured for low latency net work systems, and analysis tools compilers proof assistants and whatnot. But when compared to systems programming languages that were out there. It was one of the few that did not have multicore support.
To be clear OCaml had threads, but when you have multiple threads, only one of them can execute OCaml at any point, essentially had big runtime lock around the heat. And others could be executing C code in parallel. So this is no different than python that has noo... so eliminate the runtime lock, and allow OCaml threads to run at the same time. So Sam Gross one of the developers as pytorch, working on a project with similar goals for Python, which is to remove the global interpreter lock. So in this undertaking we had a number of challenges. So the first challenge is that OCaml was heavily used by the time we started this project, millions of OCaml code in production, and none of it written with concurrency and parallelism in mind. And even after we add concurrency and parallelism in OCaml most of the code remains sequential. And developer maintaining 1000s of lines of OCaml, and came to you and said you can have concurrency and parallelism but have you to modify all the code. This is not going to fly. So the first requirement we had for the project, we were not going to break existing code.
So OCaml is particularly favored for low latency GC and having predictable performance, if you have application that can tolerate 10 millisecond latency OCaml is great language for you.
In designing the extension we want to ensure we preserve the GC latency before focusing on scalability, and is finish finally OCaml core team and composed of volunteers, so in building the multicore OCaml project, did not want to write 1000s of lines of code and throw it against the wall and disappear. So essentially wanted to ensure the burden on OCaml developers remains small.
So one of The things we wanted to do is not have separate sequencer and panel runtimes, for OCaml. And this is different than what is done with runtime parallel execution. PS a direct consequence of this, we had to ensure that our existing sequential programs that ran on OCaml 4. Ith the same time and using just the same amount of memory on this new runtime, right, this is a tall order, and this is what we set ourselves primary work and designing, like a parallel language, which is a high level languageyou want parallel allocator and garbage collector. Particularly not normal ensure allocations and garbage collections can be low latency, so in this design our unit of parallelism which is domains each had it's own miner key, and object call indicated.
And then when the minor heap is done every survivor is promoted to the single Shared Measures.
And we had some restrictions between the pointers between minor or major heaps, we allowed pointers between minor and major heaps. But did not allow pointers behind miner heaps originally. So in the design, if one domain would access object under the domain major heap. It would do RPC call, remote domain to promote the object, the target domain and promote that object and respond to the call, with the new location of the object in major heap. So the design is nice, and has nice properties. In particular could independently garbage collect each of the miner heaps without having to stop all the domains. Major garbage collector was concurrent mark sweep collector. Which had very short stoppable sections. And the idea is that this particular design ensures that the GC latency remains low.
And you can scale with the additional domains, so fairly quickly able to bring one up and spoke about the experience in OCaml 2014. But actually building upon large sequence of prior arc. We didn't want to go for novelty. Wanted to go for something that just worked. And so, Simon, M and Simon P, had a paper in ISMM11.
Key And these could be independently garbage collected.
But this particular design did not make it into the mainline because at that time. This particular GC could not compete with what was back then. During my Ph.D. multipair LEM extension of the standard ML language.
So one of the nice things that we had done exploiting this invariance between the heap is that you could take unmodified standard about programs, and run it on non cache coherent Intel single chip computer. This was quite nice, recently at CMU had large series of work in the M language, and with the SPIELer.
The idea there arrange the heap in hierarchy, in the joint hierarchy in the program, and only cross heap pointers allowed, are ancestors to no pointing between siblings, or to descendants are the descendants of siblings. So a heap that has this property is set to be disentangled. .
>> Earlier this week, Sam had a very fast check for en TANGlement detection, and Sam giving keynote at ML workshop on escapable function and programming through disentanglement and quite excited to hear the talk. Given that we're building on good tradition of building on these, we saw excellent scalability, and maintains low latency, and importantly this particularly GC if you only had one domain only matters what OCaml had today in 4. So latency throughput and memory usage characteristics of OCaml sequential programs. So we are all very happy, we thought we had something done and we can upstream it. .
Disaster. So in order to enforce the invariance, had to introduce 3 barriers to the language, OCaml did not have reed barriers. Read barriers are essentially some check you introduce just before you access any OCaml object.
On the OCaml site this is easy.
This is probably single instructions or two instructions that branch predictor always predicts, so the impact was very low. But in your design GC read barriers also GC safe points where the GC can run, OCaml has wonderful CFFI where they can access OCaml heap. And written in such a way it assumes where GC can run, and in particular GC's could not run during an object read, but now they can due to the read barrier design.
As offshoot of this what happened every program that uses CFFI had to be rewritten, and no push code fix for this. And OCaml compilers uses CFI for implementing a lot of these features, and we have to struggle to actually get this feature right. We could see this would not be a scaleable solution because every sequential library that uses C FFI needed to be modified and no static check can help you here so we had to go back to the drawing board. And f invariant could be no cross miner key pointers. Because there are no invariants to enforce, we did not read a new read barrier. We also did notbreak C FFI now.
Because ... so our initial worry was that when we increased the number of domains, bring all domains to stop would be very expensive. That was the motivation to design where you independently garbage collect each of the domains. Turns out you can bring all the domains to stop fairly quickly because of how freak consequent miner allocations are in OCaml, and every miner allocation involves check you still have space in minor heap and can make fail, uninterrupted domain and bring all the domains to a stop.
So our original very was unfounded, and we actually found that this design work much better than earlier design which require an RPC We spoke about these designs and ENS extended to cover, programming language... and finalizers and so on. And extensive evaluation in ICFP 2020 paper. The other major problem one has to think about introducing parallelism into the language is the problem of data races. When two threads perform unsynthesize access, to shared memory location, and one of these is right, said to be data base in the program, because of compiler optimizations and hardware optimizations, you may observe behaviors which you cannot simply explain as interleaving of operations from different rates. These programs are set to exhibit non sequentially consistent behaviors.
One thing we attempted to do was to enforce sequential consistency to OCaml. Would be the cost of saying, OCaml, ill always have sequential consistency, even in the presence of databases, which turned out to be quite expensive for relaxed architectures in our measurements. It was about 2x slower for sequential programs running on this new runtime, because we wanted to enforce se.
There was plenty of language memory models we could learn from.
And so if you look at languages like C + + and swift they have very nice property called DRF-SC. If your program does not have data races only exhibit sequentially consistent behaviors. Right. But unfortunately these languages do not define the semantics for in the presence of databases, so they may do anything, so they may also catch fire. .
But OCaml programs can not crash. So this particularly memory model was not something we could adapt. . But Java memory model has this limitation that it when you have a database, and you know that there is some point in the execution where you know the database is done, you may still have the effect of the data is beyond this point. So we defined this property as data is not being scoped and time.
Essentially, you can't do model reasoning, in the presence of databases in Java. On the other hand if you look at language like Rust, no data base by construction by extending the type system, did not want to do that at that stage, because we wanted all the existing programs to work.
There is some future work here, but we chose not to do that at that point. One advantage we had with OCaml, there is no OCaml programs in the wild, so unlike C++ and java, have these expectations of what the paddle performance should be. And you have these memory models people trying to sort of wrestle with the people to say, hey, can we add more safety here, so we didn't have to worry about all of that we had free reign. We ended up designing very simple comprehensive operation memory model. Only has atomic and nonatomic locations and DRF-SC, programs remain sequentially consistent.
We also do not have out of thin air values, even in the presence of databases, memory safety is ensured.
And we do not have any relaxed axes. So the model does not have relaxed axis. So our thinking was that if someone really wanted to extract that last percentage of performance from the hardware, they would implement that particular module and say C++ or rust OCaml wrapper and be done with it, so vast majority of the students can still remain in OCaml.
So recent update of the memory model. Version 1.19 release in August attempts to go for this model. But with GO, reasonable things like accessing interface Val strings and slices involve accessing multiple words and non atomically And hence they have captured semantics. Right, so it's getting there. So the key innovation of the model property of local data race freedom, compositional reasoning in the presence of databases, so you could essentially prove, some property about your library. And this property would not be broken.
Royal linking with a code that has a database, and the performance impact on memory model on sequential programs, it was free on X68 and had less than 1% performance impact. And so formal model. And memory model. And competition to x86 -- we believe if you have high level language, and want to include relax memory model you may start from the OCaml model than say C++.
So we always knew Parallelism was performance hack, it's a resource concurrency is a language.
It's a programming abstraction, right. So, we always knew that we wanted language level threads in multicore OCaml. OCaml does have support for concurrent programming through language level -- these are monadic concurrency libraries.
So one of the properties that these were already concurrency libraries have any that they split the world into asynchronous and synchronous functions. So asynchronous functions can normally call synchronous functions. The converse is not true. You have to have some sort of special calling convention to go to the other side. And this was succinctly captured by the essay Bob N, called What color is your function. So many languages, including Java Script and trust have this particular issue. So our idea was to eliminate function colors with native concurrency support. So essentially you can implement language in two ways, right, you can say my threads my schedulers, and the synchronization mechanisms are all implemented directly on the runtime. Or can say my language exposes low level mechanism on top of which can implement all these concurrency libraries as a library, right.
So Go, implement schedulers runtime system, and con currency mechanisms, and channels implemented in the runtime.
Few downsides to this approach.
For example, these mechanisms implemented in the language of the runtime which is C, but not Haskell. These are complicated code want to write all in Haskell. And lack of flexibility, you can not easily change the runtime system to change the policy and maintenance burden false on the compiler developers to maintain the complex code. So back in 2012 spent summer at MSR Cambridge and tried to lift the concurrency substrate from the GHC from the runtime and implement directly into the language. And partly successful and ended up with one particular Deadlock. The crux of the issue the thread scheduler and GHC interacts nontrivially with lots of other systems in the runtime system, including the lazy evaluation mechanism called Black holing. So what blackballing tries to avoid is that if two threads try to force the same thunk, you want to avoid the competition, so you will pass one of the threads on this and wake it up when the evaluation is done. Requires access to the scheduler, but now at schedulers themselves are written in Haskell, which may themselves required to be suspended on a black hole. So we had the cyclic dependence, and there are probably ways around it, but we ran out of time. For me personally the take away, once you add a feature into the runtime system it's very hard to undo it. So when we started designer OCaml we knew we're not going to implement all these mechanisms in the runtime system. So we wanted to implement everything into the library, so obviously have to have some first class continuations. The question is which kind. So Carl B, and Kent, D, how to represent one shot continuation sufficiently scheme. And they also showed that you can express all sorts of concurrency mechanisms just using one shot continuations. In multi Melton we had implemented an asynchronous and parallel extension of concurrent ml using one shot continuations in Milton. Occasionally have told us that calls, and delimited continuations which are exposed to a call CC or bad abstraction And on delimited continuations do not occur in practice, right.
So, whenever you try to use undelivered continuations. ...
not for combination but for writing these concurrency libraries you eventually end up having to do a hack to introduce the delimiter. So the take away there was we needed some form of delimited continuations. So at this point, they showed us wonderful Eff language that has functional program language called effect handlers, effect handlers or this module and mechanism for programming with user defined effects.
Operationally, for us it was, it gave us a structured form of programming the delimited continuations. With effect handlers, you can encode all sorts of computational effects, including concurrency, which is what we were interested in. So one of the nice things I found with effect handlers they're much easier to KOFRN henned compared to other delimited control operators, don't want to focus too much on the code, but on the left here, OCaml with exceptions and on the right effect handlers if you ignore the semantic for the moment syntactically very similar and way I explain, operationally very similar to resumable exception where the computation would be resume later, the other reason tend to be easier than other control operators, is that you don't have to carry around this prompt ordeal with answer type polymorphism if you know what this mean. Andre put it...
We took effect handlers as mechanism to expose one shot delimited continuations in OCaml, spoke about the experience in 2015 workshop.
Since then, there has been an enormous amount of work both on the type system side of epic standards as well. As the implementation side. We've also had an impact on industry, so soon afterrepeated the talk, was then Facebook to the React team. For those who don't know, React is this world's most popular UI framework that is used by one and two websites in the world, and different panels I've had a direct impact on a feature in React called react hooks. This is due to no fault of mine.
But the idea here is that yeah cooks or mechanism to manage state and effects within react APS, and nice to see impact in the real world. So unlike research language like NF. They don't come with effect system, we don't track... so they are much more... in fact we don't even have dedicated syntax for effect handlers, and we just expose them as functions and idea that I want to provide room for syntaxic innovation in future implementation of effect handlers for the OCaml that comes within the system, focus to say I'm going to add effect handlers into the language, and preserve the performance of legacy code. And I want to ensure that debugging and profiling tools continue to work. When we add effect unless the language. Spoken about this paper in the 2021 paper and focus on particularly backwards compatibility, and compatibility with tooling. And so gone ahead and instrumented the highperformance direct style asynchronous IO library called the IO based on effect handlers and competes with state of the art, and go and rust. We can do better, but really personally, if I can write my high performance code,OCaml and written within... and I would take it. And language level threats language, thread s taking come back, project loom in open JDK that continuation to the JVM and work on adding type continuations and model concurrency in Bassam and exciting development is first class delimited continuation merged two days ago. And can back and attempt to implement control concurrent on top of this.
Okay, so what are the take aways from this multi -- OCaml experience, if you want to design program language abSTRASHGS and to some industrial strength language.
So firstly care for users. They have limited time and patience, especially if they are not going to use the new feature adding to the language. So even after the concurrency to the language. So the transition should be no-op, or have push button solution.
For us it was not push button solution, we did have to break features at the edges of the language.
So one thing we did was build this tool called OPAM health check. OPAM is package manager for OCaml, and include all the package release for OCaml.
Every week it goes through the universe of open package on different compilers and here showing with latest release compiler and OCaml 5.0l who's alpha release is out now, every green box indicated that package, and little box says that package, dependancy of that package fails to build, and red box says that particular package failing, and published online, and as package developer you can see is my package broken and get a log and go fix it. And team has been proactive in going ahead and fixing many of the broken packages. FF if you are programming abstraction, you should rigorously continuously benchmark your change on real programs. They don't just run synthetic benchmarks they run real programs. And to build reality into experience, we built tool sandmark, and OCaml benchmarks, So we pick carryover programs on the wild including CoQ proof assistant Alter Ego solver Irmin data base, which will send a large set of welcome dependencies as well. We have around 50 of these, and we regularly venture on them on these benchmarks, and all of our results are based on these results.
I suppose I have a program. And ran for 4.14, ran in 19 seconds, And on account five, it is 18 seconds. You have to ask yourself now, right, or the speed up slowdowns statistically significant. So did modern operating systems architecture and micro architectural introduce a lot of noise at very small scales.
As an anecdote While we were trying to benchmark our measured the overheads of memory model by inserting fences, we observed that I could insert fences into my sequential program and make it grow 20% faster.
I only extract load on them. So then we replace those fences with no ops off the it was still 20% faster and ran the program on different intel machine and different generation, and the speed was not any more there.
Modern processers do a lot of magic underneath and can't see what this magic is. So our approach here is essentially tune the machines to remove the noise. We also observe that we can report instructions retired along with realtime in order to interpret the meaning of whether a particular speedup or slow down is just due to noise or real change.
So we spoke about this particular experience in OCaml 2019. There is accompanying write up I linked here shows how to tune Linux machine for benchmarking and also this wonderful paper by E B, 2013.
Built a system called stabilizer idea to remove effects of memory layout to the program and is provide number of statistical tools in order to do sound performance evaluation, if you care about performance you should read this paper. So all of this is quite hard to set up this machine for tune machine, and have to run all these benchmark and is need a lot of compute, and we released sandmark as a service, as continuous benchmarking service.
As a developer you can take the feature branch and point this at the service and run all the benchmarks on multiple tune machines and produce results in nice UI and interactively explore the results. And this has been regularly used by OCaml developers every day.
So the next thing I want to emphasize is that you should invest in tooling. And should learn how to use tools, and if tools don't exist you should build them.
Much of my time during the earlier years of hacking on OCaml was debugging segmentation faults, so have program with 17 threads that fail once in 100 run reliably. And the point of failure, the crash is far removed from the root cause of the bug.
Even if you manage to capture the failure in GDB there is no way to make progress, it's some crash. Right. So this is where tool RR record and replay debugger on top of GDB comes in very handy, and capture the stress and go from the point of failure backwards and keep asking questions backwards until you reach the actual root cause of the failure. RR is so good if we manage to capture failing trace once, we guaranteed we fix that bug. If you are using to use anything, you should switch to RR. Second tool we built upon is tool called thread sanitizer, part of project.
Thread sanitizer dynamically detect data bases used for C++, go, swift and other languages and use thread sanitizer if OCaml for detecting data bases implementing efficient data base detector is a lot of work and building on what is out there and quite handy for us, other thing we put a lot of work is tracing. Our initial tracing tool was hacked up version of GHC straight scope. It's wonderful. This is trace we annotated issues we had found.
So the visual debugging of performance problems, is great when you have tracing infrastructure. If you expect there is a problem and now look at a trace the problem becomes obvious, here we're looking at something and say why is there such a long minor slice. And this would be very hard to do readingreading textual log of traces, right. So this was quite useful for us and built multiple generations of these tools, and current version we have in OCaml 5.0l we have something called runtime events, and does efficient CTF based tracing that is always on, and not emitting any trace, and the feature is always on, and take any OCaml program and enable flag or signal and starts producing live traces so you can live trace any OCaml program today. And Sadiq and Patrick are talking about the tracing framework in OCaml workshop later this week. And quite challenging maintaining the separate fork compiler for 7 + years, and had to go through multiple basis neared to keep up to date with the mainline in that process, unfortunately not always communication between the features developed at trunk and what we had. So we had to sort of drop certain features because we just couldn't figure out how to fit with the GC design we had. We are slowly working towards adding them back in.
But that's a process. In hindsight may have been easier if we had enough planning to have made smaller PR's to the language.
So PR papers add credibility to the report. I see them as nice design document of the snapshot of the system. And of course that's not reflecting what is there, but you can start there and then start reading thousands of lines of code.
Our project has always been open source and actively maincontained, surprisingly a lot of academic users from early days and having users helps convince developers this feature should be added. And build tools like continuous benchmarking and open health check in order to allay the fears that when the feature is deployed in the wild due to unrelated reason sequential program breaks.
So started working on OCaml, I started OCaml 4. And hack on something, and one day be at OCaml 5. But really turned out to be this asymtotic progress toward OCaml 5. And I'm 90% done, and really the rest of 10% takes 90% of the time. And while the first 90% can be done with few researchers, it turns out need a lot of engineers for the last %. Research primarily driven by university of Cambridge, and IITMadras and Jane street. And a lot of Connecticut contributors working on getting the language across the line. So where do we go from here. Obviously the moon, but how do we get there.
First obvious thing add an effect system to the OCaml.
Right, but it's a lot of work in a language as large as OCaml you have to think about interaction of whatever feature you are adding with the he polymorphism modularity and generativity features that you have already in the language. And then you have to do it in a backwards compatible manner as well, quite a tall order. I'm not an expert in effect systems but happy if somebody takes this up. OCaml has nice compiler to java script, but can't compile effect handlers yet to JavaScript, because we don't know how to compile delimited continuations efficiently to JavaScript.
Daniel H, and otherses have published a paper as to how to take different variants of effect handlers and compile them efficiently to untyped lambda calculus, there is some work there, but JavaScript engines are their own beasts, and hence that needs to be a lot of engineering work there to get the right combination strategy.
So there is exciting work going on within Jane street, adding more model types to OCaml. And FLARL using this feature to allocate Vals on the stack and then using the type system ensure that these value do not escape the stack frame, so you can take the contact effects and use the model types to get lexically scoped type handlers, This would be much easier, you will get effects system here but you don't have to have the full power of tracking effects in types, with more backwards compatible, then a full fledged.
.
OCaml is getting a lot of performance oriented features currently being developed or in development, including parallelism in OCaml 5. And providing allocation through model types and unbox types, and Flamdda2 aggressive compiler optimization for OCaml. And when the features land, you can have roster C like control and performance while having as default, and safety of classic ML. So f PS Richard E are talking about the unbox types.
Work at the ML workshop tomorrow that, I'll stop here and enjoy your OCaml 5.
[APPLAUSE]
>> Thank you very much for a wonderful talk KC. A huge amount of work you put into this.
Questions?
>> Ron, well, whoever you get... you need an mic.
>> Is it on?
>> So you mentioned that one of the advantages you have is adding parallelism to a world that doesn't already have it and doesn't have a lot of expectation and make trade offs performance in exchange for safety, I'm curious what would be your best argument for going the other way, what are things we give up for having extra safety, and what is the rough size we're paying to get the extra safety.
>> KC SIVARAMAKRISHNAN: I think safety is good. Don't want to compromise on safety ever.
So that's been our theme.
We could possibly have gone for memory model that is more lee laxed and makes it more complicated to reason about programs and data bases but we didn't, one nice aspect of OCaml is that it's so simple. So I think there is no argument of giving up on simple for performance.
>> I guess to ask the question a different way, say you have eager C++ programmer in the organization, and following thing you want to do. What are the giving up in performance terms,terms, not questioning whether it's a good choice, more want to know what is the nature and size of the trade off.
>> KC SIVARAMAKRISHNAN: Currently I think the big trade-off is uniform representation and overheads it brings in. Have to go through GC for reasonable code and I think people at Jane street working toward eliminating that. And once we have that and write code that have compact control over when things are allocated at that point don't think you will be giving up anything.
>> So when I was first learning functioning programming, there was a standard contraryian view this was domain specific paradigm building compilerers and theorem provers and actually become more sympathetic to that view myself over time, your benchmark suite is dominated by those kinds of systems. What do you feel about the, the applicability of your abstraction outside of that domain, and what work have you done to support that kind of workload versus the broader interest or common kinds oprogramers that vast majority of programmers are write.
>> KC SIVARAMAKRISHNAN: I think it's true we have benchmarking exist programs and looking at HTTP server, and large scale organization might care about.
Large network service, microservices here we see very competitive against say Go or even Rust. We are getting there. Never wanted to focus on... it's always been Mirage set the trend for writing efficient services and do care about Mirage APS, part of the Mirage project, and uses components. So my answer would be we are thinking about those programs as well.
But I don't know what...
programs look like, so can't really answer.
>> Do you feel it's clear HTTP server written with multicore OCaml looks nicer than HTTP server written in Go.
>> Definitely compared to Rust.
Go -- I think the difference is some abstraction of writing concurrent programs, rust you have separation between async and sync, and go through hoops, rust doesn't. And same thing with OCaml.
>> Thanks.
>> Hello. So you talking about the 7 years of maintaining separate branch that it took you. Anything that could have been done differently to get to OCaml faster maybe on your side, or on the side of the community?
And for instance, could we look at what other communities are doing, like Haskell in terms of development what are your thoughts on that
>> It's a good question. I think frankly we didn't know what we were doing.
[Laughter] so I don't think we could have gotten faster, because we had built something and said, hey, backwards compatibility and had to reengineer this. I don't know if we would have gotten faster there. But if we had had more on going communication with say the upstream developers maybe could have agreed on something where the upstream smaller features. The trouble with doing that you would have say the overhead of this feature landing some time in the future, on a sequential runtime that doesn't offer the advantages of the feature. So now you don't have that, and so both the pro's and con's of doing that.
... library that is there that doesn't live within the compiler. Hi, thanks for the great talk. Key step in the design is the observation that there aren't current parallel programs in OCaml, had to come up with design backwards compatibility with other existing programs. And what do you think is the biggest sacrifice made in that direction. We're going to have parallelism in 10 years and looking back we needed that for backwards compatibility at the time. But really annoying.
Anything comes to mind.
>> Really refreshing. Um...
>> I think there is particularly HKS language features we could do without, things like lazy Val Tannen
clear vaule, and expect lazy values, and have to raise the values being forced. I want one of them to pause and another one to just continue working and then make it up. It doesn't happen that way, when you have concurrent data forces it just says, I'm going to give up now.
This is terrible, right when you have lazy values you want to optimize for that. There are possibly more examples like this, which would have just broken, and finalizer ephemerals. Yeah, so those are ephemeral, are terrible, right so if everyone's really useful.
They are a wonderful mechanism for implementing exactly one thing, the fact that ephemerons mechanism CHIS in the language, we have to think about has to have multiple a synchronous faces. And we covered in ICFP 2020 paper. If we had the chance would thrown out ephemerons we hash tables as a primitive and the language, which is what we are going to do. But in order to get to that point, we had to have backwards compatibility. So a number of features I can rant about.
>> You have mentioned that OCaml 5 will be okay for application that can tolerate about 10 milliseconds of laylaytenc, and how do you compare so milliseconds with Haskell, assuming everything else is roughly the same.
>> I think Haskell has done the right thing now, and concurrent GC as well. If latency is what you care about. Haskell has two modes, GC, and I think it would be on par wouldn't be too far off in terms of if you want to tolerate 10 millisecond latency go for either one.
>> Pass the microphone along to Simon.
>> Thanks, and not just to tell the rest of the world, and expect been very clarifying for you and colleagues. And think, now we understand what we're trying to do. Big thumbs up for that. Just want to ask, you said doing parallel garbage collector you decided to stop the world for the minor collections you could bring threads to a stop very quickly if you did that. But messing with the allocation counter, and something that happens in GC, next bit trickier as some loops don't allocate at all. So, let's make sure that everything allocate and there's these some overhead associated with that.
Isn't there.
>> You can get around that.
There it pull points in annotated groups, There is a very nice algorithm that was published in a paper back in the 90s, I think called balance pulling, you insert points on those points you don't have allocations. And implying that took multi-iterations for us, can talk offline, you have insertion points in nonallocating loops.
>> That implies, extra complexity, and runtime overhead.
>> Small overhead, and I would be very happy to talk to you.
>> Everyone should read the paper including me.
>> There is no paper for this one. There is PR I can point you at.
[Laughter]
>> A question... across there.
>> KC congratulations on the very impressive piece of work the amount of projects that come together is staggering. My question is which part of the whole ecosystem are you most worried about when it comes to correctness. Coq, I think. Coq is a complicated implementation.
And eventually what I want to do is make all of your Coq proofs go faster. I think there is a lot of magic in Coq, in terms of Coq does horrible things assuming how the heap is laid out and directly accessing things using unsafe mechanism and so on. So I think that's what I'm most worried about, but I think we'll get there.
I think Coq developers really want to use this to speedup their proofs and have to work with them to get there.
>> Very interesting. Thank you.
A question toward the front here.
>> KC fantastic talk. The OCaml memory model is probably one of the nicest memory models nowadays, for high level languages, I'm wonder what you think is the future.
... right, everything can be mutated. Have to insert some additional fence. But in functional programming language you know which are mutable and which aren't, so only pay the cost for mutable location, yes, OCaml is wonderful but also need to be functional, and so should, immutability puts that into other languages, and say hey you have other benefits. Along with just being able to reason about your potential. .
>> Okay heading to the coffee break now. Before thanking KC, like to point out he's also going to be expanding on this on the OCaml workshop on Friday morning, so if you want to hear more about the details of OCaml 5 go there. And yeah, let's thank KC again. [Applause]
