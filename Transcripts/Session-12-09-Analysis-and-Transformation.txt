>> All right, thank you.
>> Hope everyone is back from lunch, yeah, Anton Lorenzen, finished my masters, and starting Ph.D. in Edinburgh in the fall. And reference counting with frame limited resouse.
And before I get into the work want to say a few brief words about where the setting takes place, So while our work is general enough to work also outside of the setting I think it really shines here. So we start off with the Koka language, which is a strict functional programming language with strong typing. And in particular, it has algebraic effects, and the first step will compile this algebraic effects away into pure lambda calculus, using the evidence passing style. Introduced last year in ICFP. And then we go from the pure Lambda calculus, to C-code, by inserting reference counting instructions, according to the, to the purchase algorithm which was also introduced last year at PLDI. And a nice thing about these two steps, is that none of them need to capture the stack in any way. So in particularly obtain C code, just use standard malloc and free. There's no need for any kind of GC, or root scanning or any anything like this. Before I get to our contribution let me talk about Perceus, . Unlike most reference counting implementation it's not...
based. Northe have a list xs here, and we met over this list, creating a new list ys, which we then print, and most reference counting and struck implementations, for example the one listed below. Drop these two lists at the end of the scope, in particular means that those lists are live at the same time, both in memory printing YS this is unnecessary, because XS is not needed while printing YS. And that's what Perceus does, passes ownership from the map function to the print function, and now XS is dedeleted while YS being constructed. So how does this work in the map function? So I'll focus on the first construction. We got on the cell Xs, we matched on it. And we want to continue using x and xs. So the first step have to increment the reference count offer the children, inserting the stop inSTRUSHGS, and have drop instruction, which gets rid of the cell excess, either by decrement the reference count all by freeing it if the reference count happens to be zero after the decrement. And particularly nice fact about this, now xs is delated as soon as possible, match on it, and need xs to match on it, and don't need it and can immediately get rid of it. P the Perceus it's true for all cells, this is property called garbage free, that means at one time, all cells will be freed exactly when no longer used.
And of course few other nice optimizations you can do to make the example even more efficient, and however want to talk about one particular optimization, and that's reuse analysis, as you can see here, let's assume the list xs is unique, and comes in reference count of 1, and drop instruction decrease reference count xs go down to 0 and freed.
Immediately after we allocate new cons cell, his seems kind of wasteful, that we are freeing ourselves, and then allocating a sell off the same size. .
So what we do is instead install calorie use. If it happens to be unique, and if so it saves the cell as the reuse token, R, or writes null if not unique. So if null... or reverse, actually the cell we rewrite we already have. And it turns out that this can be a surprisingly effective optimization that can often, at least in some benchmarks make gigantic difference in perperformanceperformance. However in this work, we found that it is not quite so simple, and we show several examples how reuse analysis that have like analyses that have been implemented in the past fall short. So take for example, we pass to map function, and allocate new concept, and other previous reimplementation state is that they would see, we have cons, Let's try to drop reuse access so that we can use it for the new cons cell. And in a way this is pretty good because we know have a chance to reuse xs for this in itself but in a way it's also not good, because we need to make sure that xs still live. And in particularly, that means the xs we pass down to map will not be unique, and no resues can take place in the recursive call. And that means that map can no longer update to xs and list and place, so we might have lost quite a lot of reuse opportunities, because map can reuse for each cell of the list of benefit of just having one extra reuse.
And even worse now hold on to xs, during recursive call, and doubleded our memory usage from baseline, of Perceus because we have xs and ys memory at the same time.
We have defined new reuse analysis, drop-guided reuse, that does not suffer from the problem, the guidal principle here, uided reuse analysis should not prevent ownership from being passed. That means in particular we want to only reuse dead value. And to achieve this Unlike previous reuse analysis, which usually would insert the proper use calls before inserting the reference count instructions. We now insert the drop reuse called after the reference count instruction after the Percius person algorithm has run.
Then it is particularly simple because that roll is exactly the values that will be dropped, and we can just be right, a drop into a drop reuse, where we have reuse opportunity, in particular this will avoid the problem from the last slide, and simplified the analysis, and made much less fragile.
However, we still had a bit of a problem with this, because the Percius algorithm is garbage free, will never hold on to cell longer than need to. But this is not true of previous reuse analysis, and it's also not true of our ... normally would drop xs and free xs, before recoversor recursive call. We can rewrite the drop into a drop reuse and hold on to the cell x s potentially as a reuse token.
R, and we'll hold on to it here at the drop reuse call, then do the recursive call to map, and then use it in the cons o in particular, are will suddenly stay live during this recursive call. And suddenly have this garbage in memory.
However, impeericly we found this is pretty good opportunity, this is optimization you should do, and turns out KWIE efficient for map functions we needed a better memory bound to precisely say how our automatic analysis here changes space usage of program. And we call this new memory bound frame limiters. The intuition is as follows. We have a small number of reuse cells and each function, which we can treat as a constant . Might be kept alive as we seen during the recursive call. So the example, number of recursive calls that we have times the size of cons cell, or in general, it will be frame limited, in the sense of this limited by a constant times the number of stack frames that we have at any given point in journeys around him off program.
So how can we actually reason about this and how can we prove particularly results actually frame limited. To do this we simplified the main Linear Resource calculus of Percings ius work. And made it easier to reason about when it's evaluated. And able to introduce just single stack condition, in order to be able to reason about space usage, and not only about when things go out of scope, and also, about exactly the extra space a program will need.
I'll give you one example of this calculus so you get a taste of it. Have here borrowed environment, variables which are not going to be referenced. And we have multiset of owned variables, that are reference counted. And in particular, this is linear environment, that means, if you take a look at the variable, here only consumed variable X, so linear environment only consist of X, but unlike normal linear logic, we can remove it from our context. PS we insert a specific drop instruction that decrement the reference count.
And we know when we take a look at the left construction, here we can have the stock condition to further reason about space usage on the lateral normally in the neurologic what you would do is that you split the context in to two parts, and then gamma 2 to compile E2.
And we can use the star condition to say presisly splitting. If we leave it unrestricted, certainly have sound calculus. And ensure, will at the end of the program, all garbage will be cleaned up.
So no space leaks in that sense. However can introduce the following role, if Y is a variable in gamma 2, needs to be in the free variable of E2.
Which ensures it's actually use in E2 and then recover the notion of garbage free nas. In the which was first introduced in the past this work also in this much, much more simplified copies. And we can use the following rule, where we say a variable can be in gamma to either if it's actually used in the expression. Or example, a typ analysis find out that the size of all of the memory that we can possibly refer to is less than a constant. In particular, reuse tokens.so we have syntaxic condition, condition that can allow us to actually know how like what space behavior, the program will have a contract .
The first step, we'll take an expression e, that was given by the user, and the drop guide instruction, that will yield garbage free program. And do reframe analysis, to turn some drops into drop reuses. And obtain new expression. However we can show that we have done these two accepts we have abTOIN new from E to E prime prime directly in the linear resource calculus, using the frame limited star role. This proves the output, you will relative to the source program, to concludes reuse optimization is not garbage free, and trade off to be made, and think it's better to do the trade off this way.
But it is frame limited and show other examples in the paper..
borrow inference, not frame limited. And... is frame limited in a certain tense, and can read more in the reference paper. To conclude, I want to give you one last example how analysis really shines. We have to find a new insertion algorithm for red black trees, and we start off...which is either red or black, a left child and right child, a key and a value. And we define a zipper of this tree. We of course can turn the zipper back into the a tree using the move up function, and the find, has cells which have exactly the same size as the cells of our original tree, which means that we use analysis can work here to turn the zipper back into the tree which makes us a very cheap function. In particular, ensure the zipper is always U knee, this can always happen. And we do not glance on the way down, and just record whether we went left and right, and use the appropriate zipper to remember this. And again, the free we put in, again, reuse analysis can work. If we find the see is already present in the map just move up and reconstruct the tree from the zipper, if the tree is not Brent, we have to balance, and you can use the cells of the zipper for cell of the tree. In the end this algorithm, if the tree is unique, except the one cell we put in with new key and new value, really indicates the main mum amount possible. We have the benchmark. 4,200,000 elements into the factory. You can see the reuse analysis work better here. And see Koka , OCaml, and Haskell and swift, and C + +.
We have more benchmark, queens benchmark, a constant folding benchmarks and the binary trees benchmark from the benchmark scheme. And we can see and these benchmarks that are reused analysis is never worse than the alternatives analysisand competitive against virtual systems.
Thank you, and I'm ready to take questions. Thank you.
[Applause]
>> Questions?
>> So how do you regression models cons if you have to wait for the function to execute if it's used or not.
>> For example, you mean in the map function.
>> Yeah, for example, yeah.
Soo... recursion modular concept means that we can do the recursive call to map, and sort of allocate the cons cell up front, that's the really tricky here, reuse the con ises d up front, and allocating it, and then do the recursive call, so.
>> Can you go back to the first red black tree slide.
>> First, sorry.
>> First slide on the red black trees. Sorry.
So in the move up function, when you switch from the node R to the node, do you notice that the CLK, V field all right in the right place already, so you don't have to write them to memory. And it's something we an optimization called reuse specialization and that is covered in the previous paper in more detail.
You showed us a rule for less than add a star that can be replaced with different side conditions, and one of them was all the variables in the context are appearing free and easy to which you said means that they're all actually used, but with conditional logic, how is that the case, they might only be mentioned in some branches of pattern matching, and therefore not necessarily used. That can happen it's only use one branch of pattern match mag.
>> Got it, So it's an approximation on optimality just like the garbage collector. .
>> Yes.
>> More questions. Thanks, a part that was a bit too fast for me. Knowing that something is time limited in this sense.
What guarantee do you have on the worst case... of the program. Can you... do you have a guarantee in terms of... the maximal amount, if it's going to be constant more than the optimal one.
>> Sorry, you mean the number of cells extra.
>> Yes, can you property being limited in terms of size.
>> So here we... yes, of course each visual only has a constant size, and also know constant amount of variables, that we have in the gamma constant at that moment.
Is it the case, that you can have total live space of a program. Is it true. Or maybe it's HPS. yeah, I think we can talk about this later. I think that's good.
>> Thank you, we'll take the rest of the discussion offline, thank you, Anton again [applause]
>> The next speaker is Sam Westrick, referring the work of enentanglement, in detection with near zero cost. And had is discussion paper.
>> Thank you, my name is Sam Westrick, I'm a researcher. At Carnegie Mellon University.
And I just wanted to say we're honored and humbled to be awarded distinguished paper on on this work. So this is joint work with PhD student Jatin, and our advisor .
This work is guided by more generally by a question of whether or not parallel functional programming can be fast and scalable. This has been a long standing question in the community because functional programming has great promise for parallel programming, except for the question of efficiency, or for a long standing time the question of efficiency has been a difficult one. The main challenge is really one of memory management, not so much the complication, say, but actually in the runtime system how do you implement an efficient garbage collector for a parallel functional language, okayokay. Functional programs, are somewhat unique, n that they have a high rate of allocation, rather than modifying something in place, functional programs tend to allocate something fresh. Okay. So creates high rate of allocation, and therefore high reliance on garbage collection. To get good spatial hit space efficiency.
Okay, now in the parallel setting, the rate of allocation grows linearly with the number of processors that you're using or Canada grow licensely with the number of processes Okay, so we have a much more difficult garbage collection problem essentially this garbage collection for functional languages is fundamentally harder than garbage collection for imperative languages, in some sense. Okay. Now, we, over the past five or six years I've been working on this problem and we can definitively say at this point, actually yes functional programming can be fast and scalable. Okay and the way that we've shown this is through a property that we call this disdisentanglement, 10,000 feet that's quick high level sketch of what is disentanglement it's, I'll give you an informal definition here and then we'll talk more formally later but the idea is that concurrent tasks remain oblivious to each other's allegations. So two tasks excusing executing...executing simultaneously concurrently really, and they are both performing allocations, then disentanglement says there should not be pointers across the gap shouldn't point across to each other.
So this turns out to be very closely related to determinism.
In particular we show that programs that are determined to see That's a very strong correctness condition that guarantees determinism. If you have that property then you get disentanglement for free. This means it emerges natural ly in function programs. FTd what's really cool about this is that it enables efficient and scalable automatic memory management, okay, and the intuition here is no cross pointers and cross pointers are a long standing kind of challenge in designing and efficient parallel garbage collector that pointers across separately. Concurrently, generated regions of memory can degrade the scalability of collective.
So, based on this disentanglement property over the past five or six years we've been developing this compiler that we call maple based on Milton, and we support the full standard ML language, and what we've done is we've extended it with a single constructs that we call par par basically just evaluates two things in parallel, and returns the results back to.
This is fairly robust implementation. You can go and use it right now. In fact, it's being used at Carnegie Mellon over 500 students approximately every year use it. We use it as part of our undergraduate program to help teach parallel algorithms. And so we have a completely new runtime system.
Certainly the compiler is Milton, but the runtime system is a new runtime system based on disentanglement. And one of the key features actually is that we have a provably efficient algorithm that we presented at POPL 2021, based on disentanglement. Okay, now we're getting really really good performance with this in comparison to a few other memory minutes languages. For example, in comparison to Java on 72 cores this is a decent sized multi core machine. We're about 3x faster on average across a variety of benchmarks, and these are benchmarks that we've ported from state of the art, C and C Plus Plus benchmark suites. So it kind of got what we're seeing is on average about 3x faster and about 4x less space in comparison to go about to accessory maybe 30% less space.
. And now we've actually just recently been some comparisons with the new multicore OCaml project. Nd I'm really excited to see so much interest in parallel functional programming in this community. Now multicore, OCamel, is quite a bit different from our compiler, like for example, our compiler is a whole program compiler.
That's one source of improved efficiency, but also by is quite a bit different from our compiler, like for example, our compiler is a whole program compiler. That's one source of improved efficiency, but also by performance in comparison to C and C++. The margins there are approximately a factor two in terms of time, on average, but the space footprints actually it's about identical on average in the benchmarks that were considered. So all of this sounds really great. What's the problem, and what's the focus of this talk?
So... the issue is that not all programs are disentangled of course, you have restriction you can't communicate across the gap, and if we have GC that talked for it. And very bad things can happen. Here two tasks running in parallel, and there is pointers between the objects, they allocated. In a Well now if a garbage collection happens on the left hand side, one of the things that the garbage collector may want to do computer memory. And in order to compact memory you need to move objects around, you need to rename them essentially give them a new location. Now when you do this you need to update all references to any relocated objects, but a garbage collector based on disentanglement essentially assumes that cross pointers are impossible and doesn't try to even check for themm. So causes DANGalling pointer, and very bad, and terrible things can happen. If thewe need to enforce this entanglement and that's really what we focus on in this particular paper, question of how to enforce this entanglement, So the immediate question you might have if maybe we can do it statically. Okay and this turns out to be really tricky and we've worked on this problem for quite some time and haven't yet gotten something that we feel is quite ready.
There's a few things to consider here. How might you go about doing this. Well the first thing you could try is disallowing in place updates. This works, this, this guarantees disentanglement by construction. And extremely inefficient. In particular, this is really important for parallel computing, perhaps could be most fundamental operation and parallel computing is a parallel fillarrra why.
Allocate an array and then in parallel, you fill in each element that fundamentally is mutation, regardless of whether or not at the surface level it Okay, at the low level, the garbage collector sees mutation, it is fundamentally we need to meet to be able to support it efficiently. So you might think There may be some sort of typing effects system would would be able to enforce this And we've tried to develop something like this, but basically the problem that we run into is that it's too conservative. We have a big benchmark suite of approximately 30 fairly sophisticated parallel algorithms that we've ported C and C++. And very, very common thing is parallel algorithms, despite wanting to be deterministic, that's a common correctness condition often will utilize a very small amount of non determinism, to gain significant performance advantage, and the intuition here is that with a little bit of say atomic in place, modification, maybe a compare and swap that sink that kind of is on the fly, communicating with a concurrent tasks you can eliminate a large amount of unnecessary synchronization.
This becomes very challenging to design thing that is essentially nondeterministic, and aliasing between different objects that have been allocated. It seems very difficult to detect this property statically, but it is something that we're still working on and we're interested in pursuing what we do in this paper is we instead do it dynamically. And the intuition, we want to monitor individual read and is writes to memory. If we ever detect entanglement, then we can terminate the program safely within their message, you can generalize this to say a dynamic exception that could even be handled if you'd like, but our implementation at the moment this very simple.
This is very similar to the concept of race detection. If you've seen rates detection in other contexts, Now the difference here is that we're able to detection can be accomplished with essentially zero overhead,your practice.
Race detection. As many of you I'm sure are familiar has massive. .. overheads, typical sort of overhead that you see is a factor 10 on a single core.
Those overheads tend to increase as you go parallel, our entanglement detection results in paper fully parallel, and overheads are about are consistent across different core counts. And what we're seeing is that it's on average about 1% across the board. That's for both time and space. And the max we have seen is only 10% overhead one of the key contributions in the paper is trying to identify whether or not, the entanglement detection algorithm is good. Okay, what does it mean for detection algorithm tobe good. O we formulate two conditions here we call soundness and completeness.
Okay, two sides of the coin that the non negotiable. One is that needs to be safe for disentanglement. Okay, so it should be safe to run a disentangle program, which will be memory safe to run it disentangle program. Okay. And that can be formulated thankfully as a sound this property for disentanglement. Or another way of saying that it's there's no alarm that you miss.
Now on the other side of the coin, we have completeness, and in some sense this is just as important, what we found, any stat tick technique, it would rule out ASL of our BAEFRJ marks.
So this particular technique, is to allow for all disentangle program to completion. Okay, now. should say relative to particularly execution. But we're dealing with non determinism here. Okay, but I'd be happy to talk about that.
Later. It's a bit of a subtle point, some details of this algorithm for you. How do we formally define entanglement, and is finis terms of computational graph. Think of history of execution or trace of execution, writing down what the execution did, and abstracting details of scheduling.
So the edges of edges are sequential dependencies between those instructions. Okay, and in some sense the whip of this computation graph is the parallelism that you're getting at any moment. And that parallel grows and shrinks as the run the program, in the computation graph, we can annotate different instructions and allocate where they occur, and where they are used. When I say used, even holding a pointer to allow DAGS, we consider use. That's what is relevant for garbage collection.
And includes accessing location, and all constitutes use. Turns out you can define disentanglement as POT property, basically, you are uses, between allocations, or sorry, every allegation proceeds all of the uses of that allocation. Okay, so basically just that your uses of your allocations respect the sequential dependencies that are inherent to your program, entanglement is when you essentially get to use use across the algorithm that we implement in this paper, and that we analyze in this paper is extremely simple. It's exactly the definition, ssentially, we build the computation graph during execution. Okay, we annotate, all of our allocations with a vertex in the computation graph. And then the key results is that we only need to check the results of memory reads, so.
Any time you load something from memory you get back name of location, have to check if that before me.
If you detext it's SFWOID not before you, and at the point you get detection and so you may with thinking, PS women, first observation, you don't need whole instructions, but actually you only need things at the granularity of whole tasks.
Okay. And in a parallel program that has good granularity control which is a necessity for good performance in the program anyway. These tasks are actually large enough, but essentially the overhead of maintaining the computation graph goes down close to zero. And in particular what we're taking what we're using here is a technique called SP order maintenance. PS got from the risk detection literature a lot of this work is inspired by that literature.
Okay and that's pure domains maintenance is essentially a very clever encoding of the computation graph that, to be honest users essentially no additional space at runtime.
Now the way we actually increment this is a weed ... So every time that you dereference something in memory, you have to do a check. Now that's like only immutable, the references, so that means this but there's already no overhead for immutable data. And one of the key contributions and this work is proving that immutable data is always safe for disentanglement. One of the cool things I don't have time to talk about in the talk, but hope you see until the paper, we have really effective fast path for the read barrier, essentially the fast past of barrier, very low overhead is 99% of the time, key why this...performance so efficiently, and we call that idea entanglement candidates, it's a way of tracking potentially hazardous mutable objects at runtime. And I'll just mentioned, it's very closely integrated with memory management in order to make this safe for garbage collection.
So all of this is available in maple. I should and I should mention the performance results that I mentioned earlier, you know 3x faster than Java to access with an NGO that includes the cost of entanglement detection, especially if it's a fair comparison in the sense of like our languages memory safe and we're comparing it to other memory safe languages.
So what we do in this paper really is that this disentangling property which seems to be extremely important for efficient memory management is efficiently checkable at runtime. Okay. And we have a maple implementation of it, which is practical and fast. And I really encourage you all to go check it out. And one avenue for future work that we're really excited about that this opens up is the possibility of what we call entanglement management.
Intuition there, there is that at the moment of detection handle entanglement dynamically six it up, that might cost a little bit but we can fix it up in that moment. And therefore, essentially make this disentangling product, a strictly a performance property ,. Right now with a kind of halfway between performance and correctness, and we'd like to turn it into a property which is purely performance property.
I believe that this thing that management idea can get us there.
If you'd like to hear more about in disentanglement, please come see my keynote DMH on Thursday, and I'll be going into the details of how the garbage collector works and all of that.
.
Thank you so much for listening.
>> Thank you.
[Applause]
>> Questions?
>> So if the check fails the program crashes, right, you looked at number of benchmark examples, did you find anywhere the checks fail.
>> A great example where it fails is lock free data structures. So you have communor communication, essentially on the fly communication between concurrent tasks utilizing something like the lock free data structure.
Then, then maybe you'll get entanglements. Another common case explicit just locks, right, but to be honest is extremely uncommon and inefficient parallel programs. It's much more like a concurrency issue that apparel is an issue right so in general we're finding it too there's a little bit of concurrency that gets injected into the program. You might start to see the possibility of entanglements. All the examples that you have been looking at fairly uncommon. And able to do Delani triangulation we're able to do parallel breadth first search with like the fast like sparse offerings like hash deduplication, like it's really surprising what you can do it disentanglement and not run into this entanglement problem..
>> You mentioned you can detect entanglement statically, but it's not precise enough. FT have you thought of combining the two and sort of, when you can prove disentanglement not doing the dynamic, We are already combining the two because of a we do immutable. We allied any sort of read barrier for immutable data. So in some sense, we're already combining the two and a little in some extent. But yeah, it'd be really cool to have a much more powerful static, perhaps like a typing system that could give us more information. Yeah and the thing I didn't have a chance to talk about, the entanglement,entanglement,which tracks potentially hazardous mutable cells at runtime, it works fairly well but we have identified a few places where if you're not careful, you know, it could still not do a great job and I think that could be a great opportunity to, yeah. .
>> So do you need entire KOFRN TUtationtional graph at runtime or could you possibly use like a partial or total ordering the one that actually has maintained at runtime is very partial essentially only the tasks tree, and you throw away all the old stuff that's in the past.
I mean there's some nuances there that are kind of hard to get into in a short talk.
Because we encode vertices in the graph and a clever way and kind of old vertices might linger around and we can still query hem, but actually look at the space cost of maintaining the computation graph, it's essentially just the number of active tasks.
>> One question.
>> Hi. So you mentioned that stuff like LOC3 data structures, ifif. Am I understanding correct that programs can't run, just error out.
>> So it is possible to extend that barge, garbage collection.
Switches to a slower but a version that will still handle that sort of thing. Exactly, yeah, this is this idea of, we're hoping to pursue this idea of entanglement management, we're calling an entanglement management, where at the moment you detect entanglement you fix it up, and and the intuition would be that you will pay for entanglement when it occurs. I mean, we think that this technique basically opens that door,door, to investigating that.
Thank you, [applause]
>> The next speaker, presenting generating circumstances with generators. Today I'm talking about generating circuits with generators, and first would like to motivate my choice. And why so recently, John L Hemnessy.
In the next decade, the Cambrian explosion... and so functional programming. And said that, the PL community standeds on the critical path to the new golden age computer architecture, FOFRJ FOFRJly, there is addone dance of intellectual challenges that are on the cusp of a new golden age different languages. So it looks like there are things to be done here. So there are some things that are motivating me personally. So one thing is open source sites. So for a long time. How was development tools were closed. And more and more tools, and so that's great. And another thing motivating me is RISC-V, open CPU architecture development at Berkeley bowfin in academia and in industry and.
Everyone loves it. Good motivation I think. Another thing, is FPGA's, the things that are kind of PCU's, interpret circuits.
And domain specific acceleration in the cloud.
So do some stuff for you.
So what is... digital circuit exactly. Some of you network of image of gates, and very low nodal. And take a high level approach. Of mini machines.
And this state changes from one to another. And discrete, and have two functions. One transition functions. And one that says... another output functions, what is the out put of circuit in current cycle.
Show you a few examples. Andstores the current value of the counter and then the other, which can increment the counter.
So the value of the counter trend cycle when the input is one, so we can with one d that one can be exploited bee localment.
And clearly see the transition function, and our function in this. And another variant of the counter. This... output depends on the site. No it stayed only in the next clock cycle. After the input, so, we see that in the first clock cycle. Implemented only in the second cycle.
So the last example I want to show you is one with counter flow, so you can traditionally express control flow in mini machines as state diagrams, and on the edges are the conditions, which show you, which conditions you change the current state.
So this kind of looks like flow truss. And program doesn't really use flow truss, any more.
Using... kind of looks like...
So what we really like to do is have some structural way to describe control flow.
. And this is what I decided to do. I developed programming language, and it's a language for describing automata of course. Its embedded in clash program. Cluster for hi bred.
Hybred. And also can be compiled.
I use Haskle, l. It is a...
can be embedded in a way. And another feature is... this tax is unbeded essentially. But, amount of state in machine is constant rate.
Okay. Core feature of this language. Yield statement, generic abstraction. Generates, and simultaneous l, and bvances the clock so that the next statements are using the input values from the next clock cycle. Let's go back to what I was showing before Right, so this example for circuit. And we see that each described, we have this variable, which stores the current value of the counter or loop, which runs forever. And we have statements inside the loop, the assignment will commence the call to this extent, we extend, one value, and then we change the value.
For example, if their input. Is the next output.
And... in the Moore, version.
It's a little bit more involved.
We need to yield the value which was in the state of the current cycle. But at the point, I do the weed of, or the next climate cycling.
Clear we are already in the next clock cycle, Given the semantic segment, right, so we need to access the value of the input from before the yield so we start using this statement here, and this part that will turns out is very useful language, so I added sugar following this.
So this is value of I before the yield.
So in this way, we don't need to use this let any more.
So last example, with control flow. And here we see normal structure way to describe. No longer a big switch statement.
Right?
So first loping is using continue statement. Another on until. And use magic prime. As in the same cycle. Before the yield.
And after those two loops, we had which is corresponds at the bottom, and then we need to go back to the start so is this. , so we see that this control flow and many other useful invoice of control flow can be easily described.
How this language works. And get number of transformation.
And eliminates loops, and have Lambda lifting, that makes all the functions... they have normalization which stands for almost a normal form. And in the multiple variables.
And here, the implicit replaced by explicit using data types, and next transition, makes that each function yields exactly once before making tail call so the functions can correspond 1 to 1, to states or group of states. And cleans up the code, and then when we have group of functions, and each exactly once, can mediate after that.
Ands here an example how that works. And you can read in the paper. Here we see how that works for mini counter, you have one function, and increments, the value, the mow variations as we see the as we see it yields the value which was the parameter, but course, or if the income and value. And last example, from counter flow, compiles. It's easy to see recursive functions that use if statements to select which function is the next state.
Okay. So... now it's time to see some applications.
So this is how language develops. How does it impact.
Can we do something with it?
So first example pretty simple.
Display. Probably know them.
HDA 4478200. If never programmed them. Before you start writing to them you need to initialize them there is a sequence of commands which needs to be timed exactly to initialize the displays on the after them. Often initialization, you can work with them m. So, normally you will have them using microcontrollers. And it's easy, if you try different languages it's complex.
So I SFOUPD imply LEMation called open hours I tried to implement this time, using... so my implementation, only 149 Were only 40 code lines of yield FSM, and that has those helper definition, and 26 blocks, that look like this, describes all the things to timing, and counting amount of timing for each command to output thes that happen on operations, and so on, so it's not easy to read, write, and not anything you want to work with.
And when you use, FSM becomes something like this..
And path is here. So this initial sequence is first 8 calls and cools and delay.
First arguments is amount of mille seconds to wait. Is second, because the display was giving data, and last argument is instruction we send. And thanks, to... pretty incredible.
And just wait or commands in the on the bus, and when the command that is we just send it to the days play. Display.
Obviously, implement risk five processor and implemented not one but three of them, or three of them are multi cycle. Like textbook one. And first one, not complicated. And the same data process before.
And the last one, youthe most experimental one, where I imprinted the whole multigrid processor using either the FSM.
.
So replacing control path. And keeping the same performance, as before.
The implementation. Quite a lot smaller. And some performance loss.
And didn't yet find the reason for that. Still open question why that happened. So the conclusions we can compare can compile structural procedure, and conclusions.
We can compile structural procedural content without data.
I think that that's right. So now we can write things like this in much more readable and maintainable way. And we didn't lose expressivity in development right so we can all the things we could do in verilog or VHDL, we can still do. The code in the language is shorter. And lowers the barrier to enter.
Okay, so some work I like to do later. And functions, for this.
And this is first of the functions I like to do simple.
And because, you have this kind of of effect, look if there this can be generalized in some way.
And that's all, thank you.
[Applause]
>> Questions?
>> So as I understand it the benefit you gave of the example over here.
>> When you speak to the mike don't know where it's coming from.
>> The example that you gave, I think I understand it to benefit from the fact that VHDL and Verilog doesn't have Lambda abstractions.
So, you know; you use Lambda abstraction to reduce the lines of code. More pertinent to FP understand more merely automata to compose very rich ways. Have you looked at algebra, of automata. I try to think a little about this, but I didn't really something which I I just decided to make the to allow a combining the automata just using clash as as this language for for for expressing connections in them so we can you big problem you want to solve you can you can encode into a series of simpler, smaller automata . Each of those you can you can write using, and then connect them together using crash.
>> Question over here.
>> Okay.
>> Just want to clarify what you meant by multicycle processer. Does that mean multipsychoermultipsychoer that mean it takes multiple cycles per instruction?
And yes, there's only one going at once. So it's not your classic kind of optimized processor. Is your language going to be useful for say pipelines processors in it useful for pipeline processors .
>> Because actually pipe pipe and or conflicted quite differently. This language. The whole idea was to implement control flow in some way, right.
And pipelines are not really about control flow about data floww. So not a language for pipelines.
>> So should use for relative simple state machines that are in some sense. Simple enough, they don't need all the complexities of VHDL. Is that the right way? To think about itt?
>> Probably yes.
>> One of the transformation, you mentioned, was eliminating nontail recursive calls.
>> Could you expand on are.
>> Again, which one you call stack? Reification reification.
So this is one of the most this paper, because what I'm doing there is some kind of combined de functionalisation and continuity ... I couldn't use normal ways to do it. With template Haskell. And they get passed. Over. Which is typed.
Have a different continuation for every type. Had to use another apreach. Use Some approach I taken was that I use control flow information to generate many different data types for each, for each strongly connected component .
>> One more question. So on the driver example, you said that it's 270VHDL. Versus 150 lines, versus clash. How does it compare with just clash.
Using clashes tooling of abstraction to their fullest.
>> I will answer this question, traditional implementation is in clash.
>> That can mean many different things, wiring together, and you are using like custom data types to manage your state etc So, is this comparing like high level clash or low level crash.
I tried to be somewhere the in the middle. Too involved in this amount of but, still don't want it to look like for workingg. So this is the.
>> Thank you, let's take the rest of the discussion offline, thank you, again [applause]
>> And then the next speaker talking about stage compilation with two level type theory.
>> Enter presentation mode.
>> Hello, everyone, this is going to be a talk about stage compilation for two level type theory Y stage compilation,
Writing code generating code good ergonomics and safety guarantees. And there are many examples for the existing infrastructure like this. We have typed template Haskell or the template Haskell, we have C++ templates we have traits macros and generics in rust, and in all of these cases there is a separation between what is the compile time language and what is the runtime language. In all these cases I think points to improve ergonomics and safety guarantees as well. So part of this talk is about how to do better than these systems in certain ways.
And what are motivation for using... one is low-cost, I'm saying low-cost, instead of zero cost. Because, in terms of code size when they are using sage compilation. However, I think in many situations it is get a moderate increase insize, but also get like large increase in performance. And I really like like the usual abstractions, developed in fact functional programming. And I would also like to. ... make the cost of these abstractions lower. And also domain specific languages. And this is quite important, I think most of the large performance gains, come from domain specific insights.
And it's not really feasible to have general purpose optimization in some compiler, that knows all about the insights. So make it possible for programmers to KEECH compilerses, and also, there is inlining and fusion with strong GRARN tees.
I have worked a lot, the writing high performance, Haskell code, and frustrating have fragile infrastructure, for inlining and FUNGS optimizations.
It's much more infrastructure... that for example, fusion happens and goes through. What is two level theory. First the idea was developed by Voevodsky. And idea was to do modular treatment for axioms, and two level type theory. And interestingly here.
The title says applications, but stage compilation is not mentioned at all. All applications... it turns out the system is applicable to two...
so, if punishes ood example for like a cross pollination of very distant fields. So what is the features, so there's an integration of a compact and language in the runtime, language, and there's also guarantee think of code output and also guarantee that staging and by vacillating I mean that in the, in the generated code there are no meta programming features anymore, which are, which live in the in the macro stage. And SUCHTs wide range of runtime and metalanguages. O here we can make a choice so we can make the two languages very similar, but we can also choose to make certain advantages here.
And have independent times, in metalanguage, and object language, so that's what I choose to develop in this paper.
So the setting, and also support the present, staging by evaluation. So this is analogous to normalization by evaluation in the sense that we are evaluating a stage program into a semantic domain, and then we are extracting decoder output from this multi domain. So this talk contains small programming examples, for tutorial example can see the artifact, and implementation of this, and samples, from go, and you can see the paper.
And basic rules, two universes.
And closed under type of... the universe of run it is time types, and you have something, will appear...
If you have some type in... it can not appear in the staging, because it happens to live only in the in the compile time language and during staging it has to disappear at some point.
It has to be consulted away.
Okay. And likewise if you have an inhabitant of some compile time type than that inhabitants value also cannot appear in the staging out put. And all in the same universe. No way to cross universes at all. Only way to cross the two universe is by SPAFK staging operations. So listing, if you have runtime type. And this up arrow A, and means type of metto programs, generating code with type A. So in other systems could be called code of A. And called. So instead of upper, called code, or X. So we have quoting. If you have any runtime value, and any runtime expression, you can quote it, and it's just a metaprogram that immediately returns that expression. And have splicing, so metaprogram that has slices.
Essentiallies f PSns that during staging smarter program will be executed and then decode result will be inserted into, into the output.
And we also have this to definition of inequalities. So quoting and splicing, definition of isomorphism And this is important if you want to do polymorphic and dependently type programming because programs will be only aboutto these two roles. And so And then, informally Staging means we are running or meta programs in s[isplices. And then insert the results in the code output.
Okay, so let's look at some more examples. So we can just do inline definition. In this case, the program consists of just two top level definitions. And we have a meta level definition of two which is just a quotation of a sub zero sub zer.
But here run zero.
So here the definition is just a quoted expression, and when defining the function in the runtime language, I can just use a splice. And then when I do the staging, then only the object level bindings remain.
So in case, only have F.
And I perform the splicing.
Okay. Let's look at the compile-time identity function.
So this ID is an ordinary polymorphic identity function, but here I'm using a notation like for a PI type. So if I have any typing new one, this is just a polymorphic identity, but because I have ... , I can use, use these functions in object level code. So in runtime code as well. So here, this is the identity function for Boolean, but I'm doing the splicing, and I'm calling to this metal halide and function, and I'd have to pass the time. And then I have to pass an expression, and the time that I pass is just the lift of ... So because this is the time, then I can pass a quotation of an expression as the next argument and.
And then the output, I can write alternative function.
That is bit more interesting. I say, I want to RUP run...say that I only want to quantify over the runtime types. However, the quantification itself happens in the compile time language. So this a is more like an expression of a runtime time.
And then, it's still the usual identity function. But because this is a lifting of something, it's more like an expression efforts splice it, and then I have to live with back again.
So this demonstration, we have staging and quotation for as well. So see later example, how this has to be used.
And I probably will skip this.
But the point is that this example is only up to the previous mentioned definition, so here I have this type, but actually expect type of this this form. That says, quotation, mispricing or definition isomorphisms and sorry I also mentioned, I should stronger than anything else. So this is kind of borrowed from the, from the meta notation of splicing. If you do staging here, once again, just. So, here the nothing interesting happen, but if we go to a slightly more complicated example for doing the map function inlining then really hear the need to use abstraction over runtime types, because we want to use runtime lists, and as I mentioned before, the only way to grasp between the different stages, is to use lifting and splicing and quotation. Must be a runtime type as an argument, so we can only observe over time facts like this And if you look at the definition of heritability solenoids properly explained in detail. The point is that now we can use the inline map function and if it goes on staging than what we get is essentially this further zero function And we have inline function into this definition.
And this looks.
Might look a bit noisy with all the quotations and splices, and roughly the level of noise, you have to concern yourself with, using Haskell which is quite noise, but in the system can do strong inference for quotes and splices. So before looking at the inference, to note, we have preservation of types.
So listing of a function, and isomorphic definition.
Here this is notation for dependent paradigm is isomorphic to a dependent pair of things.
And we can use these properties, and use bidirectional elaboration, and also the fact we stayed in universes. We do not have the template haske, l.
So here, we can use bidirectal elaboration, and subTIEBing for all the splices, and in the can just become like this. And also implement the artifact demo.
And we can also stage types.
So if incomplete have a natural number. I can by inductionen otheothe almost I'm using.
And you can see that what happens here is that I'm computing Tuple of certain length. How much time do I have 3 minutes. Okay. So, in I lose induction on compile time data, I can compute times, and also use dependentment elimination.
On dependancy types programs, of computed times, and in this case, to define mapping function, have to use DPEMENTent types Northrth stage compilation are quite compelling because one of the one of the use cases one of the important use cases in staging is generic programming.
And although in like normal programming, we can get by without dependent types in generic programming. It's quite common that you really need dependent types, to make a generic program. Okay, so more things, in the artifact, there is implementation stage for the PUGS. And therapeutic fusion type staged SDLC interpreter, and also demonstration of monadic let insertion. And in the paper, that's the formal so in the paper staging these evaluation of level type theory in propitiates over the object theory syntax and correctness of staging is a content activity property and correctness is shown by proof read ontological relations internally to....
>> Thank you.
[Applause]
>> Very nice work. I was wondering how difficult was this to implement compared to a normal dependly type language.
>> Not very difficult. The staging algorithm it is, is even simpler than the normalization by other evaluation that you have to implement for conversion.
Checking. So, yeah, I mean, if you have dependent types you have to do me a conversion, but staging is simpler..
>> Follow-up question, how difficult to add to Agda.
>> Let's discuss many. But the specifics details of... can make it possibly nightmarish.
[Laughter]
>> Thank you very much. Great talk. In the middleal. Yep. I had a quick question. So you showed what it's like to write these staged programs. But what happens if I want to prove something about the stage computation, do I have to prove something about generated code, or is there a way to prove stage itself. You can use the full power of the metal paper...metal level type theory to reason about object level programs, but only up to definitional equality. So, like the bat notion of equality is just to for the object, programs, and it is useful for many things, but it's not really semantic notion of program.
>> So you can observe what the result of the staging will be, but you can't prove by induction that every staged computation is well behaved. You can mov you are on...prove properties about your genetic code inside the system. Thanks very much.
>> You had function identity function earlier, you identified on tape A universe unwith.
Anything that limits you from being stage polymorphic saying saying that it works for some universe you such that the A and the A are still at the same universe level. Es, so this is potential future work. And, but in order to make the sound, you probably need another universe.
E, and then you say that polymorphism over zero and one can be expressed in U2. But this is possible.
>> Any more questions. It wasn't clear to me if there was additional power in the metalanguage. Do you get typecase for example. Do FRM PL ... this is also part of future work. Depends on the general setup. Question is, it could do... analysis of expressions in general So if you have something with type, lift the then you can get a meta program which like matches on the structure of the expression, and then this does something, and it's a complicated question. It's not yet implemented but it's also discussed in the paper, to some extent.
>> Okay. Okay, these were all the questions, thank you again.
[Applause]
>> Last speaker in this session who will be presenting experience report on random testing of higher order blockchain language.
>> ILYA SERGEY: All right, good afternoon, everyone, it's my great pleasure to present our joint work with Tram Hoang, Anton and Leonidas Lampropoulos.
So I'm going to talk about our experiment with applying property based testing to language layer in an industrial grade blockchain system. . And this was an experiment report.
What is the blockchain is a question that I'm not going to answer in this doc, c, if you've been around for the last five years, you could have noticed an explosion of blockchain consensus systems have different levels of expressivity. Or even if your area, whatsoever, but you weren't showing up at ICFP You still know what blockchain is So the question that I'm going to focus on is how do we do programming for blockchain? And what good and bad things can happen o it turns out that if you want to have your functional core imperative code replicated using Blockchain consensus protocol, it's actually pretty easy to join You're just package your code as a module with a state and a function into a model, which is usually called a smart contract, and then you make a proposal to the quorum in the system in the form of a transaction. And before you propose the code to be massively replicated. And diligence, and parser, and do the check in.
And make sure this is the case.
The most important, allocate some of amount of virtual al currency.
Typically associated with the system. Just to compensate all the parties will be replicating the same process of validation, just to double check that you are not proposing something boggles before your code is replicated and propagated to the system, after which every participant in the consensus protocol has a copy of your code. And question how to evolve the state, and use the code already deployeden othe blockchain, that also turns out to be relatively straightforward so one thing that you need to do is to identify the smart contract, the module that you're going to extract with. You're going to go, and then you're going to form a message, which again comes with some money attached to that. All that makes it into transaction, after which this message containing the directives for the language interpreter runs to this interpreter and if it passes the validation and interpreter, gives a new state, the same process repeats, involved parties just to validate that you are proposing something that, after which the changes are being replicated and the system, and the state has been consistently updated. And the reason why you need to attach some currency to that is just to make sure that you don't waste other people's resources. So that's why you are paying upfront, and some of them are going to get these funds as a reward one way or another.
So the question is what can possibly go wrong with this. As you're probably aware, many things, but part of smart contracts themselves, but happened to be introduced. In there, and I'm going to give you a three particular scenarios of how things can go very, very badly, using my favorite characters from Futurama. I mean, Lila, and Professor François. In the first scenario, me decides to score some funds, and she does it in a very fashionable way. By making decentralized content company.
So, in this campaign, the most important function is the one that will allow me to withdraw the donations as a part of the validation in the function withdrawal so she can take note of them and probably think them properly. So me knows that the type sound as guarantees provided by the ... if there are no exceptions, it will draw no exceptions.
Amy is wrong is the judgment, contrary to expectations, B is informed. MLnd this is certainly not the center expects and in tourism and that if a type has done its exception for you. She is wrong. Let's see what can possibly go wrong, out of them.
So once me has written her contract and deployed on the blockchain. She has no way to take advantage because the contract and the state has been have been massively replicate it. After that lucky start to get some donations. At the point decide to exercise her her mighty campaign and withdraw the donation backs The best thing happens, if at least one of the data submitted their own data. So the exception is strong, and the contract blocks this money forever so neither the backers can get it back. If the contract is structured in a certain weight neither me can enjoy for cash. So this is certainly not a great scenario.
Let's see what else can go wrong with of language frastructure so now we are focusing on Lila, who is a language engineer and Lila is in charge of maintaining the reference interpreter for the language that gives the logic to smart contract deployed on the blockchain. Following the suggestions by the users.
Sorry, little decides to add the operation for computing the power of a certain features certain days, or certain arguments and GHR implementation is perfectly correct. The only problem is that it takes linear time in the size of the argument, to compute the power of number.
Remember this is computationment in that computation that is going to be validated by multiple parties, not just the one who deploys the respondent transaction. The problem was Lila's code is that it disproportionately charges too little of cash for computing the power function where the cost is linear, the implementation only charge logarithmic costs. Let's see what can go wrong now so now when this is interpreted this interpreter is a part of the time for the blockchai consensus, but we have a discrepancy, the fact that transactions that have to do with computing the power of a number of very cheap to propose in terms of the funds that needs to be allocated for them, but they are quite expensive to execute in terms of the real computing resources and it doesn't take long for someone to recognize that fact, after they will close it was messages to cause contraction contraction, and eventually denial of service and then I will serve as this is a real pain in blockchains as somebody with, with my older posts and particularly unpleasant, when they're called cost by the language implementation, not some serious networking issues.
Okay, so that was probably the most exotic one let's focus on Professor funds fourth, who decided to solve the performance issues in radical could processing. Everyone likes, professor...o, this becomes the de facto part of the client and everyone starts losing this compiler for every single smart contract in. The only problem that this compiler is too cleve implementing certain optimizations, it hit famous complexity results social circle control flow analysis, taking a cubic time in the size of the program. And again, it doesn't take long for someone to recognize the efficiency and leader of the system with the contracts that are very small, but those kinds of optimizations look into the congestion and to the denial of ervice in the system. Okay, now we have seen three scenarios of rubber plants are unpleasant consequences of what I'm going to go language letterbox. Anthose are not the backs of the smart contracts.
Those are the bugs in the infrastructure that executes the smart contracts invalidate the smart contract, such as the type checker and an interpreter, such as the misalignment between the cost semantics and execution, and real education costs, and finally the box in the compiler.
Okay, so in this talk, and then the rest of it, I'm going to tell what what we did in language layer of the real world blockchain, using tech also firstive oallvery small, smart contract language which is built, or based on system with some extensions, it's intentionally non Turing complete it doesn't but it has recently different effects and all the interactive smart contract is structured as communication between independent actors. So this is a practically relevant language it has been adopted by a block chain and there are several 1000 contracts within it and when users are using the code returns filler daily If you're interested more in the details of the language itself out, ee the usual suspects, such as the type of structure and the type application. Also has no camo style imperative fragment with reading, writing, mutable state emitting events, and sending the messages, and the context of look like that you don't really have to read this, you can just notice that well, there is such a contract definition. There mutable fields and the whole structure as conditions that most time results in sending and receiving messages. So one interesting detail about implementation of silver, which is by the way, Don is interpreter, which written according to best practices in the monadic. So, monad. So not only the use of molecules quite justified so you can see the Oh camel led by innovation and highlight the the structure of the interpreter will be essential for engineering for the best thing we are going to adopt the standard mechanism of property based testing, write your properties as Boolean functions to implement the generators they played random inputs and write your properties as Boolean functions to implement the generators they played random inputs and nd has been replicated for multiple languages, and settings. Okay, so the property based testing becomes really interesting when we want to test metal properties of the language, such as the preservation so e key part here is a most of these properties.
They're really conditioned on certain vendors. And as you might know, even if they wanted something as simple as the type there's a reason, we really need to generate the term which is well typed in a certain context and solution to that which has a very little chance to work, is to generate the environment. The term and the type and check if we are lucky to have this to be well typed with regard to this type. And if not repeat this process so this interesting part of chicken the meta property itself, and much more clever solutions used to write the generator that first purchase well form types, and then based on them, reduces the chance, and this is something that has been studied quite a bit on the last few years years.
It's very much recognize to be a non trivial problem some solutions to that. I found instances in the framework section, nd more recent work by some of my coders who tested non interference Using generate on well for him to programs. Okay, so we follow this approach and we decided to adopt one of the state of the art schools for property based that provides good possibilities to generate types and structure data.
And quarter of this work. Had quite a bit of success in academia and used as teaching material in the software foundation Sed as teaching material in the software foundation series. So one of the reasons that we decided to switch it is the fact that it's very well formed programs it's also has a reasonably good interaction with which is a language in which still has been implemented and also has some advanced features such as fast and based feedback, which simply we didn't have to.
We some interesting gotchas that we have figured out or adopted from the published report, when engineering our tests and approach to seller language layout. So the first challenge that we have faced is how to generate interest in programs in the fragment of system and what makes system layout. So the first challenge that we have faced is how to generate interest in programs in the fragment of system and what makes system quantify polymorphic types, or this is not particularly difficult, we can do it in a recursive manner by first Remember in which, which type variable, we have in term, variable as free. Ands fundamental So that's relatively easy, somewhat more interested in is how to generate type application so this rule doesn't quite do justice to the problem.
So, I'm going to write at that rule. North so the main discovery we made to generate the type sigmas. And generate, tau, and tau prime replaces variable ex-, in the Ta, u and the x in retro specific . And so identify close symtack TAM ite idea is to take the type sigma philosophy and identify closed subtypes type variable so that as simple as it sounds, was just a driver or so and recording of types. Well, in reality, the algorithm was slightly more tricky, because we need to keep track of, of the type and also distribute frequencies so we wouldn't get very boring subtypes.
All the time. Okay, so that was one discovers the second one has to do with how we harness the language infrastructure already in place to facilitate the assessment, and for that. So the main motivation for that was to test a particular component of Silla compiler, which implements a control and type flow.
Remember in functional languages, the control flow analysis typically over approximates the flow of values to variables, was the main application of that being optimization function inlining and somewhat more vertical analysis approximates the set of ground types that flow to type variables, who is he main application of that being more physician, unsurprisingly to test these both we need some version state collection semantics.
Hat indeed flows to the type or value variables and primetime is correctly approximated by. The only problem that we didn't have the semantics of it. The only thing we have is our reference interpreter, and has been written in monadic , and how to abstracts the interpreter, and some of this affects, MOITH have corrected. And long story short. We only had the change the instances it. So it would report all the states. And this way, we have implemented someplace to he were P with you inn... so here to hear about the full box.
And this is was the box as of camera ready submission, which slightly increased since then.
There are some not worthy specimen here.
That happen to do with misalignment between the static and dynamic semantics of the primitivesprimitives. And have two box, that deal to denier of service.
One problem of interpreter itself. And mentions in the example, and finally, bug in the compiler, the counter flee left laneh made counterflow take pretty much forever so we had to do some contrast conservative projections that are going to be compiled you can also see that some of these bucks are really known so what we have done we have taken the previous commits, where this came from last reproduce before they got fixed, and we automatically generated programs just to make sure that our framework posts will be able to produce this box, but 7 out of 10 entirely new.
So, just as the last example.
So here's one of the box which in retrospect is extremely silly, and I wonder how the developers missed in the first place.
In the first place, but this is what is happening here. So here we have a type abstraction with two names of the same type variable used twice. And what happens is that the interpreter should really think that the type. The type of the variable we want is V prime.
In fact, it decides at runtime due to shadow and the type of we want is this We prime, which is closer in terms of scope. And variable A, 161648. Truth been detected. And I believe it this is the time I have left, to take away from the experience report, something might find useful, experience of... several credit... ish PSHL blockchain based on system using which as a tool and discovering several critical bugs. There is a simple technique of and sub-Constitution for generation the wealth out FF system.
Type instantiation. The real world usage of the technique of monadic interpreters for implementing collection semantics, that's static analysis. And if you want to see all that in action, we have an artifact Publishing's and others so feel free to download it and take a look. And this is all I have for today. Thank you so much. [APPLAUSE]
>> Quick questions before the break?
>> John.
>> That was very cool to see, can you just go back to the last bug you showed us?
This one
>> Yes. And what case like this, let A equal, it's equal.
I wonder if this is bug in simple lest form, if not, say how much you... the question is how do you imply... the fact he we see chain of... isn't in his is, this is a part of the language. So the example has already been shrunk.
>> So is the simplest possible form then.
>> I think so.
>> Another question?
>> Could you explain why you switched to property based testing, compared to formization that proveds one thing and forgel.
Ish infrastructure but it made possible for one of the earlier versions of Silla that evolved since then, to verify properties ... of smart contracts itself.
So we will never we never dare to go full blown concerts with Silla inside and we property based testing of the language infrastructure to be a viable, and the pay as you go along to the hardcore verification. In.
>> If you have any further questions, maybe it's a good idea to ask them during the break and thanks Ilya again.
[APPLAUSE]
>> And it's time for break.
