hank you, this is joint talk. And we're going to show how to calculate compilers with the help of monads. Xpress diverging behavior in our source language.
So that's the plan. So here's the setup that we have. So we have given the syntax and semantics of source language, and is he southeast Manitobaics given as interpreter, and here for very simple language.
And what we want to have is derive from that a compiler.
Systemically. And we also want to have assurance the pilar is correct. So also want to have a proof of it's correctness to exexpress some form of equation shown here. How are we going to do this. So the calculation approach just as my starting at the proof. So what we're going to do is I'm going to start proving this correctness theorem here before we even have the compiler. And as we do this proof, definition will fall out of the calculation.
This might seem like black magic, I hope by the end of the talk I convinced you this actually works out.
The nice thing about this is uses very SIFRP reasoning techniques only induction and equational reasoning which partly is one of it's problems here.
Because we only have induction it makes it difficult to account for nondetermination, so what we do here is use an monad to model the diverging behavior and use simulation for equality here.
So that's the plan, so what's the partiality monad. We have two cases now case, just going to return, and return value type A, and also defer computation with additional time step. And indefinitely, because it's a conductivity defined type, so that we get also divergent computations. And we give this operations and these indeed satisfy the monadic laws by similarity. So in principle two choicess bisimilarity. Weak kind and strong king. Let's look at weak version, Since the one that usually find in compiler verification light because it does not care about the number of steps on both sides,now 42 weak is similar to now. And generally expect in your compiler that thedoes not necessarily, necessarily take the same number of steps, as the source code. But unfortunately, it's no good for our purposes for for calculation, because we cannot reason both using transitivity and by coindeduction, f this makes it completely useless for for calculation. So let's look at bisimilarity. And so now 42 is not to later now 42. But turns out this is not a problem for calculation. Actually the calculation process will not only produce the compiler, that will also produce the semantics of our target language. And in particular enter just the right number of latent steps, so the two sides source language and target language will measure.
So we're going to go with strong bisimilarity here. And let's see by example how this works here. And again, the simple expression language from before, and now going to extend with silly loop construct just to make the language nontotal.
So we're going to extend the interpreter who just interprets loop S evalue loop, and just loop forever. And automate definition now total and partially monad, and code monad style, using return and do block here, and crucially, now this, this recursive call eval loop is now guarded by this later. And importantly so first of all, this makes this definition now total as far as the meta language is concerned, right so we can do this in coq no problem. Definition now mixes co recursion recursion. So in particular, we have here is to evaluate their recursive calls to structurally smaller x and y, arguments, and have here eval call here. And does not call on structurely smaller argument, but is guarded by later. This is well-defined semantics here.
So this is our semantics here on the left.
And now, as I said, we're going to derive this compiler here that turns our expression language to some code, and not sure what the code is derived by the calculation as will be the virtual machine, the semantics of the target language that takes a piece of code and will take a stack, which is just a list of integers and produce the stack that we have after we executed the code we have given.
So that's the setup we want to derive, and we also as I said, we want to have proof of it's correctness of the compiler.
Okay. So now the plan is to calculate this compiler using this specification we're using strong by similarity, to make it go through, we need to generalize it slightly and ask compiler to take continuation argument here.
So for given expression, continuation, produces code for the expression and followed by the code that we give here, and we accordingly generalizes, specification here right now says that if we evaluate the expression e stick the results on top of the stack, execute the continuation and let us the same as executing the compiled code on the right here. All right, and the procedure is as outlined before we prove this property, and we get the definition on the other end, that's the plan, let's do this for simple language. And this is the proof and specification. And go through induction, and coinjection, Right so this matches What we've seen earlier the semantics, which was recursive And we also got to make heavy use of the monad laws which hold up to bisimilarity.
And let's do by case of integers. And plan to start on left hand side of the equation, and work our way manipulating the term to the right hand side.
So on the left hand side specification. And transform step by step to the right hand side. I say right-hand side, but we don't know yet what the right hand side is because the compiler is not defined yet, but what we're going to do is find some C prime, some term of type code here, and then we just make the proof go through by force.
Just by defining the compiler to exactly return our C prime that we here calculated.
So this is where the definition of the compiler will indeed fall out of the calculation proof, that's what I meant earlier.
So that's the plan, let's go through step by step. First step we're going to apply the definition of interpreter, evaluate LN, and return N. And apply the monad laws. And substitute for V, and then stuck here, and want to get to form given down here, but we are up here. So the stack doesn't match up. So what this means essentially is we have to solve an equation. This one here.
Replicated here on the right.
And the way we're going to solve this equation, is again by force. By definition, we're going to introduce this as a definition, as a clause in the definition of our function exec and how do we do this?
We make sure all the variables on the right hand side, in particularly N and C are bound on the left hand side. And we're going to do this by introducing a new constructer of our code type. And takes these two arguments N and C so they're bound on the left-hand side. So just call push because that's what it does. It pushes the integer in top of the stack, and falls out of the calculation, and namely our goal to get to solve this equation here.
And not only that we introduced additional constructer in the code data type, and discovered instruction we need for our compiler.
And this completes the proof for this case.
All right, so now we can simply read off the definition of our compiler by just comparing this here to this up here, and so we have the compiler turns Val N, and continuation. And that's the trick we have here. And remaining two cases now, addition, again we start on the LEFD hand side of our equation we want to proof. And apply semantics of addition, and apply this out, and a ply monad laws and allow us to reorder the nested do block and get rid of return here, and now appear to be stuck because this does not match the left hand side of our left induction hypothesis, we may use, but trick see this as equation we need to solve, and again, solve namely by introducing new definition for the production machine, which puts in the right form to apply the induction hypothesis, so we do the induction hypothesis for X and Y and as it happens, we can again in the right shape and can apply the induction hypothesis for X, and complete the proof and in this case, can read off the definition for our compiler. And one last case we see coinduction, that's loop case, start on the left hand side, and apply the definition of our semantics and apply the definition of the bind operation that allows us to move this later here. Outside of the do block, and now we see that this term here, the left hand side of our coinduction hypothesis, which allow to apply because we're guarded by this later.
No problem can apply induction hypothesis. And now the target is to get a term of this shape here, and it's again equation we need to solve, which we again solve by definition, by defining the virile machine exec to be exactly in the right form to solve our problem here, and note it introduces the later here, right because the later appears up here.
And we can read of the definition of the compiler here which is now: Loop. The loop compiles to this loop code. And completes the calculation. We now derived compiler for the simple expression language, and derived the target code and derived the compiler, and derived the semantics of the target code. And most importantly, we also proved along the way it's actually the correct the compiler and indeed it was driving force for deriving in the first place, t this was the trick about this, but very simple language, toy language, and also applied to more interesting languages.
So we did it for a number of different lamda calculi, both in cabo value and name form. And lambda calculus, and did with language with loops and biloops all with the partialality monad.
And use the language with interrupts, and there we had to extend the partiality monad.
And all these calculations have been formalized in Agda.
All right few words about future work. A couple of things would like to do... extend to register machines, stack machines to register machines should be straightforward, I believe.
And this might be quite challenging, but I think the use of monads now allow us to consider more interesting monad we can use to structure our effects, so if you replace, for example the partiality monad with the interaction tree monad where you have additional algebraic effects I can imagine you can then peel off effects step by step doing the calculation, and get essentially multistage compiler.
So this is something that we're looking into at the moment.
And... with that I'll shut up and thank you very much.