>> Hey, everyone, it's a real pleasure to be here at ICFP. My name is Patrick Thomson.
>> And I'm apparently Rob Rix.
>> And we're here from GitHub, show of hands quick, who uses GitHub regularly. How many people sometimes use GitHub once or twice. Excellent, not a lot of source FUJ FUJ users here, glad we're in the safe place.
>> Patrick and I have known each other forever. And been fans of typed functioning programming, and Patrick one that taught me apolitictive, and devicing CFP in particular, for quite a long time so we personally are quite strongly motivated to be speaking to this audience, but additionally for from GitHub perspective. Well, there is a lot of code on GitHub, in fact that almost everybody here uses GitHub is probably an indicator of why that might be, given that fact, it'd be nice if we knew anything about it. So, who are you going to ask but folks who know something about programming languages.
>> PATRICK BAHR:
>> PATRICK THOMSON: So one of the tools that we've built to try and understand and comprehend the of code on GitHub is called semantic it's written in Haskell and it supports nine separate programming languages. We'll get into those later. And we built it to prototype and the power real actual honest to goodness GitHub products that actual people use, like for example the Table of Contents. Now sadly you can't actually find this one in the app anymore. If you're familiar with pull requests on GitHub. However, in the pull request or PR view used to be a way to go and find a little drop down that would show you a table of contents of what had changed in the file. Now you'll notice some of the entries here are in green summer and yellow.
Well we have inserted deleted changed and yellow files and symbols within those files to help you navigate your way around what might be quite a large pull request code review process. And this again, computed almost entirely with the guts of semantic.
>> And another product and perhaps our main semantic code teams main product offering is code navigation, originally this was prototyped in the Rubion rails monadlith, powers github.com via shelling out to the venerable C tags executives will, it may not shock you to learn that that approach doesn't really work at the scale that GitHub is at so we took semantic and hooked it up to an RPC framework servant with all it's fancy types and allowed people to navigate code, jump to definition and find all references on GitHub and the 9 different supported programming languages. Now, at this point, this code doesn't current the code navigation service is built on a DSL. This was not written this is not reflective necessarily of Haskell being the wrong tool for the job or semantic underperforming semantic did a killer job in practice. But what we want is community involvement. Semantic code team is small team, 7 people at max, and sometimes even 4 people. And we want language communities to maintain their own rules and heuristics for code navigation so we've implemented this as a DSL and we just passed the DSL off to the elixir programming language community who implemented, and now maintain their own code navigation rules on GitHub. We think that's pretty cool.
>> So, again, small team, big problem. Lot of languages we want to support, and supporting stuff across languages, and trying to find commonalty between them is an interesting challenge in and of itself, functional program something concerned solving large problems by breaking them down into the smaller problems and this is what we were hoping to do.
So with that in mind. Let's take a look at four case studies how that played out.
>> Before you do anything with any program text, you need to parse it. And the unpleasant truth is that in a lot of corners people consider parsing a solve problem. And in real world, there is just myriad approaches of parsing, Ruby uses parser and Python uses their own homegrown parsing parsing expression grammar framework, and most of the GCC projects parsers are actually hand written. I hope you enjoyed and maintaining a handwritten C++ parserrthat job must fall on somebody, I wish NEM the best of luck. In ideal world would use native tooling for every language. Obviously nothing understand the language better than the tools that underpin it and written by it's authors that's too operationally complex to fly in environment like GitHub, and can't maintain language stacks for languages.
We need some sort of interface, that let's us part whole variety of different languages in sufficient time and space at GitHub scale.
>> One of the ways we built on -- I guess that comes later.
So tree-sitter developed by one of our teammates, for integration into the atom code editor. And integrated successfully into atom but also into VS code into Neo vim I think Patrick mentioned emacs todaytoday. That's MRET cool. Let's you write grammar in DSL, in JavaScript that ends up looking a lot like parser combinators so quite familiar to t myself and many others in the functional programming community, and perhaps most importantly, it gives us a consistent API that we can use regardless of what language we're actually parsing.
And grammar taken and compiled down to C file, and compile and link against that and talk to that through the Haskell FFI.
>> Now tree sitter is based on the generallized left right algorithm developed by language in 74. Which interestingly enough took a decade plus to be implemented practiced. This came out of the natural language processing world, natural languagings, as we all know are riddleds with ambiguity, and possibly even nondeterministic interpretations, and these techniques were developed to have a framework, and algorithm that is capable of unction even the most byzantine and COMPLEGS grammars, and speaking of byzantine and complex grammars ruby is measuring stick for something capable of handling real world language, Ruby though on the surface may seem very beautiful syntactic language is fiendishly complicated, but we're happy to report that tree sitter, combined with a custom lexer.
That's a fairly small c program is capable f handling all the nine languages that we've chosen to arse that the generated C code is fast enough to be integrated into a high performance service you know it's servicing the entirety of GitHub user base.
>> As a quick sidenote Ruby is also the most complex in every other way in addition to syntax that's true that language source of many joys and many pains. Anyway, so as the second case I mentioned earlier that the complexity of trying to do all the various things we're doing across nine different languages. Led us to want to consider how we can factor that work out. For example, to, printing for one example of integers, integer literals, the same way regardless of whether it's in Ruby or JavaScript or go or, etc. That isn't necessarily going to give you source level printing but way to share some of that structure that work.
Trying to produce ultimately M features across by N languages.
Up from the huge mbnpolynomial down to simple m + n line, that's what we want to be looking at. One attempt was to factor out bits of syntax to individual datatypes we then combine into the ala carte style sum type. And here have binary sum, and principle take the union of all the different shapes of syntax, and gives us injection into it and KRUSHly safe projection back out, and ompatible with working with recursion schemes, you could throw fix or free around this and have something that you know how to fold over. .
>> It's also worth mentioning here, with the little language, we include parse error case, case, and that's crucial because at GitHub, we can't assume that all the code in a repository is well formed, maybe somebody made a syntax error somewhere, or maybe introduce new language syntax or haven't integrated properly into the parser yet.
Tree sitter is capable of best effort parts of possibly mall formed syntax three, threout able to... the property and make visible in our syntax trees.
This is a classic paper. And I can't recommend reading it highly enough, if you haven't yet. But it has problem in that the standard encoding of the ala carte of injection, and subsumption relation is expressed with recursive overlapping type classes. And if all this works for your smaller languages with maybe 20 or 30 different syntax types.
Languages complexes TypeScript has over and I'm not getting 140 different distinct syntax notes and when you feed a GHG type level list containing 140 different items. Tends to be unhappy, so the solution was to do some loop unrolling in the type system, which sounds mad inducing, but a bit of throw back to 70s with C programmers and dust device.
[Laughter] >> I heard some knowing laughs there.
When that's unrolled with some template Haskell that would make your eyes bleed. It worked fast enough and took the approach and published on library on Hackage called fast some and open source, and people out there using it we're using it.
>> So the way this all started, in fact, the reason that I work for GitHub period was because I used to work on a diff tool called Kaleidoscope well now I work at GitHub so how can we do diffing differently. If you are not familiar with the product, GitHub is code hosting and collaboration for communities and users of arbitrary size from small teams to quite large companies, or external company teams. Communities rather, and one of the key atoms of the UI is changes to code. You made some change and suggesting I adopt that change in my projects, I want to look through and review that, and have some discussion with you why it's for, and why you made the choice, and ask you to change something perhaps. And hopefully make improvement to it. That everyone gets to benefit from.
However, diffes are limited.
Mostly we look at textual diffes if we're looking at textual diffs, that's fine with lines of text, usually not, usually the structure we're trying to compare and much richer than that. And to that end, we developed algorithms doing syntax aware diffing, and these were are is it refined somewhat, and conversation with folks in this community. And victor Meraldo in particular, and some of the resulting comparisons we got were kind of cool.
If you move something in textual dif deleting lines and inserting lines, but no notional comparison or relationship between the two, or for example, if you are doing the thing where you insert a function, with curly braces, one of those languages uses those. And it's shown it as the first line within the braces down to the last line inserted. I forget the right structure of this.
But basically gets the braces in the wrong spot simply because the algorithm happened to end on 1 or 0 or 0 or 1. Frustrating way to look at program and understand the changes.
So we developed this, and works really quite well as far as we're concerned. But we found it difficult trying to build good products on top of this.
Our first attempt was a notion of summery which was to try and compute human readable summery of what changed within a given diff. This turns out to be terrible if you do it the naive way that we approach it because what ends up happening is you say, deleted three, inserted two or replace, replace three with two this is not going to be an improvement on just looking at you know, minus three plus 2 in the diff so that particular product feature didn't sail.
But as a whole capability, we're pretty happy with structural diffing and.
>> What fell out of first approaches with diffing, was really gratifying, which is the utility of recursion schemes in a real world context diffing is the is the process of comparing chunk by chunk syntax trees and that's recursive operations over syntax trees and turns out in practice most things you want to do with syntax tree can be expressed as syntax stream.
They've turned out to be a magnificently useful tool in our toolbox. As programmers, it's they're compatible with our ala carte approach. You can see on the right here is very simple recursion scheme over the language we showed earlier. And the application of the catamorphism there. And cata, aka, the standard fold is fairly innocuous even the more exotic recursion seems, history morphisms came in handy when we're doing did we do Zygo morphismss.
>> The pre formorphisms.
>> And finally the perhaps most exciting to me personally application trying to do here is program analysis. So we have all the source code in the universe verse and really KOOL to know anything about it So therefore, we look to the literature again. For ways that we could try to analyze and understand, answer questions about our programs, our users programs, abstract interpretation was a natural starting place just because it's so general and so powerful. But in fact it's same property that is kept us there. This is really open ended for us.
This is a really considering question offer research, we don't know what questions are important to ask. We don't have a way to engage questions are going to be important to ask next week.
So if we make ourselves into something that's a little too tightly focused on one particular kind of analysis that's, that's going to be a shame, we're going to we're going to regret that. So we built on abstracting definitional interpreters. If you haven't read this paper by the way it's one of my all time favourites and always will be.
This is on going basis the primary focus on semantic and my personal joy, and my personal focus in my work where ever possible. It did pose some interesting problems though.
For example, it wasn't exactly obvious how to take the approach as described as implemented in the paper, and implement that in Haskell with the tools we have commonly available to us, for example MTL.
>> In deed the paper in the abstract definition interpreter paper artifacts are implemented in Monk style but not in Haskell. Implement racket. To overcome the, the problems that a traditional NTL focused view of the world. The problems and traditional NTL view of the world encounters when implementing Adi. We turned again, the literature on the papers of our co authors, Nick and Tom and Rob went into the shed with a great deal of clanging and bashing emerged two weeks later, slightly blood idea but with an effect system, a practical real world effect system and performance enough for the GitHub use case that allows having multiple state types are multiple return types in the same computation hich is not possible with the standard MTL approach, and that allows one of the state it types to have a differing interpretation from the other which is really crucial and non determinism enters the picture and you want one state to have all possible path reflected changes in it.
And another state you only want winning path propagated. And there are people out there building businesseses and websites which is humbling and honoring, it's got more than 10,000 downloads at this point on GitHub. That's pretty cool.
>> On hacket.
>> Oh, goodness, yes.
>> And as quick extra note about fused effects great ICFP story, is it started it's first commit occurred on the steps of the building outside... can't remember which building it was in St. Louis.
>> 2018.
>> Yeah, 2018 and.
>> Immediately postconversation with Nick. So thank you Nick.
>> So in conclusion we have done some things, and some of those things were hard for some reasons. And because of those reasons, we applied some other things, and things we applied, would not have been available without the work of ICFP, and the ICFP community, functional programming community, and academic community, and the research oriented industry programming community as well.
So I personally would like to say incredibly thankful to everybody for your work.
>> You all rock.
>> However in the grand tradition of people with mixed news, I'm going to break the bad news first, some techniques worked BUFly the first time, and some encountereded some ups and downs, the traditional encoding of ala carte syntax that we showed earlier, is very cool ery fluent, to express in Haskell, but there's one large drawback and that you must go, units give out a little bit of type safety, because the sub terms of when we express yntax with func d the sub terms are of the type variable associated with that functor so in the standard approach you're not able to say Alright, in this particular term we know will be one of these 3 types, really could be anything, there are approaches with Patrick, with the comp data library, that work around this, you do pay a pretty heavy cost in terms of complexity. And what we actually realized was that our hypothesis that we get good code resues handwriting and reusing data types. The degree of code reuse there, is less than the code reuse, we did when he started generating, Haskell syntax types directly from the grammar descriptions and this was a really interesting result and that suggests to me that the higher level things that you can reuse, even if they lead to proliferation or perhaps a word of data types that gets you more compelling code reuse. With large Haskell, become a problem. We've used the basil build system to work around those editor tooling. Nowadays, the Haskell language server is in a really terrific place that back in 2018 it was it shakier, especially with large template Haskell slices prices that we have something we do run into is that generic programming in Haskell can be very difficult with the standard GHC generics library you need to the programmer needs to have a scenario with type families with poly kind of types and taking a novice Haskell programmer and sitting them down in front of that interface is a little bit difficult. We also had a brief.
Experiment with type file paths to catch any errors we might be making when we tried to append a file to a file. A file to a path whereace allowing pending file path to directory, this did point out places we were having creative semantics, but didn't really catch any critical bugs, and posed a good deal of computation complexity and reading complexity on opof that.
>> In contrast, at one point in time, any push to GitHub of any code, where at least one of the files was in one of the 9 languages we mentioned, or showed anyway, went through HAFK Haskell code. That's pretty damn cool. That seems to me to be a pretty tremendous win and we're talking about a lot a lot of runs a lot instantiations of the process so pretty fantastic . Tree sitter, ikewise became foundational not just for us, but for anybody doing parsing for developer productivity in any way, shape, or form. It's really been tremendous to see tree sitter take off the community that's grown around it has just blossomed. Likewise recursion schemes folds, who doesn't love a fold Come on, full degree. And it doesn't matter, matter if, if you call them folds or catamorphisms still beautiful.
Algebraic effects like why does I will never not love algebraic effects as well, the tremendous, to be able to decompose programs into DSL is like this, and it's pretty neat, also that we were able to find several places in which we could composed part of our system into an open source package that other people could use and sometimes even did.
>> Fantastic brief anecdote, that I wanted to tell about our scanning requirements and operability, that our deployment of production, never actually hit any crashes or errors, except for one. When something deep inside the GHC event loop croaked and died, and submitted on the GHC bug tracker and got reply back in couple of hours, and said, are you running Dell servers and no way, and I get on the horn and find out what brand of servers, they were Dell servers and configuration of BIO is that made Linux is E poll fall down in a given critical situation. It's horrifying at school. But most of all it's not our fault, and that's just terrific.