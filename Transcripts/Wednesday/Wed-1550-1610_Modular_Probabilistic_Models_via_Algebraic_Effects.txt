>> Hi, so I'm going to begin with a disclaimer, is that this talk isn't really going to be about algebraic effects, but about something at least as cool about probabilistic models. So the best place to start is tell you what a probabilistic model is. I find it help to view it as set of relationship between certain random variables; that is the inputs and outputs of a model. And some parameters that describe how they are related.
For example, linear regression, which models, a straight line relationship between inputs X and output Y. Given X, We first declare how the slope intercept and noise around the line are distributed. And then the final line, so it's about our output is generated according to the normal distribution using these as parameters. To implement and arrest act with models as regular programmes, We have these things called probabilistic programming languages, or ppl. So what might linear regression look like as a program. That's the thing, in a lot of PPL depends on how you want to use the model. In monad bayes one way is to simulate from it.
And for this we provide an input some fix model parameters, and we then sample the output, according to its distribution.
On the other hand if we already new the input and outputs, we could be interested in inferring what the program FERs are. So in this instead we sample each of our parameters and this line says that we condition against the likelihood that our parameters have indeed generatered our output. So I had to define two different programs for the same model. If you look at other PPLs you will encounter the same thing, the main difference is whether we sample for a random variable or whether we observe.
And this then effects what we consider the inputs of our programme and outputs. But the number of ways we can interpret a model isn't just limited to simulation and inference. For example, maybe I want to explore the behavior of certain variables by isolating which ones we sample from.
Or, if your training data was incomplete we could first want to simulate some of the data from the model, and then loop it back in to do inference on the same model.
And so, this gives rise to a frustration is that: Every time we want to interact with a model in a new way we need to define a new programme. This motivates the first topic of our reef --
research, ises, which is that we want to define, just one model, and then defer until later, wherever we sample or observe from its random variables.
I can find an existing term for this. So, an exciting point in my research l I got to coin a new phrase, which apparently is already taken. But these a called multimodal models, and, in Haskell called prop effects, which uses algebraic effects to support multi modal models, and that's the only time you'll hear me say algebraic effects in this presentation. So let's take linear regression again, on the riding hand side this is what might look like in programme effects. Tightly correspond to the notation on the left. And notice that we don't have any sample or observe operations. In fact this representation is entirely syntactic all we're saying is mu is distributed according to the normal distribution and nothing else.
An important feature of the language are these hash variables here. And we call them observable variables. They indicate as a user you are able to later provide observations when executing the model.
They directly correspond to these type-level streams up here in the observables constraint, and these constraint say mu, c, and sigma and y are observable variables type double, and may been provided in observations in something called model environment. So will become clearer interact with the model.
So let's do simulation first.
Here are declare a list of inputs from zero to 100. And then I'll assign values3, 0, 1 to our parameters and not to our output, and this he is expresses to observe our parameters but sample for our output, if we sent simulate under the environment this generates model output as seen in this graph, and conversely can do inference by specifying different environment.
So with this we sample parameters providing no values and observe against outputs we can just loop in back values from simulation.
So you might have noticed I assigned list of values for each observable variable. And this allows them to have multiple dynamic instances. So every time hash Y is referred to at runtime, what happens is first Val from the list is read, and permanently consumed. If we Len run likelihood weighting inference, each iteration will produce weighted out put environmenters, and an output environment has exactly the same structure as an input environment. The only difference being, it now value sampled during model execution. So if you are interested in Val sample from mu, you could map forget function, and sorting values along side the likelihood give this graph here, you see values around Mu = 3, having a higher like LOOSHGhood, which is consistent with what we originally provided.
Okay, so this underlies for basic ideas of what you could do with mostly modality. But at this point, I should note that there are already a handful of great languages that support this, such as Gen Turing, and Stan.
But in these languages. This is typically done by having models exist, as some form of special language constructs and these constructs normally can't be manipulated as first class values, or when they get close to it, we still don't any static type guarantees about our model.
So we can do things like referred to random variables, but I don't actually exist. .
So... this makes it difficult to do two things in particular.
The first is to define models in terms of other models, and the second is to treat models in higher order way. By embedding into Haskell models are not only first class values, but also type safe which opens the door thinking about models compositionally, and to show you why this way of thinking can be really pragmatic, a cool example is an HMM or hidden Markov model. The idea is series of latent states X which we don't know anything about. But we doe know they're related in some way to set of observations, y. Our goal is to try and learn about X, given Y.
So if we were thinking about this compositionally, big brain, then we can see HMM as being decomposed into the two submodels. And first transition model, tells us how X transitions to the next sun, and second is an observation model, which tells us how to project an observation from the transition from a latent state. We can then define the HMM as a higher order function, which is parameterized by both of these. O do this, we just define the behavior of a single node. This just performs one transition and projects out an observation. And then the last line creates a chain of end of these nodes, replicate and folds over them with... and what this does is propagate each nodes output to the next node along in the chain.
And this is it. I think this is quite an elegant abstraction but we can take modularity even further. So for this I'm going to wrap up by showing you a really cool real world application of HMMs. The spread of disease during an epidemic.
This particular example is called the SIR model. The idea is that we assume a fixed population partitions into three groups. Those susceptible to disease and those who become infected and those who recovered. Given the outbreak of a new disease, the SR model is interested in tracking how these values vary over a series of days. So there are large number of reasons it's infeasible to know these true values especially for a large population, so for these you can view these as latent states of HMM but do know there are number of reported infection cases.
Defining the rest of a model, it's then a matter of just defining the transition and observation model. So, let's do the observation model first.
This is going to be quite straightforward. We're going to assume that the number of reported infections depends only on the number of true infected individuals I.
And use report on distribution, to say that on average a fraction row will reports for infections.
The transition model is a bit more interesting, because now we can show off some composition. So, will first say, but the number of susceptible individuals who become infected is given on delta SI, and this depends on the contact rate betta between them will then appropriately, subtract this away from us, and add it to I, follow a really similar pattern when modelling the transition from infected to recovered. And what's neat is that we can then compose these two models to give an overall transition, taking us from Si to I, and I to R. Okay, so we have our observation and our transition models. If you recall, the higher order function I defined before the final SAR model is simply this, we jjust pass it off to sub models. And this is great because we can now simulate the end of the world. Assuming the world has a fixed social population. So we'll give it an initial latent state of one patient 0. And everyone else is This shows the number of sue susceptible, and recovered over a series of 100 days. But the story doesn't end there. The setting of composition makes it straightforward to incrementally extend our model. Which can be really useful on trying to strike a balance on how we should model a certain problem.
And at what point is the model too DMREKS to being useful any more. Let's say we realize that people can be resue susceptible to the disease again. We can just implement this as a new transition model very simulated beforebefore. Suppose this. And this gives the graph up here. And let's say a vaccine is introduced. We can implement this as new type of population, and create new transition submodel. And compose it, and this gives this graph.
And so, reasoning about models compositionally like this is quite uncommon from what I seen, and I tend to see models and PPLs written monolithicy and from scratch. And from talking a few statisticians, I gathered anecdotally this is partially for historic reasonses, one being that when we formalize models. We care about things being self contained.
And so what might happen is that these models are just translated from this mathematically style presentation to a program. But I'm sure you are all aware the needs of programming is very different, and probabilistic programming languages, I think this translates to the need to reuse and maintain existing models.
What I'm hoping is that the ideas behind this language, or even just by taking functional programming approach can help support these needs. Yep, thank you so much for listening to me.