>> KC SIVARAMAKRISHNAN: So in the wonderful PLDI keynote earlier this year. Gave the wonderful Simpson team talk, and joked about making talk with AI generated images.
So OCaml 5.0l is the next major elease of OCaml programming language, and brings in native support for concurrency and parallelism into OCaml. By concurrency we mean overlap execution of tasks in time. And we define parallelism as simultaneous execution of tasks, overlap in time. So we have the separation between concurrency and parallelism in OCaml five, and we provide distinct mechanisms for expressing concurrency and parallelism or concurrency we express them through effect handlers, as Sam mentioned and in this talk talk about how went from OCaml 4, to OCaml 5. With the OCaml project I had led. This journey wouldn't be fun if there wasn't any adventure and had our fair share of adversity, and what I want to tell you is the challenges we faced to retrofit concurrency and parallelism to industrial strength language of OCaml and how that shaped our approach. And talk about take aways you can take away from the experience, if you are designing your own programming language abstractions, and brave/foolish enough to retrofit them. So I split the talk into the two.
First going to talk about the journey of OCaml and then talk about the takeaways. So let's go to the year 2014. OCaml was an 18 year old industrial strength functional programming language, reasonably popular in the industry and academia.
OCaml was particularly favoured for low latency net work systems, and analysis tools compilers proof assistants and whatnot. But when compared to systems programming languages that were out there. It was one of the few that did not have multicore support.
To be clear OCaml had threads, but when you have multiple threads, only one of them can execute OCaml at any point, essentially had big runtime lock around the heat. And others could be executing C code in parallel. So this is no different than python that has noo... so eliminate the runtime lock, and allow OCaml threads to run at the same time. So Sam Gross one of the developers as pytorch, working on a project with similar goals for Python, which is to remove the global interpreter lock. So in this undertaking we had a number of challenges. So the first challenge is that OCaml was heavily used by the time we started this project, millions of OCaml code in production, and none of it written with concurrency and parallelism in mind. And even after we add concurrency and parallelism in OCaml most of the code remains sequential. And developer maintaining 1000s of lines of OCaml, and came to you and said you can have concurrency and parallelism but have you to modify all the code. This is not going to fly. So the first requirement we had for the project, we were not going to break existing code.
So OCaml is particularly favored for low latency GC and having predictable performance, if you have application that can tolerate 10 millisecond latency OCaml is great language for you.
In designing the extension we want to ensure we preserve the GC latency before focusing on scalability, and is finish finally OCaml core team and composed of volunteers, so in building the multicore OCaml project, did not want to write 1000s of lines of code and throw it against the wall and disappear. So essentially wanted to ensure the burden on OCaml developers remains small.
So one of The things we wanted to do is not have separate sequencer and panel runtimes, for OCaml. And this is different than what is done with runtime parallel execution. PS a direct consequence of this, we had to ensure that our existing sequential programs that ran on OCaml 4. Ith the same time and using just the same amount of memory on this new runtime, right, this is a tall order, and this is what we set ourselves primary work and designing, like a parallel language, which is a high level languageyou want parallel allocator and garbage collector. Particularly not normal ensure allocations and garbage collections can be low latency, so in this design our unit of parallelism which is domains each had it's own miner key, and object call indicated.
And then when the minor heap is done every survivor is promoted to the single Shared Measures.
And we had some restrictions between the pointers between minor or major heaps, we allowed pointers between minor and major heaps. But did not allow pointers behind miner heaps originally. So in the design, if one domain would access object under the domain major heap. It would do RPC call, remote domain to promote the object, the target domain and promote that object and respond to the call, with the new location of the object in major heap. So the design is nice, and has nice properties. In particular could independently garbage collect each of the miner heaps without having to stop all the domains. Major garbage collector was concurrent mark sweep collector. Which had very short stoppable sections. And the idea is that this particular design ensures that the GC latency remains low.
And you can scale with the additional domains, so fairly quickly able to bring one up and spoke about the experience in OCaml 2014. But actually building upon large sequence of prior arc. We didn't want to go for novelty. Wanted to go for something that just worked. And so, Simon, M and Simon P, had a paper in ISMM11.
Key And these could be independently garbage collected.
But this particular design did not make it into the mainline because at that time. This particular GC could not compete with what was back then. During my Ph.D. multipair LEM extension of the standard ML language.
So one of the nice things that we had done exploiting this invariance between the heap is that you could take unmodified standard about programs, and run it on non cache coherent Intel single chip computer. This was quite nice, recently at CMU had large series of work in the M language, and with the SPIELer.
The idea there arrange the heap in hierarchy, in the joint hierarchy in the program, and only cross heap pointers allowed, are ancestors to no pointing between siblings, or to descendants are the descendants of siblings. So a heap that has this property is set to be disentangled. .
>> Earlier this week, Sam had a very fast check for en TANGlement detection, and Sam giving keynote at ML workshop on escapable function and programming through disentanglement and quite excited to hear the talk. Given that we're building on good tradition of building on these, we saw excellent scalability, and maintains low latency, and importantly this particularly GC if you only had one domain only matters what OCaml had today in 4. So latency throughput and memory usage characteristics of OCaml sequential programs. So we are all very happy, we thought we had something done and we can upstream it. .
Disaster. So in order to enforce the invariance, had to introduce 3 barriers to the language, OCaml did not have reed barriers. Read barriers are essentially some check you introduce just before you access any OCaml object.
On the OCaml site this is easy.
This is probably single instructions or two instructions that branch predictor always predicts, so the impact was very low. But in your design GC read barriers also GC safe points where the GC can run, OCaml has wonderful CFFI where they can access OCaml heap. And written in such a way it assumes where GC can run, and in particular GC's could not run during an object read, but now they can due to the read barrier design.
As offshoot of this what happened every program that uses CFFI had to be rewritten, and no push code fix for this. And OCaml compilers uses CFI for implementing a lot of these features, and we have to struggle to actually get this feature right. We could see this would not be a scaleable solution because every sequential library that uses C FFI needed to be modified and no static check can help you here so we had to go back to the drawing board. And f invariant could be no cross miner key pointers. Because there are no invariants to enforce, we did not read a new read barrier. We also did notbreak C FFI now.
Because ... so our initial worry was that when we increased the number of domains, bring all domains to stop would be very expensive. That was the motivation to design where you independently garbage collect each of the domains. Turns out you can bring all the domains to stop fairly quickly because of how freak consequent miner allocations are in OCaml, and every miner allocation involves check you still have space in minor heap and can make fail, uninterrupted domain and bring all the domains to a stop.
So our original very was unfounded, and we actually found that this design work much better than earlier design which require an RPC We spoke about these designs and ENS extended to cover, programming language... and finalizers and so on. And extensive evaluation in ICFP 2020 paper. The other major problem one has to think about introducing parallelism into the language is the problem of data races. When two threads perform unsynthesize access, to shared memory location, and one of these is right, said to be data base in the program, because of compiler optimizations and hardware optimizations, you may observe behaviors which you cannot simply explain as interleaving of operations from different rates. These programs are set to exhibit non sequentially consistent behaviors.
One thing we attempted to do was to enforce sequential consistency to OCaml. Would be the cost of saying, OCaml, ill always have sequential consistency, even in the presence of databases, which turned out to be quite expensive for relaxed architectures in our measurements. It was about 2x slower for sequential programs running on this new runtime, because we wanted to enforce se.
There was plenty of language memory models we could learn from.
And so if you look at languages like C + + and swift they have very nice property called DRF-SC. If your program does not have data races only exhibit sequentially consistent behaviors. Right. But unfortunately these languages do not define the semantics for in the presence of databases, so they may do anything, so they may also catch fire. .
But OCaml programs can not crash. So this particularly memory model was not something we could adapt. . But Java memory model has this limitation that it when you have a database, and you know that there is some point in the execution where you know the database is done, you may still have the effect of the data is beyond this point. So we defined this property as data is not being scoped and time.
Essentially, you can't do model reasoning, in the presence of databases in Java. On the other hand if you look at language like Rust, no data base by construction by extending the type system, did not want to do that at that stage, because we wanted all the existing programs to work.
There is some future work here, but we chose not to do that at that point. One advantage we had with OCaml, there is no OCaml programs in the wild, so unlike C++ and java, have these expectations of what the paddle performance should be. And you have these memory models people trying to sort of wrestle with the people to say, hey, can we add more safety here, so we didn't have to worry about all of that we had free reign. We ended up designing very simple comprehensive operation memory model. Only has atomic and nonatomic locations and DRF-SC, programs remain sequentially consistent.
We also do not have out of thin air values, even in the presence of databases, memory safety is ensured.
And we do not have any relaxed axes. So the model does not have relaxed axis. So our thinking was that if someone really wanted to extract that last percentage of performance from the hardware, they would implement that particular module and say C++ or rust OCaml wrapper and be done with it, so vast majority of the students can still remain in OCaml.
So recent update of the memory model. Version 1.19 release in August attempts to go for this model. But with GO, reasonable things like accessing interface Val strings and slices involve accessing multiple words and non atomically And hence they have captured semantics. Right, so it's getting there. So the key innovation of the model property of local data race freedom, compositional reasoning in the presence of databases, so you could essentially prove, some property about your library. And this property would not be broken.
Royal linking with a code that has a database, and the performance impact on memory model on sequential programs, it was free on X68 and had less than 1% performance impact. And so formal model. And memory model. And competition to x86 -- we believe if you have high level language, and want to include relax memory model you may start from the OCaml model than say C++.
So we always knew Parallelism was performance hack, it's a resource concurrency is a language.
It's a programming abstraction, right. So, we always knew that we wanted language level threads in multicore OCaml. OCaml does have support for concurrent programming through language level -- these are monadic concurrency libraries.
So one of the properties that these were already concurrency libraries have any that they split the world into asynchronous and synchronous functions. So asynchronous functions can normally call synchronous functions. The converse is not true. You have to have some sort of special calling convention to go to the other side. And this was succinctly captured by the essay Bob N, called What color is your function. So many languages, including Java Script and trust have this particular issue. So our idea was to eliminate function colors with native concurrency support. So essentially you can implement language in two ways, right, you can say my threads my schedulers, and the synchronization mechanisms are all implemented directly on the runtime. Or can say my language exposes low level mechanism on top of which can implement all these concurrency libraries as a library, right.
So Go, implement schedulers runtime system, and con currency mechanisms, and channels implemented in the runtime.
Few downsides to this approach.
For example, these mechanisms implemented in the language of the runtime which is C, but not Haskell. These are complicated code want to write all in Haskell. And lack of flexibility, you can not easily change the runtime system to change the policy and maintenance burden false on the compiler developers to maintain the complex code. So back in 2012 spent summer at MSR Cambridge and tried to lift the concurrency substrate from the GHC from the runtime and implement directly into the language. And partly successful and ended up with one particular Deadlock. The crux of the issue the thread scheduler and GHC interacts nontrivially with lots of other systems in the runtime system, including the lazy evaluation mechanism called Black holing. So what blackballing tries to avoid is that if two threads try to force the same thunk, you want to avoid the competition, so you will pass one of the threads on this and wake it up when the evaluation is done. Requires access to the scheduler, but now at schedulers themselves are written in Haskell, which may themselves required to be suspended on a black hole. So we had the cyclic dependence, and there are probably ways around it, but we ran out of time. For me personally the take away, once you add a feature into the runtime system it's very hard to undo it. So when we started designer OCaml we knew we're not going to implement all these mechanisms in the runtime system. So we wanted to implement everything into the library, so obviously have to have some first class continuations. The question is which kind. So Carl B, and Kent, D, how to represent one shot continuation sufficiently scheme. And they also showed that you can express all sorts of concurrency mechanisms just using one shot continuations. In multi Melton we had implemented an asynchronous and parallel extension of concurrent ml using one shot continuations in Milton. Occasionally have told us that calls, and delimited continuations which are exposed to a call CC or bad abstraction And on delimited continuations do not occur in practice, right.
So, whenever you try to use undelivered continuations. ...
not for combination but for writing these concurrency libraries you eventually end up having to do a hack to introduce the delimiter. So the take away there was we needed some form of delimited continuations. So at this point, they showed us wonderful Eff language that has functional program language called effect handlers, effect handlers or this module and mechanism for programming with user defined effects.
Operationally, for us it was, it gave us a structured form of programming the delimited continuations. With effect handlers, you can encode all sorts of computational effects, including concurrency, which is what we were interested in. So one of the nice things I found with effect handlers they're much easier to KOFRN henned compared to other delimited control operators, don't want to focus too much on the code, but on the left here, OCaml with exceptions and on the right effect handlers if you ignore the semantic for the moment syntactically very similar and way I explain, operationally very similar to resumable exception where the computation would be resume later, the other reason tend to be easier than other control operators, is that you don't have to carry around this prompt ordeal with answer type polymorphism if you know what this mean. Andre put it...
We took effect handlers as mechanism to expose one shot delimited continuations in OCaml, spoke about the experience in 2015 workshop.
Since then, there has been an enormous amount of work both on the type system side of epic standards as well. As the implementation side. We've also had an impact on industry, so soon afterrepeated the talk, was then Facebook to the React team. For those who don't know, React is this world's most popular UI framework that is used by one and two websites in the world, and different panels I've had a direct impact on a feature in React called react hooks. This is due to no fault of mine.
But the idea here is that yeah cooks or mechanism to manage state and effects within react APS, and nice to see impact in the real world. So unlike research language like NF. They don't come with effect system, we don't track... so they are much more... in fact we don't even have dedicated syntax for effect handlers, and we just expose them as functions and idea that I want to provide room for syntaxic innovation in future implementation of effect handlers for the OCaml that comes within the system, focus to say I'm going to add effect handlers into the language, and preserve the performance of legacy code. And I want to ensure that debugging and profiling tools continue to work. When we add effect unless the language. Spoken about this paper in the 2021 paper and focus on particularly backwards compatibility, and compatibility with tooling. And so gone ahead and instrumented the highperformance direct style asynchronous IO library called the IO based on effect handlers and competes with state of the art, and go and rust. We can do better, but really personally, if I can write my high performance code,OCaml and written within... and I would take it. And language level threats language, thread s taking come back, project loom in open JDK that continuation to the JVM and work on adding type continuations and model concurrency in Bassam and exciting development is first class delimited continuation merged two days ago. And can back and attempt to implement control concurrent on top of this.
Okay, so what are the take aways from this multi -- OCaml experience, if you want to design program language abSTRASHGS and to some industrial strength language.
So firstly care for users. They have limited time and patience, especially if they are not going to use the new feature adding to the language. So even after the concurrency to the language. So the transition should be no-op, or have push button solution.
For us it was not push button solution, we did have to break features at the edges of the language.
So one thing we did was build this tool called OPAM health check. OPAM is package manager for OCaml, and include all the package release for OCaml.
Every week it goes through the universe of open package on different compilers and here showing with latest release compiler and OCaml 5.0l who's alpha release is out now, every green box indicated that package, and little box says that package, dependancy of that package fails to build, and red box says that particular package failing, and published online, and as package developer you can see is my package broken and get a log and go fix it. And team has been proactive in going ahead and fixing many of the broken packages. FF if you are programming abstraction, you should rigorously continuously benchmark your change on real programs. They don't just run synthetic benchmarks they run real programs. And to build reality into experience, we built tool sandmark, and OCaml benchmarks, So we pick carryover programs on the wild including CoQ proof assistant Alter Ego solver Irmin data base, which will send a large set of welcome dependencies as well. We have around 50 of these, and we regularly venture on them on these benchmarks, and all of our results are based on these results.
I suppose I have a program. And ran for 4.14, ran in 19 seconds, And on account five, it is 18 seconds. You have to ask yourself now, right, or the speed up slowdowns statistically significant. So did modern operating systems architecture and micro architectural introduce a lot of noise at very small scales.
As an anecdote While we were trying to benchmark our measured the overheads of memory model by inserting fences, we observed that I could insert fences into my sequential program and make it grow 20% faster.
I only extract load on them. So then we replace those fences with no ops off the it was still 20% faster and ran the program on different intel machine and different generation, and the speed was not any more there.
Modern processers do a lot of magic underneath and can't see what this magic is. So our approach here is essentially tune the machines to remove the noise. We also observe that we can report instructions retired along with realtime in order to interpret the meaning of whether a particular speedup or slow down is just due to noise or real change.
So we spoke about this particular experience in OCaml 2019. There is accompanying write up I linked here shows how to tune Linux machine for benchmarking and also this wonderful paper by E B, 2013.
Built a system called stabilizer idea to remove effects of memory layout to the program and is provide number of statistical tools in order to do sound performance evaluation, if you care about performance you should read this paper. So all of this is quite hard to set up this machine for tune machine, and have to run all these benchmark and is need a lot of compute, and we released sandmark as a service, as continuous benchmarking service.
As a developer you can take the feature branch and point this at the service and run all the benchmarks on multiple tune machines and produce results in nice UI and interactively explore the results. And this has been regularly used by OCaml developers every day.
So the next thing I want to emphasize is that you should invest in tooling. And should learn how to use tools, and if tools don't exist you should build them.
Much of my time during the earlier years of hacking on OCaml was debugging segmentation faults, so have program with 17 threads that fail once in 100 run reliably. And the point of failure, the crash is far removed from the root cause of the bug.
Even if you manage to capture the failure in GDB there is no way to make progress, it's some crash. Right. So this is where tool RR record and replay debugger on top of GDB comes in very handy, and capture the stress and go from the point of failure backwards and keep asking questions backwards until you reach the actual root cause of the failure. RR is so good if we manage to capture failing trace once, we guaranteed we fix that bug. If you are using to use anything, you should switch to RR. Second tool we built upon is tool called thread sanitizer, part of project.
Thread sanitizer dynamically detect data bases used for C++, go, swift and other languages and use thread sanitizer if OCaml for detecting data bases implementing efficient data base detector is a lot of work and building on what is out there and quite handy for us, other thing we put a lot of work is tracing. Our initial tracing tool was hacked up version of GHC straight scope. It's wonderful. This is trace we annotated issues we had found.
So the visual debugging of performance problems, is great when you have tracing infrastructure. If you expect there is a problem and now look at a trace the problem becomes obvious, here we're looking at something and say why is there such a long minor slice. And this would be very hard to do readingreading textual log of traces, right. So this was quite useful for us and built multiple generations of these tools, and current version we have in OCaml 5.0l we have something called runtime events, and does efficient CTF based tracing that is always on, and not emitting any trace, and the feature is always on, and take any OCaml program and enable flag or signal and starts producing live traces so you can live trace any OCaml program today. And Sadiq and Patrick are talking about the tracing framework in OCaml workshop later this week. And quite challenging maintaining the separate fork compiler for 7 + years, and had to go through multiple basis neared to keep up to date with the mainline in that process, unfortunately not always communication between the features developed at trunk and what we had. So we had to sort of drop certain features because we just couldn't figure out how to fit with the GC design we had. We are slowly working towards adding them back in.
But that's a process. In hindsight may have been easier if we had enough planning to have made smaller PR's to the language.
So PR papers add credibility to the report. I see them as nice design document of the snapshot of the system. And of course that's not reflecting what is there, but you can start there and then start reading thousands of lines of code.
Our project has always been open source and actively maincontained, surprisingly a lot of academic users from early days and having users helps convince developers this feature should be added. And build tools like continuous benchmarking and open health check in order to allay the fears that when the feature is deployed in the wild due to unrelated reason sequential program breaks.
So started working on OCaml, I started OCaml 4. And hack on something, and one day be at OCaml 5. But really turned out to be this asymtotic progress toward OCaml 5. And I'm 90% done, and really the rest of 10% takes 90% of the time. And while the first 90% can be done with few researchers, it turns out need a lot of engineers for the last %. Research primarily driven by university of Cambridge, and IITMadras and Jane street. And a lot of Connecticut contributors working on getting the language across the line. So where do we go from here. Obviously the moon, but how do we get there.
First obvious thing add an effect system to the OCaml.
Right, but it's a lot of work in a language as large as OCaml you have to think about interaction of whatever feature you are adding with the he polymorphism modularity and generativity features that you have already in the language. And then you have to do it in a backwards compatible manner as well, quite a tall order. I'm not an expert in effect systems but happy if somebody takes this up. OCaml has nice compiler to java script, but can't compile effect handlers yet to JavaScript, because we don't know how to compile delimited continuations efficiently to JavaScript.
Daniel H, and otherses have published a paper as to how to take different variants of effect handlers and compile them efficiently to untyped lambda calculus, there is some work there, but JavaScript engines are their own beasts, and hence that needs to be a lot of engineering work there to get the right combination strategy.
So there is exciting work going on within Jane street, adding more model types to OCaml. And FLARL using this feature to allocate Vals on the stack and then using the type system ensure that these value do not escape the stack frame, so you can take the contact effects and use the model types to get lexically scoped type handlers, This would be much easier, you will get effects system here but you don't have to have the full power of tracking effects in types, with more backwards compatible, then a full fledged.
.
OCaml is getting a lot of performance oriented features currently being developed or in development, including parallelism in OCaml 5. And providing allocation through model types and unbox types, and Flamdda2 aggressive compiler optimization for OCaml. And when the features land, you can have roster C like control and performance while having as default, and safety of classic ML. So f PS Richard E are talking about the unbox types.
Work at the ML workshop tomorrow that, I'll stop here and enjoy your OCaml 5.
[APPLAUSE]
