So this is a joint talk with Meven Lennon-Bertrand that you can find in this room, and Nicolas Tabareau and Ã‰ric Tanter, once again.
So I already had a really nice introduction to the topic, but let me try to pitch it my own way another time.
So my goal in this talk is to try to build dependent type proof assistant in -- which is easier to use. So I will use the same example of fixed size list, this time called vectors, which take an argument of base type A and specification of a size of a list that is given by an actual number.
We have two kind of vectors. The empty vectors that have size zero and the -- of a head on top of a tail of size N which is a size six or so of N.
On search vectors, we might want to define values kind of functions, such as map or filter function. A filter function is a function that will take -- predicate on to element contained in the vector. A vector of some length N, and that will try to filter the list, the vector by this predicate. Allow do we do that?
In the case of the empty vector, we just return ventie vector directly, and if you have a head on top of a tail, then we check whether the predicate holds of a head and -- recurs on the tail or keep the head on top of zsh on the tail.
This function filter takes an argument of size, the size we know statically, even by length, but what's the size of a result of filtering this list by an arbitrary predicate?
Well, at first we cannot say, we cannot give a close formula immediately. We have to think a bit about, wait, how does it compute, and actually the programme for computing the size of a result is that this size of a result depends on the dynamic value of a list that we get as input.
So in order to prototype efficiently independently typed language, we might want to say, okay, for now I will just put something, some unknown value that I will write question mark, and I will try to evaluate my programme and see how it works. Let's see how things unfold.
So one thing I will not -- we have to consider as implementers of independently typed language and a proof assistant is that adding this kind of question mark to -- in the type checking procedure. For example, if I have something which should be a size vector of size 1 plus some unknown term, and I want it to be a size vector of a known, so I need to be able to optimistically assume that these two things are consistent.
So that's the kind of thing that dependently a gradually dependently typed proof assistant should be able to provide you.
Now on such an example, how do we run actually the thing? Let's try to see the example like filtering two different lists. In the first case I'm filtering the lists 1 to 4 by the predicate even, and in the second case I'm filtering venti list.
In both cases we can evaluate the results of filtering this list, and here I can both cases type check because we have result of type vector of unknown length, which is consistent with the successive length, so that we can apply the add function.
So reducing the filter function gives us in the first case head applied to a list which contain only two, and in the second case, we obtain only the nil list.
Now if the first case we can further reduce the list that only contains two and get a result that will be two.
In the second case, however, we have a nil constructor, which is supposed to be in the type successor of some unknown type, and this should be incompatible with the fact that things in a successor vector, successor size should be of a head on top of a tail.
And so we need to enrich our language with arrows to handle this kind of impossible case that happens dynamically.
And so here I will get --
For prototyping purpose, I could say this second case where I'm passing nil is not useful, so I might want to give a simple specification that refine this unknown bound. For example, saying if my input list is nil, then the resulting size will be zero, and otherwise I will over-approximate seeing that the resulting size is unknown.
In such a case, the second example where we try to take the head of filtering an empty list, will not type check anymore, so we will have gained a bit of static information on which programmes type check, and so that's the motivation for using -- dependent types for prototyping purpose.
So how do we achieve such a dependently gradual dependently typed language? Well, we propose in previous work to separate this language in two sub-language. A first sub-language which is actually gradual and a second language which is -- and the gradual language elaborates the -- while it provides a computational meaning for the gradual language.
The cast contains unknown types and terms. Every type is also inhabited by arrow terms, and we have arbitrary test between pairs of types.
In order to control this test, a notion of precision between types gives us the information about how precise the information of the type that we have is. So the most precise types are the static types while where we lose precision we get to an unknown type, which is actually maximal for the precision order.
Precision between types, controlled cast by enforcing the cast are valid in the following sense, meaning that if two types, if A is more precise than B, then casting from A to B is an injection and it has a retract, which is a cast from B to A.
And the fact that we can interpret the notion of precision as such -- projection pair will be the fundamental semantic criterion to build all our language.
So now in previous work we also observe that actually trying to get all properties that we want from such gradual dependently typed language is not possible. We cannot have at the same time the gradual guarantees that we want, and normalising language, and conservativity of the base language that we tried to gradualise.
And in order to have a proof assistant, normalisation cannot be trained. We want to have normalisation to be sure that we can actually have a decidable type checking algorithm and that we can actually implement our proof assistant.
Moreover, we cannot trade the -- of the base calculus because we want to be able to re-use all programmes from the existing literature, from existing programmes.
So in this work, I will -- so we had three different variants in previous work that were trading the different possibilities, and in this work I will concentrate on this variant, which has both an ambiting of CIC and that is normalising.
So now I will look at the cast calculus, which has this property. So I have this cast CICN which is both normalising and conservative over CIC, however it is not globally gradual, and the main question that this work tries to answer is how far it is from being gradual.
We are not able to show graduality globally for every types, but maybe it might be possible for sometimes to show some gradual properties. And how do we do that? Well, we will actually internalise the notion of precision inside the type theory so that we can actually reason inside the type theory on the precision between types.
So we could GRIP the resulting theory, which expands this calculus with a notion of precision, actually two families of precision. One for precision between types and a family for precision between terms.
And these types of precision are actually proposition that we -- that inhabit a pure universe of proposition, PROP, and this universe of proposition contains no errors, no casts and no unknown terms, which make it a good place to have sound reasoning for the type theory.
So what does this internal notion of precision actually means? Well, we can explain what it means in terms of dynamic gradual guarantee. So dynamic gradual guarantee is saying that if I have a term X, which is more precise than term Y, and a context for values in type A, applying the context to X should be more precise than applying the context to Y at Boolean types.
And we have the dynamic gradual guarantee will hold for a context C if and only if this context is related by precision inside the language at type of Boolean predicate on A, and if and only if this context is monitored.
In order to understand concretely what this formula actually means, we should look at precision on Boolean types, which contain true and false, as usual, but also the exceptional values provided by the gradual language, arrows, and unknown. And we have a lattice where arrow is the smallest element and unknown is the biggest one, meaning that the dynamic gradual guarantee actually means some kind of error approximation.
Meaning that if my value X plugged in the context C does not error, then I'm sure that value Y plugged in context C will not error either.
So now that we have some understanding of this notion of precision, how do we prove actually in our language the -- how do we derive a proof of precision? Well, let's have a look at two terms.
The first term is a function from natural to natural but add one to a natural number. And the second one takes an unknown -- a term of an unknown type and cast this element to natural numbers and then add one.
If we are able to prove that the first static term is more precise than the other known function, then this will mean that other known will not have a role on good input, and how do we do such a derivation? Well, we start from the judgment that we want to prove.
We look at function types, and at function types precision is actually extensional. It maps elements related by precision to elements related by precision.
Now we will look, we will intro the argument in the context and do some computation to unfold the add one and add unknown, and we need to compare two application of plus one of successor, and successor is actually monotone, so we can actually just forget about it and show it's enough to show that X is more precise than the cast.
Finally, to complete this proof, it's enough to know that whenever we have two types that are related by precision, the ambiting projection properties will allow us to get that this cast is actually a right so we can use it directly to prove that X is more precise than C at the type not more precise than unknown.
So now this proof that I just gave is made out of some principles that we have on precision, such that trancivity, quasi-reflexivity, the add junction properties that I just used, and also an important property that allows us to characterise precision between types but are not directly related by precision.
On the other hand, we cannot have everything in our language. In particular, the precision on types is not the same thing as the precision on the universe of types, and we have -- this allows us to keep a normalising language, but it makes reasoning a bit surprising, let's say, and we have to use some explicit cumulativity in order to regain enough expressivity in our language.
Really quickly I would like to outline that we have some interesting metatheory properties on this language. It is normalising. It is consistent, normalising, and moreover we can carve out some fragment of this language where we can automatically derive the terms are gradual.
So to conclude, we had a proof of concept of this type theory in Agda, and we have a model that validates the theory that I just alluded to.
We have non-gradual operations in this language, so for example we have a catch operation on inductive type, but I did not have time to talk more, much about. And this paper can also be seen as a way to address the question of catch in that gradual world.
And we add precision internally in order to reason posthoc on the gradual terms of this language.
So for future work, we would like to have Alex from a gradual source. Right now we are working only with a cast calculus, and we would like to have more general inductive type, maybe an actual implementation from the source language, but this will have to wait for another time.
Thank you.

