So this is a joint talk with Meven Lennon-Bertrand that you can find in this room, and Nicolas Tabareau and Ã‰ric Tanter, once again.
So I already had a really nice introduction to the topic, but let me try to pitch it my own way another time.
So my goal in this talk is to try to build a dependently typed proof assistant in -- which is easier to use. So I will use the same example of fixed size list, this time called vectors, which take an argument of base type A and a specification of a size of a list that is given by an actual number.
We have two kind of vectors. The empty vector that has size zero and the cons of a head on top of a tail of size n which has size successor of n.
On such vectors, we might want to define various kind of functions, such as map or filter function. A filter function is a function that will take a boolean predicate on the elements in the vector, a vector of some length n, and that will try to filter the list, the vector by this predicate. How do we do that?
In the case of the empty vector, we just return the empty vector directly, and if you have a head on top of a tail, then we check whether the boolean predicate holds of a head and either recurse on the tail or keep the head on top of the recursion on the tail.
This function filter takes an argument of size, a size we know statically, given by length, but what's the size of the result of filtering this list by an arbitrary predicate?
Well, at first we cannot say, we cannot give a close formula immediately. We have to think a bit about, wait, how does that compute, and actually the programme for computing the size of a result is that this size of the result depends on the dynamic value of the list that we get as input.
So in order to prototype efficiently in a dependently typed language, we might want to say, okay, for now I will just put something, some unknown value that I will write question mark, and I will try to evaluate my programme and see how it works. Let's see how things unfold.
So one thing I will note, and we have to consider as implementers of independently typed language and a proof assistant is that adding this kind of question marks creates some kind of troubles in the type checking procedure. For example, if I have something which should be a size vector of size 1 plus some unknown term, and I want it to be of size vector of unknown, so I need to be able to optimistically assume that these two things are consistent.
So that's the kind of thing that dependently -- a gradually dependently typed proof assistant -- should be able to provide you.
Now on such an example, how do we run actually the thing? Let's try to see the example like filtering two different lists. In the first case I'm filtering the lists 1 to 4 by the predicate even, and in the second case I'm filtering the empty list.
In both cases we can evaluate the results of filtering this list, and here I can in both cases type check because we have result of type vector of unknown length, which is consistent with the successor of length, so that we can apply the head function.
So reducing the filter function gives us in the first case head applied to a list which contain only two, and in the second case, we obtain only the nil list.
Now in the first case we can further reduce the list that only contains two and get a result that will be two.
In the second case, however, we have a nil constructor, which is supposed to be in a type successor of some unknown type, and this should be incompatible with the fact that things in a successor vector, successor size should be of a head on top of a tail.
And so we need to enrich our language with errors to handle this kind of impossible case that happens dynamically.
And so here I will get --
For prototyping purpose, I could say this second case where I'm passing nil is not useful, so I might want to give a simple specification that refine this unknown bound. For example, saying if my input list is nil, then the resulting size will be zero, and otherwise I will over-approximate saying that the resulting size is unknown.
In such a case, the second example where we try to take the head of filtering an empty list, will not type check anymore, so we will have gained a bit of static information on which programmes type check, and so that's the motivation for using gradual dependent types for prototyping purpose.
So how do we achieve such a dependently gradual dependently typed language? Well, we propose in previous work to separate this language in two sub-language. A first surface language which is actually gradual and a second language which is a cast calculus and the gradual language elaborates to the cast calculus while the cast calculus provides a computational meaning for the gradual language.
The cast calculus contains unknown types and terms. Every type is also inhabited by error terms, and we have arbitrary casts between pairs of types.
In order to control this cast, a notion of precision between types gives us the information about how precise the information -- the type that we have is. So the most precise types are the static types while when we lose precision we get to an unknown type, which is actually maximal for the precision order.
Precision between types control cast by enforcing the casts are valid in the following sense, meaning that if two types, if A is more precise than B, then casting from A to B is an injection and it has a retract, which is a cast from B to A.
And the fact that we can interpret the notion of precision as such embedding-projection pairs will be the fundamental semantic criterion to build all our language.
So now in previous work we also observed that actually trying to get all properties that we want from such gradual dependently typed language is not possible. We cannot have at the same time the gradual guarantees that we want, a normalising language, and conservativity over the base language that we try to gradualise.
And in order to have a proof assistant, normalisation cannot be traded. We want to have normalisation to be sure that we can actually have a decidable type checking algorithm and that we can actually implement our proof assistant.
Moreover, we cannot trade the embedding of the base calculus because we want to be able to re-use all programmes from the existing literature, from existing programmes.
So in this work, I will -- so we had three different variants in previous work that were trading the different possibilities, and in this work I will concentrate on this variant, which has both an embedding of CIC and that is normalising.
So now I will look at the cast calculus, which has this property. So I have this cast CIC^N which is both normalising and conservative over CIC, however it is not globally [gradual, and the main question that this work tries to answer is how far it is from being gradual.
We are not able to show graduality globally for every types, but maybe it might be possible for sometimes] to show some gradual properties. And how do we do that? Well, we will actually internalise the notion of precision inside the type theory so that we can actually reason inside the type theory on the precision between types.
So we call GRIP the resulting type theory, which extend this cast calculus with a notion of precision, actually two families of precision: One for precision between types and a family for precision between terms.
And these types of precision are actually propositions that inhabit a pure universe of proposition, PROP. And this universe of proposition is pure in the sense that it contains no errors, no casts and no unknown terms, which make it a good place to have sound reasoning for the type theory.
So what does this internal notion of precision actually means? Well, we can explain what it means in terms of dynamic gradual guarantee. So dynamic gradual guarantee is saying that if I have a term X, which is more precise than a term Y, and a context for values in type A, applying the context to X should be more precise than applying the context to Y at Boolean types.
And we have that the dynamic gradual guarantee will hold for a context C if and only if this context is related by precision inside the language at type of Boolean predicates on A, and if and only if this context is monotone.
In order to understand concretely what this formula actually means, we should look at precision on Boolean types, which contain true and false, as usual, but also the exceptional values provided by the gradual language, errors, and unknown. And we have a lattice where error is the smallest element and unknown is the biggest one, meaning that the dynamic gradual guarantee actually means some kind of error approximation.
Meaning that if my value X plugged in the context C does not error, then I'm sure that value Y plugged in context C will not error either.
So now that we have some understanding of this notion of precision, how do we prove actually in our language the -- how do we derive a proof of precision? Well, let's have a look at two terms.
The first term is a function from natural to natural but add one to a natural number. And the second one takes an unknown -- a term of an unknown type and cast this element to natural numbers and then add one.
If we are able to prove that the first static term is more precise than the add-unknown function, then this will mean that add-unknown will not have errors on good input. And how do we do such a derivation? Well, we start from the judgment that we want to prove.
We look at function types, and at function types precision is actually extensional. It maps elements related by precision to elements related by precision.
Now we look, we will intro the arguments in the context and do some computation to unfold the add one and add unknown, and we need to compare two applications of plus one of successor, and successor is actually monotone, so we can actually just forget about it and show -- it's enough to show that X is more precise than the cast.
Finally, to complete this proof, it's enough to know that whenever we have two types that are related by precision, the embedding projection property will allow us to get that this cast is actually a right adjoint so that we can use directly to prove that x is more precise than z at the type nat more precise than unknown.
So now this proof that I just gave is made out of some principles that we have on precision, such that transitivity, quasi-reflexivity, the adjunction properties that I just used, that are coming from embedding projection pairs, and also an important property that allows us to characterise precision between types that are not directly related by precision.
On the other hand, we cannot have everything in our language. In particular, the precision on types is not the same thing as the precision on the universe of types, and we have -- this allows us to keep a normalising language, but it makes reasoning a bit surprising, let's say, -- and we have to use some explicit cumulativity in order to regain enough expressivity in our language.
So really quickly, I would like to outline that we have some interesting metatheory properties on this language. It is normalising. It is consistent, normalising, and moreover we can carve out some fragment of this language where we can automatically derive that terms are gradual.
So to conclude, we have a proof of concept of this type theory in Agda, and we have a model that validates the metatheory that I just alluded to.
We have non-gradual operations in this language, so for example we have a catch operation on inductive type, but I did not have time to talk more, much about. And this paper can also be seen as a way to address the question of catch in a gradual world.
And we add precision internally in order to reason posthoc on the gradual terms of this language.
So for future work, we would like to have elaboration from a gradual source. Right now we are working only with a cast calculus, and we would like to have more general inductive types, maybe an actual implementation from the source language, but this will have to wait for another time.
Thank you.

