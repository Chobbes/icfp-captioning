Beniamino, and Gabriele Vanoni. 
And let's start talking about reasonable computational models. 
Computational model, is if you like way of computing functions, so any input X, you represent the input X as state of automaton and little elementary step go to the toward the final state which you can extract the result, namely the value of the function on X. 
And you would like this automaton such that each transition is elementary enough, that it's not only effective but implemented in constant time or very low time complexity. And if you do that, you can arguably say the model induces notion of time, and notion of space. 
So the time is model number of transition, and space is modeled maximum size of configurations you encounter along your computation. So many computational models. So there are many computational models as we all know, for example Turing machines are arguably the standard reference computational model, and you know the, in this case, the fact that each step is Elementary is of course very easy to realize. And also have different computational models, and random access machines and reference model for concrete complexity of algorithmsnobody study the complexity of algorithms directly at the level of Turing machines. On the other hand, Turing machines and random access machines are, in a sense equivalent as we all know, and that's what we mean by reasonablereasonable. Simulate each other. And polynomial in time, and linear overhead in space. 
So for example, we can a problem to be polynomial time, computable languages, either by looking at turring machines and random access machines without any ambiguity. And same thing can be said about algorithmic space, and computable problems, question is what happens with functional programs, can we give functional programs the same state we give Turing machines our random access machines, this is problem we have been working on, researchers have been working on for years now, and this paper and this talk will be in particularly about aspects related to space. 
But, let's take a look at what happens in time first, let's do that by looking at Krivineen many, Thomasz talked about a much more sophisticated machine. 
The Krivine machine is arguably the simplest machine, Lambda calculus evaluation, and based on environment closure and stackses. And compared to the machines we saw in the previous talk by Thomasz extremely simple in the sense it has just 3 rules, configuration, three polls and term and environment and stack, and just need one rule for each operator in the Lambda calculus, one for application, and obstruction, and variables, and couple things want to observe about this machine, the first one is that in the first rule you see environments duplicated, copied, and typically, implement this copy by sharing, as we already seen. 
And this of course, is done for the sake of being efficient in terms of time. 
On the other hand, you have crucial environment that is following one. Every reachable configuration is such that all terms appearing in the configuration have subterms of initial term, which is very good, which means you don't create new terms, just navigate the initial term as far as the first component is concerned, of course, and the theorem, which has already been mentioned the number of reduction steps in the Krivine abstract machine. 
Reasonable time, and model Beniamino said in the first talk, and derive, result, telling you the number of beta steps is invariant... reasonable time cost model. Very good, so by the way, let me stress by reasonable, what we mean is polynomial overheads between Lambda calculus on one end, and turing machines on the other side. So let's look at types, would like to reflect the time behavior of terms. 
Compositionally, in a type system. 
So has to be able to reason about it. Not just by operation of semantics by the machine by way of types, inherently compositional. So suppose you have computation starting from the initial term of configuration from T, and since U is subterm of T, you know that in any type derivation for T simple types for example, U is there. Because it's a subterm of the initial term. The point is that this correspondence between configuration, and subderivations is not UB JEKTive at all. The same subterm can be encountered in many difference places along with the computation, so the size of... 
tells you nothing about the complexity of reduction, situation changes dramatically if you switch to multi types, which, as Beniamino said the important intersection types in knowledge reporting T intersection types actually you, you are not so far conceptually from simple types, what you have on the left side of the arrow, not just one time but multiset of types, and you type variables typing judgments with multi sets and you treat that these environments. Much in the same style as with you treat environment in linear type systems. So the multisets, added up for for example when you type applications. So you do that rather than simple types, you still have this picture, and correspondence between configuration and subtype derivations. The point is that this is now an injection. So, every configuration. . 
These correspond to SDIFRNT subdare variations that means the This means that the size of type derivation tells you a lot about the time complexity of a term. And indeed, you can even go beyond that and say you label your typing judge NLTs with actually a weight. That tells you better... how many rulings instance you find in the type derivation. And you can prove -- Carvalho 2006. Multi types are a sound and complete methodology for deriving the time behavior of terms in the machine. So closed term, reduces in W steps, and on the type star, because star is the type of result in multitypes. 
Very good, so now, let's take a look at space. Because we could like to do more or less the same thing but with space. Point is that if you just take a look at one computation in the machine, and focus on your beloved intermediate configuration you would like to say how big it is. 
You want to use sharing because this way you are efficient as far as time is concerned, point is if you use sharing space becomes essentially unaccountable. When are you doing garbage collection, are you using reference counting. 
What's the cost of it? 
What is the space cost of it. 
Sharing makes space a little bit more difficult to be dealt with, and must be done with great care, and on the other hand, closure, become unnecessarily too deep, and easily made on change of variables, and of course this is not good for point of view of space, but most fundamentally the main problem I would say, where is the input? 
When you want to evaluation space consumption of algorithms, he input is outside of the picture, you don't want to counter the the size of the input in your space consumption, because you want to deal with things like sub linear space algorithms. Where is the input here. Intuitively it's T, but T it's everywhere in U, E, pi. 
And must be dealt with. This is what we did this year actually by introducing what we call the space KAM machine. And so, in which space is dealt with with great care. 
So we have 5 rules instead of 3. We have 2 rules for applications, and the first one is implementment standard technique to deal with the chain of assignments, of chain of variables, which is called, which is called unchaining, in deed when the variable, you can shortcut everything, and this is quite convenient. And second rule, copying is indeed performed but only when necessary, this, on top of that, share something by design limited to terms. 
We don't share at all environment, we don't share at all closures, we only share the initial term. This is implicitty a way to account for the decide of input. Only the terms are shared. Finally the third rule tells you we do garbage collection, but in very simple trivial way, but in a meager form, namely, whenever we realize the variable simply does not occur free in the body, we can again shortcut the whole thing, and do some kind of garbage collection. And this year, we proved the size configurations measure this way. 
O excluding pointers to the initial term is indeed reasonable space cost more than for the space Comm, and by that we mean that there is a linear override between a Turing machines and random attacks machines on one Kenned, and Space KAM, I wrote KAM, but we're working with the space KAM. In this work, what we did was reflect the space behavior of the space KAM which is reasonable, so from the point of view of complexity is makes sense in two types. So we still have the pictures if you work with multitypes, and correspondence is still injection. On the other hand, immediately realized that what we want to measure is namely the size of closures, because ultimately, environments and stack are made of closures and place no role at all in multitypes. Multitypes do not keep track of size of closures at all. A closure, which will be substituted for any of the variable, it can be anything, can be very deep, you can not see anything about it by looking at ordinary multitypes, and then our idea was to enrich multitypes, but by some information related to the closure, the size of closures. 
Actually, this turns out to be quite simple, and we just needed to have a system of labels, so every multitype becomes now closure type, namely multiset labeled with a natural number. 
This natural number is simply meant to capture size of all closures. And which substituted for the variable, in deed in typing judgments, variables are typed with closure types, so you also need this label. 
Actually, let's take a look at couple of rules just to see what happens at the typing rules. Of course the type system is not as simple as the usual one, the usual multitype system. 
You need a little bit more rules, arguably you need new rules because the underlying machine has changed, but example of rule variables it's at usual one, don't need anything special, except the fact you need to take into account whenever the variable, in the interneediate configuration, it's size morallly should take into account the underlying closure, which is K, and also of the stack. And that's why we have to take the size of A into account. The size of types, just measure the size of underlying stack. When you do for example, when you type applications, well, you do it more or less as in ordinary intersection types, but the key here is the following, you have to take the max rather than the sum, because space dealt with not summing up the weights, but by taking the max, when you type a value, as a final result, should take into account the final result can be a closure. 
So the size of free variables. 
Should be taken into account. 
So the theorem am, the main result, the closure type sound and complete methodology for evaluating the space. And namely every close term receive subtype, weight W, even though if the space consumption is precisely that weight. So contributions has shown that reasonable cost model, for the Lambda calculus can be reflected in two types, so that we give a compositional flavour to it. 
The same thing can be said about the time cost of the space com which must be defined as low level time cost. Because, of course sometimes you have to copy environments, and from the point of view time can be problematic, and future and on going work is about first of all, the generalization of all data by Call-by-Value machines, and space time generalized by ready to provide value. And working on generalization closure types, and more interestingly, we are looking at type systems in which the idea of measuring the size of closures becomes a way to do complexity analysis. 
And now of course you can't say that. Because, multitypes are not at all a way to do verification programs, because by design undecidable problem. 
Thank you, very much.
