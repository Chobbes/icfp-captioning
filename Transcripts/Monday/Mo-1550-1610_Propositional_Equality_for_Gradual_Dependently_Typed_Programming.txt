So this is joint work with Ron Garcia and Ã‰ric Tanter. So what are the research questions that we answer in our paper? So the first thing we're looking at is can we use gradual typing to take equational specifications and move these between run time, like assertion checking, and compile time, as in proofs of equality.
And we are able to do that, and the way that we're able to do that is with a gradual notion of propositional equality. Now the second question we answer is: Given that we have this gradual notion of propositional equality, can we include it on our language without compromising the various properties of that language that we'd like it to have?
And it turns out that we can, and the way that we do this is by having an equality that's based around an entire family of RAFL constructors, instead of just a single one, and that that uses -- it dynamically tracks information about the consistency of the equated values.
So to get us into this, I'm going to start with an example of where we might want to check specifications dynamically and why the current approaches don't quite give us what we want.
We'll start with the quick sort that everybody knows and loves, except it will have a bug. So quick sort, as you probably -- as you're familiar with, sorting the empty list gives us the empty list, and sorting cons of a head and a tail, we take the tail and we filter it into everything that's less than the head, everything that's greater than the head, and then we concatenate, sort those and concatenate them together in the right order, except there's a problem because we have greater than instead of greater than and equal to, any duplicates will get erroneously removed.
For example, if we sort 989, we get 89, which is clearly not an arrangement of the original list, but the problem with this is that this is just silently incorrect. Unless the programmer has a test to catch this or is manually looking at the results of this code, there's not going to be any sort of error thrown by this.
So how can we prevent this? Well, one way of preventing this is to use dependent types, which allow us to take the specification and write it out quite explicitly using the type system.
So the way we're going to do this is with fixed length lists. So here, a fixed length list is indexed by a number, so that's the dependent part of dependent types, the type depends on a number. And it's going to be a dependent pair between a list, so some just a regular underlying list, accompanied by a proof that the length of that list actually matches the index that we have in our type.
And so is this going to catch the bug? So let's look at how we would change our code. So we change our list in type signature to F list indexed by N, and so we're saying that whatever the input length is, the output had better have the same length. Now our nil case, nil is paired with the proof that the length was actually equal to zero, and we can use that same proof in our results.
But when we get to our cons case, we get a proof that the length of cons is one plus the length of the tail, but when we're producing our result, we don't know -- well, we need a proof to put here, and the thing that we actually have to prove looks like this.
It's that the length of doing this filter and then concatenating back together is the same as the length of the original list.
The problem is that there's a conceptual gap between the programmer seeing this and trying to produce a proof of this type, and what the actual problem is, which is that sort is accidentally removing duplicates. These two things don't look the same, and the programmer might try to fill this goal. They're never going to be able to because of the bug. But the compiler isn't giving them any feedback saying this is impossible. The only feedback they get is the difficulty they encounter trying to build an impossible proof.
One of our guiding philosophies is that difficulty proving a goal shouldn't be the only feedback the programmer gets. We obviously have to work within the realm of decidability, but when we can get more information dynamically, we want to give that information to the programmer to help them find what the problem is.
So instead of having them try to write this proof and fail, what are some alternatives? So if you've used Agda or Idris you'd say, well, we'll just put a hole in and keep working. The problem with that is now we can't run this code. The compiler has used the code as incomplete, and you can't compile it. Instead we can declare it as an axiom. Say suppose we have a proof of that type and we'll get going.
This causes problems with type checking because it blocks reduction, but the worst problem is that it doesn't actually check the error. It assumes that the thing you have is correct, and if you ever use it in a bad way, because reduction gets blocked, it will just happily proceed.
So the alternative that we're proposing is to use the imprecise term from gradual dependent types. Sorry.
So that imprecise term, we write it as question mark, and so this has been established from a couple of existing works on gradual dependent types, but what makes it different than holes or axioms is that the imprecise term has dynamics semantics associated with it.
So if we use that question mark in the result of sort and then we run it on that same concrete list, then when it runs, it performs all sorts of gradual run time checks to make sure that even in the presence of imprecision that everything is safe.
And then at the end we get 8,9 paired with this imprecise proof that it has the right length. The programmers are happy because they can keep going despite not having to write the proof.
We haven't gained anything over the version we started with. We have the same silent failure. We just got this question mark out at the end as our proof, and what's strange is that question mark ends up in this case having the type two equals three, and the run time said, well, I'm not going to throw an error. I was able to run safely. You never took the head of an empty list. You never accessed an unallocated memory, but that type is absurd, and wouldn't it be great if the run time could see that this equality couldn't ever be inhabited and notify the user of that? And that's exactly what we've designed with our gradual propositional equality.
So what does this look like? To start for our design, I'm going to do a little review of threesomes in the middle types. These come from the non-dependent gradual dependent -- or the non-dependent gradual type literature.
So if we have a cast from a type A to a type B, the threesome approach says we view that cast as going through A compose B, and so that's called the middle type, and it's precision bound for both A and B. It's a type that's as precise as both of those types.
But what's really useful about this middle type is it allows us to use a single type to collect a lot of information. So if we have a bunch of casts all happening in succession, we can now represent that as a single cast through a middle type where that middle type is the composition of all of the different types we're casting through, including the final destination.
And so the middle type gives us a way of remembering the constraints from multiple casts. So how can we adapt this to dependent types and propositional equality?
So in a static dependently typed language, you prove an equality with REFL, and REFL is the connonical proof that for every X you give it, that X is equal to itself.
Our gradual version looks like this, we have REFL that can prove that any X and Y are equal to each other, provided that you give a witness W, and that witness is a witness of consistency. So there's an entire space of possible witnesses, and that space is all of the terms that are as precise as both X and Y, and if there's something that is as precise as both of them, then the information that they have is consistent with each other and we say that we can equate them modulo that imprecision.
So how do we actually create an equality proof? So to create an equality proof, we can do our witness proofs can replicate the static approach, so X is a witness that X is gradually equal to X.
But we also can have an imprecise proof. So if we have question mark, which is the imprecise proof of X equal to Y, what this ends up reducing to is a proof that X is equal to Y, witnessed by the composition of X and Y, which is by -- witnessed by something that is as precise as both of them.
And this is what's going to give us an advantage over the previous iteration of question mark that we saw in those previous slides.
So we've got in our space of witnesses at least one thing, which is X compose Y.
Now once we have an equality proof, what can we do with it? How can we transform it? So if we're casting between equality types, and these casts are usually generated during elaboration of gradual languages. They are opaque to the user but how the semantics are defined.
When we're casting between these equality types, we're doing that by composing the two things that we're equating in the destination type, but we also compose with the witness that was associated with that proof before. And so the key here is that this produces something that is as precise as the new things we're equating, X prime and -- X and Y, but it's also as precise as X prime and Y prime, and everything else that we saw before by composing an composing and composing every time we use one of these witnesses we're able to track all of the different equality constraints on that we see as the programme runs.
So our space of witnesses, we have this X compose Y, but as we compose with more things, we get more and more precise witnesses, and this is one of the key things about our work, is that every time an equality is cast, the new witness retains all of the precision information that we had from before, and that's what lets us use this to catch errors and to inform the programmer.
So what happens when two values are inconsistent? So if you've got X and Y that are not consistent with each other, then in this case the witness space collapses to a single value, which is error. So we've got this whole space of witnesses, but that collapses down, and X compose Y, if X and Y are inconsistent with each other, is an error.
So to see how this works and how this helps us, let's go back to our sorting example. If we sort the list from before, 989, in our language, we get the same thing, 89 with this imprecise proof, but with our gradual handling of propositional equality that's question mark of type 2 equals 3, and that reduces to the proof witnessed by the composition of 2 and 3, which reduces to the proof of error.
So the programmer has now been informed by the raising of this error that their goal was impossible to prove. There must be some -- the bug must be there because two cannot possibly equal three, so the specific version is false, so the more general version must be false as well.
And now we're able to show the programmer that they need to fix something and roughly where they need to fix it.
So once we have -- we've showed you how to build equality proofs, what can we do with those equality proofs?
So in static dependent types there's a transport operation. If you have some type P that's indexed by X and you have a proof that X is equal to Y, then you can use that proof to transport your P of X into a P of Y, and from the static version, we say, well, the proof must be refl, so then we have a P of X and our destination type is also X, so we can just use the thing we originally got in.
Now of course when our X and Y might not be exactly the same, that doesn't work. So what we do is we cast through the type P indexed by the witness, and once again, now whenever we're eliminating our equality, we've retained all of the different constraints that we encountered in building that proof.
So we retain that information.
Now I talked a bit about the -- in the beginning about the challenges of the metatheory, so I'm going to give just a very quick overview of what those challenges are and how we overcome them.
So if we look at this function, it's just a function, it takes in two arguments, ignores the second one, and doubles the first argument by adding it to itself.
Now we've got another function that does the exact same thing, except it ignores its first argument and doubles the second one. These should be statically distinguishable in a static language the compiler will look at these and say, no, these are not definitionally equal. We can't use these interchangeably.
But the flip side is we have another function that does the exact same thing as our F, but it does it by doing two times instead of X plus X. Now these are observationally equivalent. We shouldn't be able to distinguish these at run time. There should be no way to crack the function open and break the static equivalences that we had in our non-gradual language.
So how can we remedy these two things, especially given that F composed with itself should give F? What can we compose this F -- with that would respect their dynamic equivalence?
So the way we do this is to compose two functions. We compose them by producing a new function that takes in its argument and then composes the bodies of the functions after applying its argument, and the key here is composition is an operator in our language, unlike in the non-dependent versions of the middle type.
And what this lets us do -- I apologise for the misplaced square -- is this whole thing is blocked by a neutral term. Because it's an operator of the language, we can say once we hit something that's neutral, if there's a variable that's blocking us, we just won't evaluate any further. We won't check if they are consistent until you supply a concrete -- to the function.
But then the question is we've got these two functions that should be different, and by composing them don't we have a witness of consistency that these two different functions are different?
And so one of the key insights of our paper is that the static notion of consistency is not the same as the dynamic notion of consistency, which is when two terms compose to a non-error. We want to use this one in the conversion check to preserve our static distinguishability, and we want to use this one for our witnesses of equality.
So there's some more goodies in the paper. We talk more about the design of our precision. We talk about the metatheory, and we talk about some extensions to the language.
So I'll leave you with that. Thank you.
