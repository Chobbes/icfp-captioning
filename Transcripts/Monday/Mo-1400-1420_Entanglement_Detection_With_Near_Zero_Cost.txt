So this is joint work with PhD student Jatin, and our advisor .
This work is guided by more generally by a question of whether or not parallel functional programming can be fast and scalable. This has been a long standing question in the community because functional programming has great promise for parallel programming, except for the question of efficiency, or for a long standing time the question of efficiency has been a difficult one. The main challenge is really one of memory management, not so much the complication, say, but actually in the runtime system how do you implement an efficient garbage collector for a parallel functional language, okayokay. Functional programs, are somewhat unique, n that they have a high rate of allocation, rather than modifying something in place, functional programs tend to allocate something fresh. Okay. So creates high rate of allocation, and therefore high reliance on garbage collection. To get good spatial hit space efficiency.
Okay, now in the parallel setting, the rate of allocation grows linearly with the number of processors that you're using or Canada grow licensely with the number of processes Okay, so we have a much more difficult garbage collection problem essentially this garbage collection for functional languages is fundamentally harder than garbage collection for imperative languages, in some sense. Okay. Now, we, over the past five or six years I've been working on this problem and we can definitively say at this point, actually yes functional programming can be fast and scalable. Okay and the way that we've shown this is through a property that we call this disdisentanglement, 10,000 feet that's quick high level sketch of what is disentanglement it's, I'll give you an informal definition here and then we'll talk more formally later but the idea is that concurrent tasks remain oblivious to each other's allegations. So two tasks excusing executing...executing simultaneously concurrently really, and they are both performing allocations, then disentanglement says there should not be pointers across the gap shouldn't point across to each other.
So this turns out to be very closely related to determinism.
In particular we show that programs that are determined to see That's a very strong correctness condition that guarantees determinism. If you have that property then you get disentanglement for free. This means it emerges natural ly in function programs. FTd what's really cool about this is that it enables efficient and scalable automatic memory management, okay, and the intuition here is no cross pointers and cross pointers are a long standing kind of challenge in designing and efficient parallel garbage collector that pointers across separately. Concurrently, generated regions of memory can degrade the scalability of collective.
So, based on this disentanglement property over the past five or six years we've been developing this compiler that we call maple based on Milton, and we support the full standard ML language, and what we've done is we've extended it with a single constructs that we call par par basically just evaluates two things in parallel, and returns the results back to.
This is fairly robust implementation. You can go and use it right now. In fact, it's being used at Carnegie Mellon over 500 students approximately every year use it. We use it as part of our undergraduate program to help teach parallel algorithms. And so we have a completely new runtime system.
Certainly the compiler is Milton, but the runtime system is a new runtime system based on disentanglement. And one of the key features actually is that we have a provably efficient algorithm that we presented at POPL 2021, based on disentanglement. Okay, now we're getting really really good performance with this in comparison to a few other memory minutes languages. For example, in comparison to Java on 72 cores this is a decent sized multi core machine. We're about 3x faster on average across a variety of benchmarks, and these are benchmarks that we've ported from state of the art, C and C Plus Plus benchmark suites. So it kind of got what we're seeing is on average about 3x faster and about 4x less space in comparison to go about to accessory maybe 30% less space.
. And now we've actually just recently been some comparisons with the new multicore OCaml project. Nd I'm really excited to see so much interest in parallel functional programming in this community. Now multicore, OCamel, is quite a bit different from our compiler, like for example, our compiler is a whole program compiler.
That's one source of improved efficiency, but also by is quite a bit different from our compiler, like for example, our compiler is a whole program compiler. That's one source of improved efficiency, but also by performance in comparison to C and C++. The margins there are approximately a factor two in terms of time, on average, but the space footprints actually it's about identical on average in the benchmarks that were considered. So all of this sounds really great. What's the problem, and what's the focus of this talk?
So... the issue is that not all programs are disentangled of course, you have restriction you can't communicate across the gap, and if we have GC that talked for it. And very bad things can happen. Here two tasks running in parallel, and there is pointers between the objects, they allocated. In a Well now if a garbage collection happens on the left hand side, one of the things that the garbage collector may want to do computer memory. And in order to compact memory you need to move objects around, you need to rename them essentially give them a new location. Now when you do this you need to update all references to any relocated objects, but a garbage collector based on disentanglement essentially assumes that cross pointers are impossible and doesn't try to even check for themm. So causes DANGalling pointer, and very bad, and terrible things can happen. If thewe need to enforce this entanglement and that's really what we focus on in this particular paper, question of how to enforce this entanglement, So the immediate question you might have if maybe we can do it statically. Okay and this turns out to be really tricky and we've worked on this problem for quite some time and haven't yet gotten something that we feel is quite ready.
There's a few things to consider here. How might you go about doing this. Well the first thing you could try is disallowing in place updates. This works, this, this guarantees disentanglement by construction. And extremely inefficient. In particular, this is really important for parallel computing, perhaps could be most fundamental operation and parallel computing is a parallel fillarrra why.
Allocate an array and then in parallel, you fill in each element that fundamentally is mutation, regardless of whether or not at the surface level it Okay, at the low level, the garbage collector sees mutation, it is fundamentally we need to meet to be able to support it efficiently. So you might think There may be some sort of typing effects system would would be able to enforce this And we've tried to develop something like this, but basically the problem that we run into is that it's too conservative. We have a big benchmark suite of approximately 30 fairly sophisticated parallel algorithms that we've ported C and C++. And very, very common thing is parallel algorithms, despite wanting to be deterministic, that's a common correctness condition often will utilize a very small amount of non determinism, to gain significant performance advantage, and the intuition here is that with a little bit of say atomic in place, modification, maybe a compare and swap that sink that kind of is on the fly, communicating with a concurrent tasks you can eliminate a large amount of unnecessary synchronization.
This becomes very challenging to design thing that is essentially nondeterministic, and aliasing between different objects that have been allocated. It seems very difficult to detect this property statically, but it is something that we're still working on and we're interested in pursuing what we do in this paper is we instead do it dynamically. And the intuition, we want to monitor individual read and is writes to memory. If we ever detect entanglement, then we can terminate the program safely within their message, you can generalize this to say a dynamic exception that could even be handled if you'd like, but our implementation at the moment this very simple.
This is very similar to the concept of race detection. If you've seen rates detection in other contexts, Now the difference here is that we're able to detection can be accomplished with essentially zero overhead,your practice.
Race detection. As many of you I'm sure are familiar has massive. .. overheads, typical sort of overhead that you see is a factor 10 on a single core.
Those overheads tend to increase as you go parallel, our entanglement detection results in paper fully parallel, and overheads are about are consistent across different core counts. And what we're seeing is that it's on average about 1% across the board. That's for both time and space. And the max we have seen is only 10% overhead one of the key contributions in the paper is trying to identify whether or not, the entanglement detection algorithm is good. Okay, what does it mean for detection algorithm tobe good. O we formulate two conditions here we call soundness and completeness.
Okay, two sides of the coin that the non negotiable. One is that needs to be safe for disentanglement. Okay, so it should be safe to run a disentangle program, which will be memory safe to run it disentangle program. Okay. And that can be formulated thankfully as a sound this property for disentanglement. Or another way of saying that it's there's no alarm that you miss.
Now on the other side of the coin, we have completeness, and in some sense this is just as important, what we found, any stat tick technique, it would rule out ASL of our BAEFRJ marks.
So this particular technique, is to allow for all disentangle program to completion. Okay, now. should say relative to particularly execution. But we're dealing with non determinism here. Okay, but I'd be happy to talk about that.
Later. It's a bit of a subtle point, some details of this algorithm for you. How do we formally define entanglement, and is finis terms of computational graph. Think of history of execution or trace of execution, writing down what the execution did, and abstracting details of scheduling.
So the edges of edges are sequential dependencies between those instructions. Okay, and in some sense the whip of this computation graph is the parallelism that you're getting at any moment. And that parallel grows and shrinks as the run the program, in the computation graph, we can annotate different instructions and allocate where they occur, and where they are used. When I say used, even holding a pointer to allow DAGS, we consider use. That's what is relevant for garbage collection.
And includes accessing location, and all constitutes use. Turns out you can define disentanglement as POT property, basically, you are uses, between allocations, or sorry, every allegation proceeds all of the uses of that allocation. Okay, so basically just that your uses of your allocations respect the sequential dependencies that are inherent to your program, entanglement is when you essentially get to use use across the algorithm that we implement in this paper, and that we analyze in this paper is extremely simple. It's exactly the definition, ssentially, we build the computation graph during execution. Okay, we annotate, all of our allocations with a vertex in the computation graph. And then the key results is that we only need to check the results of memory reads, so.
Any time you load something from memory you get back name of location, have to check if that before me.
If you detext it's SFWOID not before you, and at the point you get detection and so you may with thinking, PS women, first observation, you don't need whole instructions, but actually you only need things at the granularity of whole tasks.
Okay. And in a parallel program that has good granularity control which is a necessity for good performance in the program anyway. These tasks are actually large enough, but essentially the overhead of maintaining the computation graph goes down close to zero. And in particular what we're taking what we're using here is a technique called SP order maintenance. PS got from the risk detection literature a lot of this work is inspired by that literature.
Okay and that's pure domains maintenance is essentially a very clever encoding of the computation graph that, to be honest users essentially no additional space at runtime.
Now the way we actually increment this is a weed ... So every time that you dereference something in memory, you have to do a check. Now that's like only immutable, the references, so that means this but there's already no overhead for immutable data. And one of the key contributions and this work is proving that immutable data is always safe for disentanglement. One of the cool things I don't have time to talk about in the talk, but hope you see until the paper, we have really effective fast path for the read barrier, essentially the fast past of barrier, very low overhead is 99% of the time, key why this...performance so efficiently, and we call that idea entanglement candidates, it's a way of tracking potentially hazardous mutable objects at runtime. And I'll just mentioned, it's very closely integrated with memory management in order to make this safe for garbage collection.
So all of this is available in maple. I should and I should mention the performance results that I mentioned earlier, you know 3x faster than Java to access with an NGO that includes the cost of entanglement detection, especially if it's a fair comparison in the sense of like our languages memory safe and we're comparing it to other memory safe languages.
So what we do in this paper really is that this disentangling property which seems to be extremely important for efficient memory management is efficiently checkable at runtime. Okay. And we have a maple implementation of it, which is practical and fast. And I really encourage you all to go check it out. And one avenue for future work that we're really excited about that this opens up is the possibility of what we call entanglement management.
Intuition there, there is that at the moment of detection handle entanglement dynamically six it up, that might cost a little bit but we can fix it up in that moment. And therefore, essentially make this disentangling product, a strictly a performance property ,. Right now with a kind of halfway between performance and correctness, and we'd like to turn it into a property which is purely performance property.
I believe that this thing that management idea can get us there.
If you'd like to hear more about in disentanglement, please come see my keynote DMH on Thursday, and I'll be going into the details of how the garbage collector works and all of that.
.
Thank you so much for listening.
>> Thank you.
