So the primary goal of this particular paper is to look at sort of the informal way that we analyze algorithms, or given some sort of functional program, tend to, especially teaching say undergraduates, right, we look at the algorithm would basically write down a function on the board that looks a lot like the algorithm more or less, right, it's supposed to represent the cost in terms of the size of the input. And that's what we'll call cost recurrence, we write that down and then we go ahead and we solve it or come up with some sort of a closed form cetera. 
And so the that I want to talk about is to try to use denotational semantics give a formal account of that sort of informal process of coming up with that cost recurrence. Not focusing on solving it directly, but coming up with the recurrence, where does it actually come from. Just to be clear, to set a line on that, not so much focusing on effectively building a better mousetrap and not trying to come up with techniques for more PL-based approaches to cost analysis, like ARA and other approaches, really just want to focus on giving a formal account of that informal process. So the big picture here starting off with some sort of source language, which for right now Call-by-Value version of ML which is data types and followed, and handle general recursion in other papers but the basic source language we're working with. And syntactic part, we do sort of a GIST translation into the writer monad to get what we call a syntactic recurrence and that's basically now call by name predicate of polymorphism I should have said when I think of call by name ml or call by value ml. Polymorphism. So extract that into form some form of a predicate of polymorphic lambda calculus and then hit with a model, interpret the recurrence in some sort of a denotational semantics, that's where we give this notion of size for he arguments of the original source program, and it's at that point we should start seeing the recurrences we should expect to see showing up. So example here right, let's say typical program for binary search tree membership testing, which is basically a tree fold as shown here. And let's focus on on counting say the number of recursive calls induced by that followed, the program we're starting with the goal to come up with the recurrence you would expect to see doing cost analysis for this. So first step translation into the writer monad that then translates a program type sigma into new program of type that I'll call cost and proterrible of sigma. 
And type C there is cost type, which in the syntax is basically a type, and generally interpreted by numbers or something like that. And potential sigma is the other part there. And that's what we think of as type of potentials or sizes of usage cost of value type sigma in the original source language, so size is the right way to think about it, potential is the term that coauthor Jim L and I adopted a long time ago, and possibly poorly. So just to clarify does not directly have anything to do with potentials in amortized analysis, now we're really thinking about as potentialcosts of evaluating how might get used and contribute to cost. So in this case, since cost really explicit in the syntax here, we call that pair a complexity. But in the notation, I'll do E withcost being the pair, because it's easier to read. And basically what you expect as cost the translation you get from the binary search tree membership program, that is again a tree fold. And in this case, say you have a node, what are you going to do right, compared the two values again, and say the comparison comes out less than: 
the cost, the as a result is basically, potential of the left hand recursive, that are 0. And recursive call there. And P is subscripts basically the projections on the pair, and cost of recursive function, and cost of comparison, and along with the cost for actually this particular unfolding of the fold. 
So where this gets interesting is then when we interpret that syntactic recurrence there a model. So the goal in this case to analyze the binary search tree membership function in terms of height of argument in the first place, and want to model essentially interpreting by height. And interpret the empty constructor by 0, the node constructor as just adding one to the maximum of the interpretations of the subtrees right and that gives us an interpretation of feces. 
Wherever tree is now interpreted by height. So doing that we still have to interpret fold as well in this model. And in this case, essentially interpreting fold over a height now. Because the tree argument is now just a number. So interpreting over the height is maximum over the branches and have to take the maximum, for the node branch, right, over in some sense all the possible heights one might encounter for the subtrees of a tree of height H, nd hence that's where that age zero Max H one is less than h coming through for that for that overall maximum. Also what do we do about that label. If we're talking relevant notion ofsize, the height of tree the LANLs are irrelevant right and so in fact it ends up working towards antics where we basically say well let's set that label value to essentially an infinite value. That infinity comes out of the fact that when you sort of dig into the details of defining these models, It's actually very convenient for to interpret all the types as complete lattices.o there is a top element and infinite infinity. And so in this case we're basically saying, I don't care about the size of the labels, r more accurately, I do this interpretation, when the label has size Atmos infinitely large, which can be anything. 
So when we do that the interpretation of the original recurrence back here. Get exactly the recurrence you expect to get looking at original source program and stairing, and what we will get as recurrence, to describe the cost of program in terms of height of tree. And have to more or less maximize overall possible subtrees that have height smaller than H, that exactly what that recurrence is there. And this is giving the idea of flavour of what we expect to be coming out of this sort of program. 
And so that's the basic structure. 
And so... for all this to fly, we need to theorem that ties all this together and what does that boil down to. And we start doing syntactic translation, the vertical bars and bound on the denotation of the syntactic extraction of the original program, someone has to be a bound on the original cost of the program, E. Breaking that down into two steps, ne involving a logical relation that relates programs in the source language to syntactic recurrences in the in the syntactic recurrence language, right, and that particular relation then, proved the fundamental theorem and its consequences the definition of that logic tells you that the cost component of the extracted syntactic recurrence is in fact a bound on the operational costs from the source programming language. And then also bound related to the result you get. 
That bound then is... defined in terms of a notion of a size order in the recurrence language itself. And then you prove you actually have a model of the recurrence language. And those two together then glue together to tell you semantic recurrence you extract it looks like what we normally see on the board actually gives about the original program the as it turns the cost of the original program as well as it turns out, the size of the result. That size order what we use to acclimatize that recurrence language rather than equations, by a size ordering. He axioms on that size order tend to come to the come in a couple of flavors, some of them had to do with how the introductory and elimination forms for each type interact with each other. And others with the monotonicity of the size. And so its first kinds that are actually three a little bit interesting, that's where you say you actually start seeing things look a lot like Galois connections coming up there. And so, they basically end up playing the role of saying that the introduction form for a type operate for any given type essentially looks a lot like the abstraction function you would see in Galois onnection, whereas the elimination form ends up looking a lot like the concretization function, and that sort of makes sense when you think about it, because the whole point here is that you're really sort of trying to abstract away information about the original values in terms of the size sizes are sort of abstract descriptions of your original values. So the thing I want to talk a little bit about various various kind of models. Each data type counting up the number of main constructors in a value of that type. 
And so how you count those up, maybe count up total numbers. 
And maximum depth of constructors, and so that's one where you interpret... natural number with top, so you infinity element to take the maximums that show up in the definitions. 
That's all well and fine. 
But you if are say have in front of you a function that takes nat label tree, and adds up all the nat on the labels you are going to run into the trouble in the model. Not trouble it's going to give UT wrong result, it's perfectly fine model. If it's model of recurrence language, and re-Manitobaic recurrence in the solution is going to be abound by definition. The problem is with the setting you get bound of infinity for the cost. Yes, your program runs and its cost is utmost infinite and the result as it turns out will also have a balance of infinity. Yes, you do get a result of the size is utmost infinite, which is true, not necessarily most useful information might want. 
Right, but you can define more complex models, where for example, you can count all of the constructors of all of them individually right so you interpret really every single datatype. As a function that maps all data types numbers with the intent being that INTERPS stay value of nat tree maps nat to say nat inductive type to maximum label size, and the number of nat trees itself to the number of nodes, tree nodes. 
When you do that, and pain comes in. Fold has fairly similar looking interpretation, the big maximum we seen previously for heights here, now becomes being a big maximum and the values pointwise counters for all possible data types, and end up maximizing pointwise smaller things, so little paintful, and so if you are looking at summing up the labels on nat tree, you end up getting the same recurrence when you write these down informally. On the other hand, that's a bit of annoyance having that additional complexity if you do actually want to analyze functions that don't depend on the labels right because now you're sort of having to worry and especially in terms of the computation, you have to worry about all that information where the followed is defined. That comes up in particular, with polymorphic functions. Right where in fact you're your typical polymorphic function I'm going to switch to list instead of trees now. And should not depend on the label as all. And so what's going on there right in the model, both the two previous models I talked abouts about the interpretation of universal quantifiers for different product overall the all of the unquantified types, so there's there's no real problem doing that. Right. But then if you're looking at that all constructor model. When you want toreason about polymorphic function you said have to at least have to instantiate a person to tight to reason about the corresponding recurrence of that type, and is you're computing fold now over the complex pointwise descriptions of how many constructors you have of every single data type. 
And what we'd really like is what we do informally when we say analyze a polymorphic LIS function we say we're just gonna analyze this in terms of the length away right if she'd be a function from numbers to numbers, even though we know we might have complex element types in particular and I'll point out that LMN there, looks very similar. It's not quite but it's very close to the standard interpretation of the quantifier, the universal quantifier you would get from that main constructor counting model there'd be an extra if dexing over all the small types but in every for every single type since you're just counting the number of main constructors, you would get numbers to numbers as the actual type. Can do that right, another model. In this case, model are you with merge them, and Galois connection, between the non quantified types of the main constructor and the constructor models right again you can think of just counting the number of main constraints are basically being essentially an abstract form of counting more abstract presentation then counting all the constructors, we can leverage that to get a non standard interpretation of universal constructor. In which case that we get now is that the small types are all interpreted with his very fine grained information that describes all the possible type, all the possible use of constructors and quantified types, interpreted using main constructors what ends up happening now. Really only have to consider examining folds at... we're now going to essentially say the value if interpreting analyzing function of alpha list, and interpreting value of type in this the model. 
Type nat. And concretize that, and point WIES, counting everything again. And then, FASHTHs everything mapped to infinity, anything else showing up in the list. S have you as having a trivial bound on the size, why is that well it's because it was a polymorphic function in the first place. 
And so we do that, then, for example, if we say take reverse like linear time.high ordered reverse function. What you end up with exactly the recurrences you expect when you're analyzing that informal way just by presupposing that it only depend on the length of the list. So let's go to basic stuff there's other stuff in the paper. And continuing to do on going work, and bit of bee in my bonnet right now, taking the same approach to understanding algorithms we reason a lot more abstracting, and analyzing happysort and going to sort a list assuming I have longtime happy implementation, And then I can analyze the sorting algorithm based on that, without thinking about the heap implementation. At the same time I didn't have to go off and analyze some heap implementation and reason about that separately. Alright, but that's real self and I'd be happy to take questions.
